{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torchsummary\n",
    "import glob\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from timm.data import Dataset, create_loader, resolve_data_config,  FastCollateMixup, mixup_batch, AugMixDataset\n",
    "from timm.models import create_model, resume_checkpoint, convert_splitbn_model, apply_test_time_pool\n",
    "from timm.utils import *\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from timm.optim import create_optimizer\n",
    "from timm.scheduler import create_scheduler\n",
    "from munch import Munch\n",
    "import yaml\n",
    "import sys\n",
    "from resnet_generator import Generator\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from apex import amp\n",
    "# from apex.parallel import DistributedDataParallel as DDP\n",
    "# from apex.parallel import convert_syncbn_model\n",
    "has_apex = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device  True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = '0,1,2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device \" , use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = '14.49.45.144' \n",
    "os.environ['MASTER_PORT'] = '16022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training with a single process on 3 GPUs.\n"
     ]
    }
   ],
   "source": [
    "with open('config/train.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = Munch(config)\n",
    "args.prefetcher = not args.no_prefetcher\n",
    "args.distributed = False\n",
    "args.device = 'cuda'\n",
    "args.world_size = 3\n",
    "args.rank = 0\n",
    "logging.info('Training with a single process on %d GPUs.' % args.num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    args.num_gpu = 1\n",
    "    args.device = 'cuda:%d' % args.local_rank\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://', rank=args.rank, world_size=args.world_size)\n",
    "    args.world_size = torch.distributed.get_world_size()\n",
    "    args.rank = torch.distributed.get_rank()\n",
    "    assert args.rank >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc8fb034170>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(args.seed + args.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ns = timm.create_model('tf_efficientnet_b7_ns', pretrained=True)\n",
    "model_ns = model_ns.cuda()\n",
    "model_ns = torch.nn.DataParallel(model_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw = timm.create_model('tf_efficientnet_b7', pretrained=True)\n",
    "model_raw = model_raw.cuda()\n",
    "model_raw = torch.nn.DataParallel(model_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(args, img_size=600, max_conv_dim=512)\n",
    "model = model.cuda()\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data processing configuration for current model + dataset:\n",
      "INFO:root:\tinput_size: (3, 600, 600)\n",
      "INFO:root:\tinterpolation: bicubic\n",
      "INFO:root:\tmean: (0.485, 0.456, 0.406)\n",
      "INFO:root:\tstd: (0.229, 0.224, 0.225)\n",
      "INFO:root:\tcrop_pct: 0.875\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/home/data/imagenet/train'\n",
    "val_dir = '/home/data/imagenet/val'\n",
    "data_config = resolve_data_config(vars(args), model=model, verbose=args.local_rank == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aug_splits = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = create_optimizer(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:NVIDIA APEX not installed. AMP off.\n"
     ]
    }
   ],
   "source": [
    "use_amp = False\n",
    "if has_apex and args.amp:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    model_raw =  amp.initialize(model_raw)\n",
    "    model_ns = amp.initialize(model_ns)\n",
    "    use_amp = True\n",
    "if args.local_rank == 0:\n",
    "    logging.info('NVIDIA APEX {}. AMP {}.'.format(\n",
    "        'installed' if has_apex else 'not installed', 'on' if use_amp else 'off'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    if args.sync_bn:\n",
    "        assert not args.split_bn\n",
    "        try:\n",
    "            if has_apex:\n",
    "                model = convert_syncbn_model(model)\n",
    "            else:\n",
    "                model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "            if args.local_rank == 0:\n",
    "                logging.info(\n",
    "                    'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                    'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "        except Exception as e:\n",
    "            logging.error('Failed to enable Synchronized BatchNorm. Install Apex or Torch >= 1.1')\n",
    "    if has_apex:\n",
    "        model = DDP(model, delay_allreduce=True)\n",
    "    else:\n",
    "        if args.local_rank == 0:\n",
    "            logging.info(\"Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP.\")\n",
    "        model = DDP(model, device_ids=[args.local_rank])  # can use device str in Torch >= 1.1\n",
    "    # NOTE: EMA model does not need to be wrapped by DDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler, num_epochs = create_scheduler(args, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_state = {}\n",
    "resume_epoch = None\n",
    "start_epoch = 0\n",
    "if args.start_epoch is not None:\n",
    "    # a specified start_epoch will always override the resume epoch\n",
    "    start_epoch = args.start_epoch\n",
    "elif resume_epoch is not None:\n",
    "    start_epoch = resume_epoch\n",
    "if lr_scheduler is not None and start_epoch > 0:\n",
    "    lr_scheduler.step(start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scheduled epochs: 200\n"
     ]
    }
   ],
   "source": [
    "if args.local_rank == 0:\n",
    "    logging.info('Scheduled epochs: {}'.format(num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_dir)\n",
    "# val_dataset = Dataset(val_dir, load_bytes=False, class_map='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model created, param count: 4253205\n"
     ]
    }
   ],
   "source": [
    "param_count = sum([m.numel() for m in model.parameters()])\n",
    "logging.info('Model created, param count: %d' % (param_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collate_fn = None\n",
    "if args.prefetcher and args.mixup > 0:\n",
    "    assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "    collate_fn = FastCollateMixup(args.mixup, args.smoothing, args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_raw, test_time_pool = apply_test_time_pool(model_raw, data_config, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_loader(\n",
    "        train_dataset,\n",
    "        input_size=data_config['input_size'],\n",
    "        batch_size=args.batch_size,\n",
    "        is_training=True,\n",
    "        use_prefetcher=args.prefetcher,\n",
    "        re_prob=args.reprob,\n",
    "        re_mode=args.remode,\n",
    "        re_count=args.recount,\n",
    "        re_split=args.resplit,\n",
    "        color_jitter=args.color_jitter,\n",
    "        auto_augment=args.aa,\n",
    "        num_aug_splits=num_aug_splits,\n",
    "        interpolation=args.train_interpolation,\n",
    "        mean=data_config['mean'],\n",
    "        std=data_config['std'],\n",
    "        num_workers=args.workers,\n",
    "        distributed=args.distributed,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=args.pin_mem,\n",
    "        use_multi_epochs_loader=args.use_multi_epochs_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_pct = 1.0 if test_time_pool else data_config['crop_pct']\n",
    "# val_loader = create_loader(\n",
    "#     val_dataset,\n",
    "#     input_size=data_config['input_size'],\n",
    "#     batch_size=args.batch_size,\n",
    "#     is_training=False,\n",
    "#     use_prefetcher=args.prefetcher,\n",
    "#     interpolation=data_config['interpolation'],\n",
    "#     mean=data_config['mean'],\n",
    "#     std=data_config['std'],\n",
    "#     num_workers=args.workers,\n",
    "#     crop_pct=crop_pct,\n",
    "#     pin_memory=args.pin_mem,\n",
    "#     tf_preprocessing=args.tf_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.jsd:\n",
    "#     assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "#     train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "#     validate_loss_fn = nn.CrossEntropyLoss()\n",
    "# elif args.mixup > 0.:\n",
    "#     # smoothing is handled with mixup label transform\n",
    "#     train_loss_fn = SoftTargetCrossEntropy()\n",
    "#     validate_loss_fn = nn.CrossEntropyLoss()\n",
    "# elif args.smoothing:\n",
    "#     train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "#     validate_loss_fn = nn.CrossEntropyLoss()\n",
    "# else:\n",
    "#     train_loss_fn = nn.CrossEntropyLoss()\n",
    "#     validate_loss_fn = train_loss_fn\n",
    "\n",
    "loss_l1_fn = nn.L1Loss()\n",
    "train_loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = args.eval_metric\n",
    "best_metric = None\n",
    "best_epoch = None\n",
    "saver = None\n",
    "output_dir = ''\n",
    "if args.local_rank == 0:\n",
    "    output_base = args.output if args.output else './output'\n",
    "    exp_name = '-'.join([\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        args.model,\n",
    "        str(data_config['input_size'][-1])\n",
    "    ])\n",
    "    output_dir = get_outdir(output_base, 'train', exp_name)\n",
    "    decreasing = True if eval_metric == 'loss' else False\n",
    "    saver = CheckpointSaver(checkpoint_dir=output_dir, decreasing=decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema = None\n",
    "if args.model_ema:\n",
    "    # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n",
    "    model_ema = ModelEma(\n",
    "        model,\n",
    "        decay=args.model_ema_decay,\n",
    "        device='cpu' if args.model_ema_force_cpu else '',\n",
    "        resume=args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, model_raw, model_ns, loader, optimizer, loss_fn, loss_traj_fn, args,\n",
    "               lr_scheduler=None, saver=None, output_dir='', use_amp=False, model_ema=None):\n",
    "\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "    losses_l1 = AverageMeter()\n",
    "    losses_traj = AverageMeter()\n",
    "    losses_recon = AverageMeter()\n",
    "    \n",
    "    model.train()\n",
    "    model_ns.eval()\n",
    "    model_raw.eval()\n",
    "    \n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    num_updates = epoch * len(loader)\n",
    "    for batch_idx, (inputs, target) in enumerate(loader):\n",
    "        last_batch = batch_idx == last_idx\n",
    "        data_time_m.update(time.time() - end)\n",
    "        if not args.prefetcher:\n",
    "            inputs, target = inputs.cuda(), target.cuda()\n",
    "        \n",
    "        inputs_out = model(inputs)\n",
    "        inputs_z = inputs + inputs_out\n",
    "        with torch.no_grad():\n",
    "            out_ns, traj_ns = model_ns(inputs)\n",
    "            output, traj_raw = model_raw(inputs_z)\n",
    "        \n",
    "        inputs = inputs.detach()\n",
    "        out_ns = out_ns.detach()\n",
    "        loss_recon = loss_fn(inputs_z, inputs)\n",
    "        loss_l1 = loss_fn(output, out_ns)\n",
    "        loss_traj = 0\n",
    "        \n",
    "        for i in range(len(traj_raw)):\n",
    "            traj_ns[i] = traj_ns[i].detach()\n",
    "            value = loss_traj_fn(traj_raw[i], traj_ns[i])        \n",
    "            loss_traj += value\n",
    "        \n",
    "        loss = args.lambda_l1 * loss_l1 + args.lambda_traj * loss_traj + args.lambda_recon * loss_recon\n",
    "        \n",
    "        if not args.distributed:\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "            losses_traj.update(loss_traj.item(), inputs.size(0))\n",
    "            losses_recon.update(loss_recon.item(), inputs.size(0))\n",
    "            losses_m.update(loss.item(), inputs.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "        num_updates += 1\n",
    "\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        if last_batch or batch_idx % args.log_interval == 0:\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            lr = sum(lrl) / len(lrl)\n",
    "\n",
    "            if args.distributed:\n",
    "                reduced_loss = reduce_tensor(loss.data, args.world_size)\n",
    "                losses_m.update(reduced_loss.item(), inputs.size(0))\n",
    "\n",
    "            if args.local_rank == 0:\n",
    "                logging.info(\n",
    "                    'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
    "                    'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
    "                    'Loss_l1: {loss_l1.val:>9.6f} ({loss_l1.avg:>6.4f})  '\n",
    "                    'Loss_traj: {loss_traj.val:>9.6f} ({loss_traj.avg:>6.4f})  '\n",
    "                    'Loss_recon: {loss_recon.val:>9.6f} ({loss_recon.avg:>6.4f})  '\n",
    "                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
    "                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'LR: {lr:.3e}  '\n",
    "                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
    "                        epoch,\n",
    "                        batch_idx, len(loader),\n",
    "                        100. * batch_idx / last_idx,\n",
    "                        loss=losses_m,\n",
    "                        loss_l1=losses_l1,\n",
    "                        loss_traj=losses_traj,\n",
    "                        loss_recon=losses_recon,\n",
    "                        batch_time=batch_time_m,\n",
    "                        rate=inputs.size(0) * args.world_size / batch_time_m.val,\n",
    "                        rate_avg=inputs.size(0) * args.world_size / batch_time_m.avg,\n",
    "                        lr=lr,\n",
    "                        data_time=data_time_m))\n",
    "\n",
    "                if args.save_images and output_dir:\n",
    "                    torchvision.utils.save_image(\n",
    "                        inputs_z,\n",
    "                        os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),\n",
    "                        padding=0,\n",
    "                        normalize=True)\n",
    "\n",
    "        if saver is not None and args.recovery_interval and (\n",
    "                last_batch or (batch_idx + 1) % args.recovery_interval == 0):\n",
    "            saver.save_recovery(\n",
    "                model, optimizer, args, epoch, model_ema=model_ema, use_amp=use_amp, batch_idx=batch_idx)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
    "\n",
    "        end = time.time()\n",
    "        # end for\n",
    "\n",
    "    if hasattr(optimizer, 'sync_lookahead'):\n",
    "        optimizer.sync_lookahead()\n",
    "\n",
    "    return OrderedDict([('loss', losses_m.avg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model_raw, model, val_loader, criterion, args):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model_raw.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # warmup, reduce variability of first batch time, especially for comparing torchscript vs non\n",
    "        end = time.time()\n",
    "        for i, (inputs, target) in enumerate(val_loader):\n",
    "            if args.no_prefetcher:\n",
    "                target = target.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "                \n",
    "            # synthesizing input + generator\n",
    "            inputs_out = inputs + model_out\n",
    "            # compute output\n",
    "            output, foward_list = model_raw(inputs_out)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            \n",
    "            model_out = model(inputs)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output.data, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(acc1.item(), inputs.size(0))\n",
    "            top5.update(acc5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.log_freq == 0:\n",
    "                logging.info(\n",
    "                    'Test: [{0:>4d}/{1}]  '\n",
    "                    'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '\n",
    "                    'Acc@1: {top1.val:>7.3f} ({top1.avg:>7.3f})  '\n",
    "                    'Acc@5: {top5.val:>7.3f} ({top5.avg:>7.3f})'.format(\n",
    "                        i, len(val_loader), batch_time=batch_time,\n",
    "                        rate_avg=inputs.size(0) / batch_time.avg,\n",
    "                        loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    results = OrderedDict(\n",
    "        top1=round(top1.avg, 4), top1_err=round(100 - top1.avg, 4),\n",
    "        top5=round(top5.avg, 4), top5_err=round(100 - top5.avg, 4),\n",
    "        param_count=round(param_count / 1e6, 2),\n",
    "        img_size=data_config['input_size'][-1],\n",
    "        cropt_pct=crop_pct,\n",
    "        interpolation=data_config['interpolation'])\n",
    "\n",
    "    logging.info(' * Acc@1 {:.3f} ({:.3f}) Acc@5 {:.3f} ({:.3f})'.format(\n",
    "       results['top1'], results['top1_err'], results['top5'], results['top5_err']))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [   0/28470 (  0%)]  Loss: 367.277557 (367.2776)  Loss_l1:  0.461082 (0.4611)  Loss_traj: 72.944885 (72.9449)  Loss_recon:  0.247719 (0.2477)  Time: 18.823s,    7.17/s  (18.823s,    7.17/s)  LR: 1.000e-04  Data: 2.387 (2.387)\n",
      "INFO:root:Train: 0 [  50/28470 (  0%)]  Loss: 361.795044 (367.1494)  Loss_l1:  0.433760 (0.4469)  Loss_traj: 71.882683 (72.9368)  Loss_recon:  0.212826 (0.2306)  Time: 1.807s,   74.72/s  (2.160s,   62.51/s)  LR: 1.000e-04  Data: 0.019 (0.066)\n",
      "INFO:root:Train: 0 [ 100/28470 (  0%)]  Loss: 362.355194 (365.7909)  Loss_l1:  0.425834 (0.4414)  Loss_traj: 72.010201 (72.6742)  Loss_recon:  0.175032 (0.2128)  Time: 1.872s,   72.12/s  (1.992s,   67.76/s)  LR: 1.000e-04  Data: 0.018 (0.043)\n",
      "INFO:root:Train: 0 [ 150/28470 (  1%)]  Loss: 364.405640 (364.3867)  Loss_l1:  0.422709 (0.4369)  Loss_traj: 72.428596 (72.4012)  Loss_recon:  0.149104 (0.1960)  Time: 1.805s,   74.77/s  (1.935s,   69.76/s)  LR: 1.000e-04  Data: 0.019 (0.035)\n",
      "INFO:root:Train: 0 [ 200/28470 (  1%)]  Loss: 355.934357 (362.8839)  Loss_l1:  0.429318 (0.4334)  Loss_traj: 70.731842 (72.1070)  Loss_recon:  0.128559 (0.1819)  Time: 1.858s,   72.68/s  (1.907s,   70.79/s)  LR: 1.000e-04  Data: 0.018 (0.031)\n",
      "INFO:root:Train: 0 [ 250/28470 (  1%)]  Loss: 352.248901 (361.6992)  Loss_l1:  0.440346 (0.4304)  Loss_traj: 69.986595 (71.8755)  Loss_recon:  0.114209 (0.1696)  Time: 1.806s,   74.74/s  (1.890s,   71.43/s)  LR: 1.000e-04  Data: 0.019 (0.028)\n",
      "INFO:root:Train: 0 [ 300/28470 (  1%)]  Loss: 361.072296 (360.5822)  Loss_l1:  0.419022 (0.4278)  Loss_traj: 71.775818 (71.6569)  Loss_recon:  0.098120 (0.1588)  Time: 1.803s,   74.87/s  (1.879s,   71.86/s)  LR: 1.000e-04  Data: 0.018 (0.027)\n",
      "INFO:root:Train: 0 [ 350/28470 (  1%)]  Loss: 355.131378 (359.6180)  Loss_l1:  0.424676 (0.4254)  Loss_traj: 70.584122 (71.4683)  Loss_recon:  0.087414 (0.1493)  Time: 1.805s,   74.81/s  (1.870s,   72.18/s)  LR: 1.000e-04  Data: 0.019 (0.026)\n",
      "INFO:root:Train: 0 [ 400/28470 (  1%)]  Loss: 351.805359 (358.7163)  Loss_l1:  0.416778 (0.4235)  Loss_traj: 69.929108 (71.2917)  Loss_recon:  0.075930 (0.1408)  Time: 1.805s,   74.81/s  (1.864s,   72.43/s)  LR: 1.000e-04  Data: 0.018 (0.025)\n",
      "INFO:root:Train: 0 [ 450/28470 (  2%)]  Loss: 355.665527 (357.9995)  Loss_l1:  0.388385 (0.4217)  Loss_traj: 70.730667 (71.1516)  Loss_recon:  0.070275 (0.1332)  Time: 1.805s,   74.80/s  (1.859s,   72.62/s)  LR: 1.000e-04  Data: 0.019 (0.024)\n",
      "INFO:root:Train: 0 [ 500/28470 (  2%)]  Loss: 347.845154 (357.2499)  Loss_l1:  0.391587 (0.4201)  Loss_traj: 69.165131 (71.0046)  Loss_recon:  0.061540 (0.1264)  Time: 1.805s,   74.79/s  (1.855s,   72.77/s)  LR: 1.000e-04  Data: 0.018 (0.024)\n",
      "INFO:root:Train: 0 [ 550/28470 (  2%)]  Loss: 348.266785 (356.6930)  Loss_l1:  0.407795 (0.4188)  Loss_traj: 69.234253 (70.8958)  Loss_recon:  0.056540 (0.1203)  Time: 1.877s,   71.93/s  (1.852s,   72.88/s)  LR: 1.000e-04  Data: 0.018 (0.023)\n",
      "INFO:root:Train: 0 [ 600/28470 (  2%)]  Loss: 350.422363 (356.0895)  Loss_l1:  0.433002 (0.4177)  Loss_traj: 69.641266 (70.7772)  Loss_recon:  0.051025 (0.1147)  Time: 1.804s,   74.81/s  (1.850s,   72.98/s)  LR: 1.000e-04  Data: 0.018 (0.023)\n",
      "INFO:root:Train: 0 [ 650/28470 (  2%)]  Loss: 345.707428 (355.7194)  Loss_l1:  0.420602 (0.4164)  Loss_traj: 68.711632 (70.7055)  Loss_recon:  0.046263 (0.1097)  Time: 1.862s,   72.51/s  (1.848s,   73.07/s)  LR: 1.000e-04  Data: 0.018 (0.022)\n",
      "INFO:root:Train: 0 [ 700/28470 (  2%)]  Loss: 348.158417 (355.3068)  Loss_l1:  0.415671 (0.4154)  Loss_traj: 69.207191 (70.6250)  Loss_recon:  0.044088 (0.1051)  Time: 1.804s,   74.85/s  (1.846s,   73.14/s)  LR: 1.000e-04  Data: 0.018 (0.022)\n",
      "INFO:root:Train: 0 [ 750/28470 (  3%)]  Loss: 341.383270 (354.9361)  Loss_l1:  0.403346 (0.4144)  Loss_traj: 67.865204 (70.5526)  Loss_recon:  0.040528 (0.1010)  Time: 1.805s,   74.81/s  (1.844s,   73.21/s)  LR: 1.000e-04  Data: 0.018 (0.022)\n",
      "INFO:root:Train: 0 [ 800/28470 (  3%)]  Loss: 343.685303 (354.5113)  Loss_l1:  0.394753 (0.4137)  Loss_traj: 68.334763 (70.4691)  Loss_recon:  0.037724 (0.0971)  Time: 1.806s,   74.77/s  (1.843s,   73.27/s)  LR: 1.000e-04  Data: 0.019 (0.022)\n",
      "INFO:root:Train: 0 [ 850/28470 (  3%)]  Loss: 349.349823 (354.1772)  Loss_l1:  0.388329 (0.4130)  Loss_traj: 69.474358 (70.4037)  Loss_recon:  0.036381 (0.0936)  Time: 1.805s,   74.78/s  (1.841s,   73.32/s)  LR: 1.000e-04  Data: 0.019 (0.022)\n",
      "INFO:root:Train: 0 [ 900/28470 (  3%)]  Loss: 354.239777 (353.8913)  Loss_l1:  0.399640 (0.4123)  Loss_traj: 70.441612 (70.3479)  Loss_recon:  0.033515 (0.0904)  Time: 1.879s,   71.86/s  (1.840s,   73.36/s)  LR: 1.000e-04  Data: 0.019 (0.021)\n",
      "INFO:root:Train: 0 [ 950/28470 (  3%)]  Loss: 345.362885 (353.6208)  Loss_l1:  0.391162 (0.4115)  Loss_traj: 68.675003 (70.2951)  Loss_recon:  0.032075 (0.0874)  Time: 1.806s,   74.74/s  (1.839s,   73.40/s)  LR: 1.000e-04  Data: 0.019 (0.021)\n",
      "INFO:root:Train: 0 [1000/28470 (  4%)]  Loss: 346.176453 (353.3530)  Loss_l1:  0.389229 (0.4110)  Loss_traj: 68.839821 (70.2427)  Loss_recon:  0.031234 (0.0846)  Time: 1.865s,   72.40/s  (1.838s,   73.43/s)  LR: 1.000e-04  Data: 0.019 (0.021)\n",
      "INFO:root:Train: 0 [1050/28470 (  4%)]  Loss: 351.867462 (353.1074)  Loss_l1:  0.365085 (0.4105)  Loss_traj: 70.002335 (70.1946)  Loss_recon:  0.030353 (0.0820)  Time: 1.805s,   74.80/s  (1.838s,   73.47/s)  LR: 1.000e-04  Data: 0.018 (0.021)\n",
      "INFO:root:Train: 0 [1100/28470 (  4%)]  Loss: 348.508911 (352.9096)  Loss_l1:  0.419290 (0.4099)  Loss_traj: 69.276886 (70.1561)  Loss_recon:  0.028029 (0.0796)  Time: 1.805s,   74.78/s  (1.837s,   73.50/s)  LR: 1.000e-04  Data: 0.018 (0.021)\n",
      "INFO:root:Train: 0 [1150/28470 (  4%)]  Loss: 345.039734 (352.6827)  Loss_l1:  0.409651 (0.4095)  Loss_traj: 68.592888 (70.1116)  Loss_recon:  0.027048 (0.0774)  Time: 1.804s,   74.84/s  (1.836s,   73.53/s)  LR: 1.000e-04  Data: 0.018 (0.021)\n",
      "INFO:root:Train: 0 [1200/28470 (  4%)]  Loss: 348.180420 (352.4895)  Loss_l1:  0.403073 (0.4090)  Loss_traj: 69.227844 (70.0739)  Loss_recon:  0.025842 (0.0753)  Time: 1.806s,   74.76/s  (1.835s,   73.55/s)  LR: 1.000e-04  Data: 0.019 (0.021)\n",
      "INFO:root:Train: 0 [1250/28470 (  4%)]  Loss: 342.869324 (352.2901)  Loss_l1:  0.440797 (0.4088)  Loss_traj: 68.128075 (70.0346)  Loss_recon:  0.024963 (0.0733)  Time: 1.807s,   74.71/s  (1.835s,   73.58/s)  LR: 1.000e-04  Data: 0.019 (0.021)\n",
      "INFO:root:Train: 0 [1300/28470 (  5%)]  Loss: 356.559570 (352.1309)  Loss_l1:  0.402702 (0.4083)  Loss_traj: 70.904221 (70.0036)  Loss_recon:  0.024946 (0.0714)  Time: 1.820s,   74.17/s  (1.834s,   73.60/s)  LR: 1.000e-04  Data: 0.034 (0.021)\n",
      "INFO:root:Train: 0 [1350/28470 (  5%)]  Loss: 348.997894 (351.9977)  Loss_l1:  0.388521 (0.4079)  Loss_traj: 69.406441 (69.9777)  Loss_recon:  0.023087 (0.0697)  Time: 1.874s,   72.05/s  (1.834s,   73.61/s)  LR: 1.000e-04  Data: 0.018 (0.021)\n",
      "INFO:root:Train: 0 [1400/28470 (  5%)]  Loss: 342.251404 (351.8549)  Loss_l1:  0.402661 (0.4078)  Loss_traj: 68.043022 (69.9496)  Loss_recon:  0.022971 (0.0680)  Time: 1.808s,   74.65/s  (1.833s,   73.63/s)  LR: 1.000e-04  Data: 0.022 (0.021)\n",
      "INFO:root:Train: 0 [1450/28470 (  5%)]  Loss: 359.383759 (351.7441)  Loss_l1:  0.406260 (0.4074)  Loss_traj: 71.465965 (69.9281)  Loss_recon:  0.022613 (0.0664)  Time: 1.869s,   72.22/s  (1.833s,   73.65/s)  LR: 1.000e-04  Data: 0.019 (0.020)\n",
      "INFO:root:Train: 0 [1500/28470 (  5%)]  Loss: 344.556305 (351.6214)  Loss_l1:  0.385357 (0.4071)  Loss_traj: 68.521675 (69.9041)  Loss_recon:  0.021134 (0.0650)  Time: 1.805s,   74.80/s  (1.833s,   73.66/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [1550/28470 (  5%)]  Loss: 344.401672 (351.4890)  Loss_l1:  0.399217 (0.4068)  Loss_traj: 68.476913 (69.8783)  Loss_recon:  0.021000 (0.0636)  Time: 1.807s,   74.71/s  (1.832s,   73.68/s)  LR: 1.000e-04  Data: 0.019 (0.020)\n",
      "INFO:root:Train: 0 [1600/28470 (  6%)]  Loss: 345.589844 (351.3985)  Loss_l1:  0.401167 (0.4065)  Loss_traj: 68.712616 (69.8607)  Loss_recon:  0.020939 (0.0622)  Time: 1.810s,   74.59/s  (1.832s,   73.70/s)  LR: 1.000e-04  Data: 0.024 (0.020)\n",
      "INFO:root:Train: 0 [1650/28470 (  6%)]  Loss: 358.459106 (351.3047)  Loss_l1:  0.421020 (0.4063)  Loss_traj: 71.266792 (69.8425)  Loss_recon:  0.020065 (0.0610)  Time: 1.805s,   74.80/s  (1.831s,   73.71/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [1700/28470 (  6%)]  Loss: 342.951996 (351.2044)  Loss_l1:  0.400593 (0.4062)  Loss_traj: 68.185928 (69.8227)  Loss_recon:  0.019413 (0.0597)  Time: 1.861s,   72.52/s  (1.831s,   73.72/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [1750/28470 (  6%)]  Loss: 341.524445 (351.0932)  Loss_l1:  0.391983 (0.4060)  Loss_traj: 67.909225 (69.8009)  Loss_recon:  0.018403 (0.0586)  Time: 1.803s,   74.88/s  (1.831s,   73.74/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [1800/28470 (  6%)]  Loss: 347.851990 (351.0076)  Loss_l1:  0.402463 (0.4058)  Loss_traj: 69.164124 (69.7842)  Loss_recon:  0.019055 (0.0575)  Time: 1.870s,   72.20/s  (1.831s,   73.74/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [1850/28470 (  6%)]  Loss: 338.828979 (350.8933)  Loss_l1:  0.422144 (0.4057)  Loss_traj: 67.340126 (69.7617)  Loss_recon:  0.017652 (0.0564)  Time: 1.805s,   74.81/s  (1.830s,   73.75/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [1900/28470 (  7%)]  Loss: 343.665619 (350.8093)  Loss_l1:  0.401618 (0.4054)  Loss_traj: 68.327858 (69.7453)  Loss_recon:  0.018251 (0.0554)  Time: 1.806s,   74.77/s  (1.830s,   73.76/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [1950/28470 (  7%)]  Loss: 344.890137 (350.7339)  Loss_l1:  0.397289 (0.4053)  Loss_traj: 68.577179 (69.7306)  Loss_recon:  0.017798 (0.0545)  Time: 1.805s,   74.80/s  (1.830s,   73.77/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2000/28470 (  7%)]  Loss: 348.994080 (350.6714)  Loss_l1:  0.392587 (0.4052)  Loss_traj: 69.402725 (69.7184)  Loss_recon:  0.017539 (0.0535)  Time: 1.804s,   74.83/s  (1.830s,   73.78/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2050/28470 (  7%)]  Loss: 352.761353 (350.6046)  Loss_l1:  0.386164 (0.4050)  Loss_traj: 70.162712 (69.7053)  Loss_recon:  0.016974 (0.0526)  Time: 1.805s,   74.80/s  (1.829s,   73.79/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2100/28470 (  7%)]  Loss: 343.162567 (350.5398)  Loss_l1:  0.390859 (0.4049)  Loss_traj: 68.238472 (69.6926)  Loss_recon:  0.015936 (0.0518)  Time: 1.804s,   74.82/s  (1.829s,   73.80/s)  LR: 1.000e-04  Data: 0.017 (0.020)\n",
      "INFO:root:Train: 0 [2150/28470 (  8%)]  Loss: 349.729034 (350.5155)  Loss_l1:  0.406102 (0.4049)  Loss_traj: 69.536438 (69.6880)  Loss_recon:  0.016333 (0.0510)  Time: 1.881s,   71.76/s  (1.829s,   73.81/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2200/28470 (  8%)]  Loss: 344.742554 (350.4747)  Loss_l1:  0.389958 (0.4047)  Loss_traj: 68.555504 (69.6803)  Loss_recon:  0.015221 (0.0502)  Time: 1.806s,   74.74/s  (1.829s,   73.82/s)  LR: 1.000e-04  Data: 0.019 (0.020)\n",
      "INFO:root:Train: 0 [2250/28470 (  8%)]  Loss: 342.924072 (350.4149)  Loss_l1:  0.406717 (0.4045)  Loss_traj: 68.174980 (69.6686)  Loss_recon:  0.015567 (0.0494)  Time: 1.883s,   71.68/s  (1.829s,   73.82/s)  LR: 1.000e-04  Data: 0.019 (0.020)\n",
      "INFO:root:Train: 0 [2300/28470 (  8%)]  Loss: 340.796661 (350.3749)  Loss_l1:  0.409435 (0.4043)  Loss_traj: 67.746841 (69.6609)  Loss_recon:  0.015285 (0.0487)  Time: 1.806s,   74.76/s  (1.829s,   73.83/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2350/28470 (  8%)]  Loss: 355.475677 (350.3241)  Loss_l1:  0.389880 (0.4042)  Loss_traj: 70.702103 (69.6511)  Loss_recon:  0.015777 (0.0480)  Time: 1.805s,   74.79/s  (1.828s,   73.83/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2400/28470 (  8%)]  Loss: 344.553467 (350.2772)  Loss_l1:  0.384651 (0.4040)  Loss_traj: 68.523140 (69.6420)  Loss_recon:  0.014518 (0.0473)  Time: 1.806s,   74.74/s  (1.828s,   73.84/s)  LR: 1.000e-04  Data: 0.019 (0.020)\n",
      "INFO:root:Train: 0 [2450/28470 (  9%)]  Loss: 346.794434 (350.2322)  Loss_l1:  0.395328 (0.4039)  Loss_traj: 68.960732 (69.6332)  Loss_recon:  0.014156 (0.0466)  Time: 1.806s,   74.75/s  (1.828s,   73.85/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2500/28470 (  9%)]  Loss: 349.575562 (350.1641)  Loss_l1:  0.399228 (0.4039)  Loss_traj: 69.513039 (69.6198)  Loss_recon:  0.014224 (0.0460)  Time: 1.867s,   72.32/s  (1.828s,   73.85/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2550/28470 (  9%)]  Loss: 345.309631 (350.0983)  Loss_l1:  0.397456 (0.4037)  Loss_traj: 68.661575 (69.6069)  Loss_recon:  0.014502 (0.0454)  Time: 1.804s,   74.85/s  (1.828s,   73.86/s)  LR: 1.000e-04  Data: 0.019 (0.020)\n",
      "INFO:root:Train: 0 [2600/28470 (  9%)]  Loss: 344.872192 (350.0555)  Loss_l1:  0.367684 (0.4036)  Loss_traj: 68.604004 (69.5985)  Loss_recon:  0.013761 (0.0448)  Time: 1.872s,   72.11/s  (1.828s,   73.86/s)  LR: 1.000e-04  Data: 0.019 (0.020)\n",
      "INFO:root:Train: 0 [2650/28470 (  9%)]  Loss: 350.205841 (350.0028)  Loss_l1:  0.376008 (0.4036)  Loss_traj: 69.662399 (69.5881)  Loss_recon:  0.013798 (0.0442)  Time: 1.803s,   74.88/s  (1.828s,   73.87/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2700/28470 (  9%)]  Loss: 351.022430 (349.9704)  Loss_l1:  0.377016 (0.4035)  Loss_traj: 69.824722 (69.5818)  Loss_recon:  0.013767 (0.0436)  Time: 1.806s,   74.73/s  (1.828s,   73.87/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n",
      "INFO:root:Train: 0 [2750/28470 ( 10%)]  Loss: 343.575043 (349.9296)  Loss_l1:  0.391106 (0.4034)  Loss_traj: 68.321266 (69.5739)  Loss_recon:  0.013182 (0.0431)  Time: 1.805s,   74.80/s  (1.827s,   73.88/s)  LR: 1.000e-04  Data: 0.018 (0.020)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    if args.distributed:\n",
    "        loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "    train_metrics = train_epoch(\n",
    "        epoch, model, model_ns, model_raw, train_loader, optimizer, train_loss_fn, loss_l1_fn, args,\n",
    "        lr_scheduler=lr_scheduler, saver=saver, output_dir=output_dir,\n",
    "        use_amp=use_amp, model_ema=model_ema)\n",
    "\n",
    "#     if (epoch+1)%10 == 0:\n",
    "#         eval_metrics = val_epoch(model_raw, model, val_loader, validate_loss_fn, args)\n",
    "\n",
    "    if model_ema is not None and not args.model_ema_force_cpu:\n",
    "        if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "            distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "        ema_eval_metrics = validate(\n",
    "            model_ema.ema, loader_eval, validate_loss_fn, args, log_suffix=' (EMA)')\n",
    "        eval_metrics = ema_eval_metrics\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        # step LR for next epoch\n",
    "        lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        update_summary(\n",
    "            epoch, train_metrics, eval_metrics, os.path.join(output_dir, 'summary.csv'),\n",
    "            write_header=best_metric is None)\n",
    "\n",
    "    if saver is not None:\n",
    "    # save proper checkpoint with eval metric\n",
    "        save_metric = eval_metrics[eval_metric]\n",
    "        best_metric, best_epoch = saver.save_checkpoint(\n",
    "            model, optimizer, args,\n",
    "            epoch=epoch, model_ema=model_ema, metric=save_metric, use_amp=use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
