{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torchsummary\n",
    "import glob\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from timm.data import Dataset, create_loader, resolve_data_config,  FastCollateMixup, mixup_batch, AugMixDataset\n",
    "from timm.models import create_model, resume_checkpoint, convert_splitbn_model, apply_test_time_pool\n",
    "from timm.utils import *\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from timm.optim import create_optimizer\n",
    "from timm.scheduler import create_scheduler\n",
    "from munch import Munch\n",
    "import yaml\n",
    "import sys\n",
    "from generator import Generator\n",
    "from datetime import datetime\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "from apex.parallel import convert_syncbn_model\n",
    "has_apex = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device  True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = '0,1,2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device \" , use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training with a single process on 3 GPUs.\n"
     ]
    }
   ],
   "source": [
    "with open('config/train.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = Munch(config)\n",
    "args.prefetcher = not args.no_prefetcher\n",
    "args.distributed = False\n",
    "args.device = 'cuda'\n",
    "args.world_size = 1\n",
    "args.rank = 0\n",
    "logging.info('Training with a single process on %d GPUs.' % args.num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5bbc854170>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(args.seed + args.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ns = timm.create_model('tf_efficientnet_b7_ns', pretrained=True)\n",
    "model_ns = model_ns.cuda()\n",
    "# model_ns = torch.nn.DataParallel(model_ns).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw = timm.create_model('tf_efficientnet_b7', pretrained=True)\n",
    "model_raw = model_raw.cuda()\n",
    "# model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator((3, 600, 600), args)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data processing configuration for current model + dataset:\n",
      "INFO:root:\tinput_size: (3, 600, 600)\n",
      "INFO:root:\tinterpolation: bicubic\n",
      "INFO:root:\tmean: (0.485, 0.456, 0.406)\n",
      "INFO:root:\tstd: (0.229, 0.224, 0.225)\n",
      "INFO:root:\tcrop_pct: 0.875\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/home/data/imagenet/train'\n",
    "val_dir = '/home/data/imagenet/val'\n",
    "data_config = resolve_data_config(vars(args), model=model, verbose=args.local_rank == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aug_splits = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.split_bn:\n",
    "#     assert num_aug_splits > 1 or args.resplit\n",
    "#     model = convert_splitbn_model(model, max(num_aug_splits, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = create_optimizer(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:NVIDIA APEX installed. AMP on.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "use_amp = False\n",
    "if has_apex and args.amp:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    use_amp = True\n",
    "if args.local_rank == 0:\n",
    "    logging.info('NVIDIA APEX {}. AMP {}.'.format(\n",
    "        'installed' if has_apex else 'not installed', 'on' if use_amp else 'off'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    if args.sync_bn:\n",
    "        assert not args.split_bn\n",
    "        try:\n",
    "            if has_apex:\n",
    "                model = convert_syncbn_model(model)\n",
    "            else:\n",
    "                model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "            if args.local_rank == 0:\n",
    "                logging.info(\n",
    "                    'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                    'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "        except Exception as e:\n",
    "            logging.error('Failed to enable Synchronized BatchNorm. Install Apex or Torch >= 1.1')\n",
    "    if has_apex:\n",
    "        model = DDP(model, delay_allreduce=True)\n",
    "    else:\n",
    "        if args.local_rank == 0:\n",
    "            logging.info(\"Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP.\")\n",
    "        model = DDP(model, device_ids=[args.local_rank])  # can use device str in Torch >= 1.1\n",
    "    # NOTE: EMA model does not need to be wrapped by DDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler, num_epochs = create_scheduler(args, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_state = {}\n",
    "resume_epoch = None\n",
    "start_epoch = 0\n",
    "if args.start_epoch is not None:\n",
    "    # a specified start_epoch will always override the resume epoch\n",
    "    start_epoch = args.start_epoch\n",
    "elif resume_epoch is not None:\n",
    "    start_epoch = resume_epoch\n",
    "if lr_scheduler is not None and start_epoch > 0:\n",
    "    lr_scheduler.step(start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scheduled epochs: 200\n"
     ]
    }
   ],
   "source": [
    "if args.local_rank == 0:\n",
    "    logging.info('Scheduled epochs: {}'.format(num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_dir)\n",
    "val_dataset = Dataset(val_dir, load_bytes=False, class_map='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model created, param count: 70208448\n"
     ]
    }
   ],
   "source": [
    "param_count = sum([m.numel() for m in model.parameters()])\n",
    "logging.info('Model created, param count: %d' % (param_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collate_fn = None\n",
    "if args.prefetcher and args.mixup > 0:\n",
    "    assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "    collate_fn = FastCollateMixup(args.mixup, args.smoothing, args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw, test_time_pool = apply_test_time_pool(model_raw, data_config, args)\n",
    "model_raw = torch.nn.DataParallel(model_raw).cuda()\n",
    "model_ns = torch.nn.DataParallel(model_ns).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_loader(\n",
    "        train_dataset,\n",
    "        input_size=data_config['input_size'],\n",
    "        batch_size=args.batch_size,\n",
    "        is_training=True,\n",
    "        use_prefetcher=args.prefetcher,\n",
    "        re_prob=args.reprob,\n",
    "        re_mode=args.remode,\n",
    "        re_count=args.recount,\n",
    "        re_split=args.resplit,\n",
    "        color_jitter=args.color_jitter,\n",
    "        auto_augment=args.aa,\n",
    "        num_aug_splits=num_aug_splits,\n",
    "        interpolation=args.train_interpolation,\n",
    "        mean=data_config['mean'],\n",
    "        std=data_config['std'],\n",
    "        num_workers=args.workers,\n",
    "        distributed=args.distributed,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=args.pin_mem,\n",
    "        use_multi_epochs_loader=args.use_multi_epochs_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pct = 1.0 if test_time_pool else data_config['crop_pct']\n",
    "val_loader = create_loader(\n",
    "    val_dataset,\n",
    "    input_size=data_config['input_size'],\n",
    "    batch_size=args.batch_size,\n",
    "    is_training=False,\n",
    "    use_prefetcher=args.prefetcher,\n",
    "    interpolation=data_config['interpolation'],\n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=args.workers,\n",
    "    crop_pct=crop_pct,\n",
    "    pin_memory=args.pin_mem,\n",
    "    tf_preprocessing=args.tf_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.jsd:\n",
    "    assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "    train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing).cuda()\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "elif args.mixup > 0.:\n",
    "    # smoothing is handled with mixup label transform\n",
    "    train_loss_fn = SoftTargetCrossEntropy().cuda()\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "elif args.smoothing:\n",
    "    train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing).cuda()\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    train_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "    validate_loss_fn = train_loss_fn\n",
    "\n",
    "loss_l1_fn = nn.L1Loss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = args.eval_metric\n",
    "best_metric = None\n",
    "best_epoch = None\n",
    "saver = None\n",
    "output_dir = ''\n",
    "if args.local_rank == 0:\n",
    "    output_base = args.output if args.output else './output'\n",
    "    exp_name = '-'.join([\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        args.model,\n",
    "        str(data_config['input_size'][-1])\n",
    "    ])\n",
    "    output_dir = get_outdir(output_base, 'train', exp_name)\n",
    "    decreasing = True if eval_metric == 'loss' else False\n",
    "    saver = CheckpointSaver(checkpoint_dir=output_dir, decreasing=decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema = None\n",
    "if args.model_ema:\n",
    "    # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n",
    "    model_ema = ModelEma(\n",
    "        model,\n",
    "        decay=args.model_ema_decay,\n",
    "        device='cpu' if args.model_ema_force_cpu else '',\n",
    "        resume=args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, model_ns, model_raw, loader, optimizer, loss_fn, loss_traj_fn, args,\n",
    "               lr_scheduler=None, saver=None, output_dir='', use_amp=False, model_ema=None):\n",
    "\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "    losses_l1 = AverageMeter()\n",
    "    losses_traj = AverageMeter()\n",
    "    \n",
    "    model.train()\n",
    "    model_ns.eval()\n",
    "    model_raw.eval()\n",
    "    \n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    num_updates = epoch * len(loader)\n",
    "    for batch_idx, (inputs, target) in enumerate(loader):\n",
    "        last_batch = batch_idx == last_idx\n",
    "        data_time_m.update(time.time() - end)\n",
    "        if not args.prefetcher:\n",
    "            inputs, target = inputs.cuda(), target.cuda()\n",
    "        \n",
    "        z_out = model(z)\n",
    "        inputs_z = inputs + z_out\n",
    "        inputs_z = Variable(inputs_z.data, requires_grad=True)\n",
    "        \n",
    "        output, traj_raw = model_raw(inputs_z)\n",
    "        with torch.no_grad():\n",
    "            output_ns, traj_ns = model_ns(inputs)\n",
    "        \n",
    "        loss_ns = loss_fn(output_ns, target)\n",
    "        loss_raw = loss_fn(output, target)\n",
    "        loss_l1 = torch.abs(loss_raw - loss_ns)\n",
    "        \n",
    "        loss_traj = 0\n",
    "        for i in range(len(traj_raw)):\n",
    "            value = loss_traj_fn(traj_raw[i], traj_ns[i])        \n",
    "            loss_traj += value\n",
    "        \n",
    "        loss = args.lambda_l1 * loss_l1 + args.lambda_traj * loss_traj\n",
    "        \n",
    "        if not args.distributed:\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "            losses_traj.update(loss_traj.item(), inputs.size(0))\n",
    "            losses_m.update(loss.item(), inputs.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "        num_updates += 1\n",
    "\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        if last_batch or batch_idx % args.log_interval == 0:\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            lr = sum(lrl) / len(lrl)\n",
    "\n",
    "            if args.distributed:\n",
    "                reduced_loss = reduce_tensor(loss.data, args.world_size)\n",
    "                losses_m.update(reduced_loss.item(), inputs.size(0))\n",
    "\n",
    "            if args.local_rank == 0:\n",
    "                logging.info(\n",
    "                    'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
    "                    'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
    "                    'Loss_l1: {loss_l1.val:>9.6f} ({loss_l1.avg:>6.4f})  '\n",
    "                    'Loss_traj: {loss_traj.val:>9.6f} ({loss_traj.avg:>6.4f})  '\n",
    "                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
    "                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'LR: {lr:.3e}  '\n",
    "                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
    "                        epoch,\n",
    "                        batch_idx, len(loader),\n",
    "                        100. * batch_idx / last_idx,\n",
    "                        loss=losses_m,\n",
    "                        loss_l1=losses_l1,\n",
    "                        loss_traj=losses_traj,\n",
    "                        batch_time=batch_time_m,\n",
    "                        rate=inputs.size(0) * args.world_size / batch_time_m.val,\n",
    "                        rate_avg=inputs.size(0) * args.world_size / batch_time_m.avg,\n",
    "                        lr=lr,\n",
    "                        data_time=data_time_m))\n",
    "\n",
    "                if args.save_images and output_dir:\n",
    "                    torchvision.utils.save_image(\n",
    "                        inputs_z,\n",
    "                        os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),\n",
    "                        padding=0,\n",
    "                        normalize=True)\n",
    "\n",
    "        if saver is not None and args.recovery_interval and (\n",
    "                last_batch or (batch_idx + 1) % args.recovery_interval == 0):\n",
    "            saver.save_recovery(\n",
    "                model, optimizer, args, epoch, model_ema=model_ema, use_amp=use_amp, batch_idx=batch_idx)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
    "\n",
    "        end = time.time()\n",
    "        # end for\n",
    "\n",
    "    if hasattr(optimizer, 'sync_lookahead'):\n",
    "        optimizer.sync_lookahead()\n",
    "\n",
    "    return OrderedDict([('loss', losses_m.avg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model_raw, model, val_loader, criterion, args):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model_raw.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # warmup, reduce variability of first batch time, especially for comparing torchscript vs non\n",
    "        inputs = torch.randn((args.batch_size,) + data_config['input_size']).cuda()\n",
    "        z = Variable(torch.Tensor(np.random.normal(0, 1, (inputs.size(0), args.latent_dim)))).cuda()\n",
    "        z_out = model(z)\n",
    "        inputs_z = inputs + z_out\n",
    "        \n",
    "        model_raw(inputs_z)\n",
    "        end = time.time()\n",
    "        for i, (inputs, target) in enumerate(val_loader):\n",
    "            if args.no_prefetcher:\n",
    "                target = target.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "\n",
    "            # synthesizing input + generator\n",
    "\n",
    "            # compute output\n",
    "            output, foward_list = model_raw(inputs_z)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output.data, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(acc1.item(), inputs.size(0))\n",
    "            top5.update(acc5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.log_freq == 0:\n",
    "                logging.info(\n",
    "                    'Test: [{0:>4d}/{1}]  '\n",
    "                    'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '\n",
    "                    'Acc@1: {top1.val:>7.3f} ({top1.avg:>7.3f})  '\n",
    "                    'Acc@5: {top5.val:>7.3f} ({top5.avg:>7.3f})'.format(\n",
    "                        i, len(val_loader), batch_time=batch_time,\n",
    "                        rate_avg=inputs.size(0) / batch_time.avg,\n",
    "                        loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    results = OrderedDict(\n",
    "        top1=round(top1.avg, 4), top1_err=round(100 - top1.avg, 4),\n",
    "        top5=round(top5.avg, 4), top5_err=round(100 - top5.avg, 4),\n",
    "        param_count=round(param_count / 1e6, 2),\n",
    "        img_size=data_config['input_size'][-1],\n",
    "        cropt_pct=crop_pct,\n",
    "        interpolation=data_config['interpolation'])\n",
    "\n",
    "    logging.info(' * Acc@1 {:.3f} ({:.3f}) Acc@5 {:.3f} ({:.3f})'.format(\n",
    "       results['top1'], results['top1_err'], results['top5'], results['top5_err']))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [   0/40036 (  0%)]  Loss: 68.870277 (68.8703)  Loss_l1:  0.024983 (0.0250)  Loss_traj: 68.745361 (68.7454)  Time: 19.191s,    1.67/s  (19.191s,    1.67/s)  LR: 1.000e-04  Data: 1.343 (1.343)\n",
      "INFO:root:Train: 0 [  50/40036 (  0%)]  Loss: 71.447937 (70.4086)  Loss_l1:  0.174618 (0.1011)  Loss_traj: 70.574844 (69.9030)  Time: 1.250s,   25.59/s  (1.638s,   19.54/s)  LR: 1.000e-04  Data: 0.014 (0.040)\n",
      "INFO:root:Train: 0 [ 100/40036 (  0%)]  Loss: 69.644737 (70.3646)  Loss_l1:  0.105872 (0.0924)  Loss_traj: 69.115372 (69.9025)  Time: 1.223s,   26.17/s  (1.465s,   21.85/s)  LR: 1.000e-04  Data: 0.015 (0.028)\n",
      "INFO:root:Train: 0 [ 150/40036 (  0%)]  Loss: 70.725746 (70.3518)  Loss_l1:  0.009817 (0.0946)  Loss_traj: 70.676659 (69.8787)  Time: 1.301s,   24.60/s  (1.405s,   22.77/s)  LR: 1.000e-04  Data: 0.014 (0.023)\n",
      "INFO:root:Train: 0 [ 200/40036 (  0%)]  Loss: 70.423164 (70.3899)  Loss_l1:  0.053527 (0.0930)  Loss_traj: 70.155525 (69.9251)  Time: 1.249s,   25.62/s  (1.374s,   23.29/s)  LR: 1.000e-04  Data: 0.014 (0.021)\n",
      "INFO:root:Train: 0 [ 250/40036 (  1%)]  Loss: 71.254326 (70.4405)  Loss_l1:  0.104577 (0.0996)  Loss_traj: 70.731445 (69.9424)  Time: 1.356s,   23.60/s  (1.356s,   23.60/s)  LR: 1.000e-04  Data: 0.015 (0.020)\n",
      "INFO:root:Train: 0 [ 300/40036 (  1%)]  Loss: 70.421005 (70.4584)  Loss_l1:  0.028834 (0.1001)  Loss_traj: 70.276840 (69.9577)  Time: 1.228s,   26.07/s  (1.344s,   23.81/s)  LR: 1.000e-04  Data: 0.014 (0.019)\n",
      "INFO:root:Train: 0 [ 350/40036 (  1%)]  Loss: 72.800255 (70.4846)  Loss_l1:  0.168756 (0.0985)  Loss_traj: 71.956474 (69.9921)  Time: 1.323s,   24.18/s  (1.336s,   23.96/s)  LR: 1.000e-04  Data: 0.014 (0.018)\n",
      "INFO:root:Train: 0 [ 400/40036 (  1%)]  Loss: 71.044373 (70.4513)  Loss_l1:  0.070075 (0.0979)  Loss_traj: 70.694000 (69.9616)  Time: 1.330s,   24.06/s  (1.329s,   24.07/s)  LR: 1.000e-04  Data: 0.014 (0.018)\n",
      "INFO:root:Train: 0 [ 450/40036 (  1%)]  Loss: 71.345924 (70.4130)  Loss_l1:  0.007782 (0.0957)  Loss_traj: 71.307014 (69.9343)  Time: 1.235s,   25.92/s  (1.323s,   24.18/s)  LR: 1.000e-04  Data: 0.014 (0.017)\n",
      "INFO:root:Train: 0 [ 500/40036 (  1%)]  Loss: 71.608620 (70.4086)  Loss_l1:  0.125025 (0.0957)  Loss_traj: 70.983490 (69.9298)  Time: 1.323s,   24.18/s  (1.320s,   24.25/s)  LR: 1.000e-04  Data: 0.014 (0.017)\n",
      "INFO:root:Train: 0 [ 550/40036 (  1%)]  Loss: 71.240471 (70.3974)  Loss_l1:  0.014895 (0.0960)  Loss_traj: 71.166000 (69.9173)  Time: 1.225s,   26.13/s  (1.316s,   24.32/s)  LR: 1.000e-04  Data: 0.014 (0.017)\n",
      "INFO:root:Train: 0 [ 600/40036 (  1%)]  Loss: 70.137016 (70.3901)  Loss_l1:  0.075897 (0.0956)  Loss_traj: 69.757530 (69.9119)  Time: 1.330s,   24.06/s  (1.314s,   24.36/s)  LR: 1.000e-04  Data: 0.014 (0.017)\n",
      "INFO:root:Train: 0 [ 650/40036 (  2%)]  Loss: 71.928391 (70.4151)  Loss_l1:  0.247078 (0.0972)  Loss_traj: 70.693001 (69.9289)  Time: 1.231s,   26.00/s  (1.312s,   24.39/s)  LR: 1.000e-04  Data: 0.015 (0.016)\n",
      "INFO:root:Train: 0 [ 700/40036 (  2%)]  Loss: 70.107742 (70.4219)  Loss_l1:  0.194328 (0.0986)  Loss_traj: 69.136101 (69.9288)  Time: 1.332s,   24.02/s  (1.310s,   24.42/s)  LR: 1.000e-04  Data: 0.015 (0.016)\n",
      "INFO:root:Train: 0 [ 750/40036 (  2%)]  Loss: 71.792259 (70.4275)  Loss_l1:  0.154291 (0.0992)  Loss_traj: 71.020805 (69.9315)  Time: 1.222s,   26.19/s  (1.308s,   24.46/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [ 800/40036 (  2%)]  Loss: 69.953781 (70.4305)  Loss_l1:  0.046282 (0.0986)  Loss_traj: 69.722374 (69.9375)  Time: 1.250s,   25.61/s  (1.307s,   24.49/s)  LR: 1.000e-04  Data: 0.022 (0.016)\n",
      "INFO:root:Train: 0 [ 850/40036 (  2%)]  Loss: 70.530609 (70.4275)  Loss_l1:  0.135927 (0.0984)  Loss_traj: 69.850975 (69.9355)  Time: 1.349s,   23.72/s  (1.306s,   24.51/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [ 900/40036 (  2%)]  Loss: 71.655220 (70.4256)  Loss_l1:  0.080640 (0.0989)  Loss_traj: 71.252014 (69.9313)  Time: 1.246s,   25.68/s  (1.304s,   24.53/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [ 950/40036 (  2%)]  Loss: 71.242805 (70.4342)  Loss_l1:  0.137879 (0.0992)  Loss_traj: 70.553406 (69.9384)  Time: 1.324s,   24.17/s  (1.303s,   24.55/s)  LR: 1.000e-04  Data: 0.015 (0.016)\n",
      "INFO:root:Train: 0 [1000/40036 (  2%)]  Loss: 72.064278 (70.4297)  Loss_l1:  0.064538 (0.0987)  Loss_traj: 71.741585 (69.9364)  Time: 1.225s,   26.11/s  (1.302s,   24.57/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [1050/40036 (  3%)]  Loss: 69.898537 (70.4382)  Loss_l1:  0.051528 (0.0998)  Loss_traj: 69.640900 (69.9392)  Time: 1.333s,   24.01/s  (1.302s,   24.58/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [1100/40036 (  3%)]  Loss: 71.353363 (70.4299)  Loss_l1:  0.328033 (0.0997)  Loss_traj: 69.713196 (69.9312)  Time: 1.298s,   24.65/s  (1.301s,   24.60/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [1150/40036 (  3%)]  Loss: 70.784592 (70.4199)  Loss_l1:  0.212890 (0.0995)  Loss_traj: 69.720146 (69.9223)  Time: 1.234s,   25.93/s  (1.300s,   24.61/s)  LR: 1.000e-04  Data: 0.015 (0.016)\n",
      "INFO:root:Train: 0 [1200/40036 (  3%)]  Loss: 69.678864 (70.4224)  Loss_l1:  0.007288 (0.1000)  Loss_traj: 69.642426 (69.9224)  Time: 1.298s,   24.65/s  (1.300s,   24.62/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [1250/40036 (  3%)]  Loss: 71.758232 (70.4223)  Loss_l1:  0.122187 (0.0997)  Loss_traj: 71.147293 (69.9239)  Time: 1.228s,   26.06/s  (1.299s,   24.63/s)  LR: 1.000e-04  Data: 0.014 (0.016)\n",
      "INFO:root:Train: 0 [1300/40036 (  3%)]  Loss: 68.752792 (70.4286)  Loss_l1:  0.048626 (0.1000)  Loss_traj: 68.509659 (69.9287)  Time: 1.319s,   24.25/s  (1.299s,   24.64/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1350/40036 (  3%)]  Loss: 72.144539 (70.4291)  Loss_l1:  0.071456 (0.0998)  Loss_traj: 71.787262 (69.9301)  Time: 1.252s,   25.56/s  (1.298s,   24.66/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1400/40036 (  3%)]  Loss: 69.861069 (70.4357)  Loss_l1:  0.074283 (0.1000)  Loss_traj: 69.489655 (69.9359)  Time: 1.323s,   24.18/s  (1.297s,   24.66/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1450/40036 (  4%)]  Loss: 71.145615 (70.4299)  Loss_l1:  0.206789 (0.1000)  Loss_traj: 70.111664 (69.9299)  Time: 1.318s,   24.29/s  (1.297s,   24.67/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1500/40036 (  4%)]  Loss: 70.246773 (70.4247)  Loss_l1:  0.130103 (0.1006)  Loss_traj: 69.596260 (69.9218)  Time: 1.232s,   25.97/s  (1.297s,   24.68/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1550/40036 (  4%)]  Loss: 70.136116 (70.4251)  Loss_l1:  0.079694 (0.1015)  Loss_traj: 69.737648 (69.9176)  Time: 1.313s,   24.38/s  (1.296s,   24.69/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1600/40036 (  4%)]  Loss: 70.921158 (70.4171)  Loss_l1:  0.156801 (0.1015)  Loss_traj: 70.137154 (69.9097)  Time: 1.236s,   25.90/s  (1.296s,   24.69/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1650/40036 (  4%)]  Loss: 71.412819 (70.4182)  Loss_l1:  0.106022 (0.1015)  Loss_traj: 70.882706 (69.9108)  Time: 1.329s,   24.08/s  (1.296s,   24.70/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1700/40036 (  4%)]  Loss: 71.794830 (70.4150)  Loss_l1:  0.193844 (0.1014)  Loss_traj: 70.825615 (69.9079)  Time: 1.262s,   25.37/s  (1.295s,   24.70/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1750/40036 (  4%)]  Loss: 69.738434 (70.4102)  Loss_l1:  0.033785 (0.1018)  Loss_traj: 69.569511 (69.9013)  Time: 1.352s,   23.66/s  (1.295s,   24.71/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1800/40036 (  4%)]  Loss: 72.467514 (70.4117)  Loss_l1:  0.241455 (0.1018)  Loss_traj: 71.260239 (69.9029)  Time: 1.304s,   24.54/s  (1.295s,   24.71/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1850/40036 (  5%)]  Loss: 69.712723 (70.4114)  Loss_l1:  0.090160 (0.1018)  Loss_traj: 69.261925 (69.9022)  Time: 1.255s,   25.49/s  (1.295s,   24.72/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1900/40036 (  5%)]  Loss: 72.509285 (70.4208)  Loss_l1:  0.171035 (0.1021)  Loss_traj: 71.654106 (69.9105)  Time: 1.347s,   23.76/s  (1.294s,   24.72/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [1950/40036 (  5%)]  Loss: 69.879723 (70.4206)  Loss_l1:  0.006825 (0.1021)  Loss_traj: 69.845596 (69.9102)  Time: 1.241s,   25.79/s  (1.294s,   24.73/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [2000/40036 (  5%)]  Loss: 70.980766 (70.4256)  Loss_l1:  0.071028 (0.1025)  Loss_traj: 70.625626 (69.9130)  Time: 1.375s,   23.28/s  (1.294s,   24.73/s)  LR: 1.000e-04  Data: 0.015 (0.015)\n",
      "INFO:root:Train: 0 [2050/40036 (  5%)]  Loss: 69.863304 (70.4288)  Loss_l1:  0.037090 (0.1024)  Loss_traj: 69.677849 (69.9167)  Time: 1.222s,   26.18/s  (1.294s,   24.73/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2100/40036 (  5%)]  Loss: 70.304855 (70.4262)  Loss_l1:  0.295635 (0.1026)  Loss_traj: 68.826683 (69.9134)  Time: 1.347s,   23.76/s  (1.294s,   24.74/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2150/40036 (  5%)]  Loss: 70.636070 (70.4240)  Loss_l1:  0.037179 (0.1028)  Loss_traj: 70.450172 (69.9101)  Time: 1.379s,   23.21/s  (1.293s,   24.74/s)  LR: 1.000e-04  Data: 0.015 (0.015)\n",
      "INFO:root:Train: 0 [2200/40036 (  5%)]  Loss: 71.165314 (70.4226)  Loss_l1:  0.097237 (0.1027)  Loss_traj: 70.679131 (69.9089)  Time: 1.244s,   25.73/s  (1.293s,   24.74/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2250/40036 (  6%)]  Loss: 70.243126 (70.4248)  Loss_l1:  0.168420 (0.1026)  Loss_traj: 69.401031 (69.9117)  Time: 1.374s,   23.29/s  (1.293s,   24.74/s)  LR: 1.000e-04  Data: 0.019 (0.015)\n",
      "INFO:root:Train: 0 [2300/40036 (  6%)]  Loss: 69.758942 (70.4238)  Loss_l1:  0.015925 (0.1030)  Loss_traj: 69.679314 (69.9088)  Time: 1.243s,   25.74/s  (1.293s,   24.74/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2350/40036 (  6%)]  Loss: 70.319679 (70.4242)  Loss_l1:  0.016495 (0.1029)  Loss_traj: 70.237206 (69.9095)  Time: 1.347s,   23.76/s  (1.293s,   24.75/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2400/40036 (  6%)]  Loss: 70.036552 (70.4228)  Loss_l1:  0.235267 (0.1029)  Loss_traj: 68.860214 (69.9084)  Time: 1.235s,   25.92/s  (1.293s,   24.75/s)  LR: 1.000e-04  Data: 0.015 (0.015)\n",
      "INFO:root:Train: 0 [2450/40036 (  6%)]  Loss: 69.511421 (70.4232)  Loss_l1:  0.146914 (0.1033)  Loss_traj: 68.776848 (69.9065)  Time: 1.340s,   23.89/s  (1.293s,   24.75/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2500/40036 (  6%)]  Loss: 70.582481 (70.4237)  Loss_l1:  0.096278 (0.1035)  Loss_traj: 70.101089 (69.9063)  Time: 1.364s,   23.45/s  (1.293s,   24.75/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2550/40036 (  6%)]  Loss: 70.090218 (70.4213)  Loss_l1:  0.113680 (0.1034)  Loss_traj: 69.521820 (69.9043)  Time: 1.240s,   25.80/s  (1.293s,   24.75/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2600/40036 (  6%)]  Loss: 68.983177 (70.4241)  Loss_l1:  0.075804 (0.1034)  Loss_traj: 68.604156 (69.9073)  Time: 1.303s,   24.56/s  (1.293s,   24.76/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2650/40036 (  7%)]  Loss: 69.228165 (70.4268)  Loss_l1:  0.054005 (0.1036)  Loss_traj: 68.958138 (69.9086)  Time: 1.242s,   25.76/s  (1.292s,   24.76/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2700/40036 (  7%)]  Loss: 71.476166 (70.4270)  Loss_l1:  0.346244 (0.1035)  Loss_traj: 69.744942 (69.9094)  Time: 1.331s,   24.05/s  (1.292s,   24.76/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2750/40036 (  7%)]  Loss: 70.045273 (70.4249)  Loss_l1:  0.094882 (0.1031)  Loss_traj: 69.570862 (69.9093)  Time: 1.224s,   26.15/s  (1.292s,   24.76/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2800/40036 (  7%)]  Loss: 69.359306 (70.4241)  Loss_l1:  0.126888 (0.1033)  Loss_traj: 68.724869 (69.9077)  Time: 1.327s,   24.11/s  (1.292s,   24.76/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2850/40036 (  7%)]  Loss: 70.046364 (70.4276)  Loss_l1:  0.050985 (0.1034)  Loss_traj: 69.791443 (69.9108)  Time: 1.326s,   24.13/s  (1.292s,   24.77/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2900/40036 (  7%)]  Loss: 71.771561 (70.4270)  Loss_l1:  0.154203 (0.1032)  Loss_traj: 71.000542 (69.9113)  Time: 1.254s,   25.52/s  (1.292s,   24.77/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [2950/40036 (  7%)]  Loss: 70.983315 (70.4262)  Loss_l1:  0.166807 (0.1031)  Loss_traj: 70.149277 (69.9109)  Time: 1.338s,   23.92/s  (1.292s,   24.77/s)  LR: 1.000e-04  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 0 [3000/40036 (  7%)]  Loss: 70.972809 (70.4246)  Loss_l1:  0.171188 (0.1032)  Loss_traj: 70.116867 (69.9085)  Time: 1.249s,   25.62/s  (1.292s,   24.77/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3050/40036 (  8%)]  Loss: 71.480324 (70.4309)  Loss_l1:  0.262712 (0.1033)  Loss_traj: 70.166763 (69.9145)  Time: 1.335s,   23.97/s  (1.292s,   24.78/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3100/40036 (  8%)]  Loss: 70.962929 (70.4320)  Loss_l1:  0.084035 (0.1031)  Loss_traj: 70.542755 (69.9164)  Time: 1.229s,   26.05/s  (1.291s,   24.78/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3150/40036 (  8%)]  Loss: 70.486725 (70.4328)  Loss_l1:  0.019972 (0.1031)  Loss_traj: 70.386864 (69.9172)  Time: 1.330s,   24.06/s  (1.291s,   24.78/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3200/40036 (  8%)]  Loss: 69.867737 (70.4334)  Loss_l1:  0.170369 (0.1030)  Loss_traj: 69.015892 (69.9184)  Time: 1.306s,   24.51/s  (1.291s,   24.78/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3250/40036 (  8%)]  Loss: 69.096138 (70.4333)  Loss_l1:  0.021925 (0.1029)  Loss_traj: 68.986511 (69.9190)  Time: 1.234s,   25.93/s  (1.291s,   24.79/s)  LR: 1.000e-04  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 0 [3300/40036 (  8%)]  Loss: 69.960320 (70.4375)  Loss_l1:  0.069511 (0.1029)  Loss_traj: 69.612770 (69.9229)  Time: 1.284s,   24.91/s  (1.291s,   24.79/s)  LR: 1.000e-04  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 0 [3350/40036 (  8%)]  Loss: 69.389984 (70.4376)  Loss_l1:  0.037809 (0.1026)  Loss_traj: 69.200943 (69.9248)  Time: 1.224s,   26.15/s  (1.291s,   24.79/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3400/40036 (  8%)]  Loss: 72.159805 (70.4414)  Loss_l1:  0.010046 (0.1026)  Loss_traj: 72.109581 (69.9283)  Time: 1.332s,   24.02/s  (1.291s,   24.79/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3450/40036 (  9%)]  Loss: 68.635330 (70.4430)  Loss_l1:  0.005607 (0.1027)  Loss_traj: 68.607292 (69.9294)  Time: 1.245s,   25.70/s  (1.291s,   24.79/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3500/40036 (  9%)]  Loss: 70.980171 (70.4469)  Loss_l1:  0.048872 (0.1028)  Loss_traj: 70.735809 (69.9330)  Time: 1.341s,   23.85/s  (1.291s,   24.80/s)  LR: 1.000e-04  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 0 [3550/40036 (  9%)]  Loss: 69.945084 (70.4473)  Loss_l1:  0.088849 (0.1028)  Loss_traj: 69.500839 (69.9334)  Time: 1.322s,   24.21/s  (1.290s,   24.80/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3600/40036 (  9%)]  Loss: 71.408646 (70.4505)  Loss_l1:  0.047782 (0.1028)  Loss_traj: 71.169731 (69.9364)  Time: 1.244s,   25.72/s  (1.290s,   24.80/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3650/40036 (  9%)]  Loss: 72.222931 (70.4504)  Loss_l1:  0.075051 (0.1029)  Loss_traj: 71.847679 (69.9357)  Time: 1.340s,   23.88/s  (1.290s,   24.80/s)  LR: 1.000e-04  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 0 [3700/40036 (  9%)]  Loss: 71.069252 (70.4499)  Loss_l1:  0.193738 (0.1028)  Loss_traj: 70.100563 (69.9359)  Time: 1.219s,   26.24/s  (1.290s,   24.80/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [3750/40036 (  9%)]  Loss: 70.651466 (70.4499)  Loss_l1:  0.146248 (0.1028)  Loss_traj: 69.920227 (69.9358)  Time: 1.309s,   24.45/s  (1.290s,   24.81/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [3800/40036 (  9%)]  Loss: 70.384483 (70.4490)  Loss_l1:  0.072238 (0.1026)  Loss_traj: 70.023293 (69.9360)  Time: 1.220s,   26.24/s  (1.290s,   24.81/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [3850/40036 ( 10%)]  Loss: 69.225418 (70.4472)  Loss_l1:  0.020431 (0.1025)  Loss_traj: 69.123260 (69.9346)  Time: 1.368s,   23.40/s  (1.290s,   24.81/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [3900/40036 ( 10%)]  Loss: 70.249611 (70.4472)  Loss_l1:  0.180053 (0.1026)  Loss_traj: 69.349350 (69.9341)  Time: 1.291s,   24.79/s  (1.290s,   24.81/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [3950/40036 ( 10%)]  Loss: 69.070259 (70.4455)  Loss_l1:  0.114011 (0.1026)  Loss_traj: 68.500198 (69.9326)  Time: 1.226s,   26.11/s  (1.290s,   24.81/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [4000/40036 ( 10%)]  Loss: 68.898170 (70.4463)  Loss_l1:  0.062003 (0.1025)  Loss_traj: 68.588158 (69.9339)  Time: 1.293s,   24.75/s  (1.289s,   24.82/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [4050/40036 ( 10%)]  Loss: 68.857903 (70.4467)  Loss_l1:  0.051381 (0.1025)  Loss_traj: 68.600998 (69.9340)  Time: 1.239s,   25.84/s  (1.289s,   24.82/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4100/40036 ( 10%)]  Loss: 70.472527 (70.4484)  Loss_l1:  0.022101 (0.1027)  Loss_traj: 70.362022 (69.9348)  Time: 1.380s,   23.19/s  (1.289s,   24.82/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4150/40036 ( 10%)]  Loss: 70.573898 (70.4512)  Loss_l1:  0.033234 (0.1025)  Loss_traj: 70.407730 (69.9385)  Time: 1.249s,   25.62/s  (1.289s,   24.82/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4200/40036 ( 10%)]  Loss: 69.866699 (70.4522)  Loss_l1:  0.276013 (0.1027)  Loss_traj: 68.486633 (69.9387)  Time: 1.342s,   23.84/s  (1.289s,   24.82/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4250/40036 ( 11%)]  Loss: 70.556313 (70.4496)  Loss_l1:  0.097857 (0.1026)  Loss_traj: 70.067024 (69.9366)  Time: 1.280s,   25.00/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [4300/40036 ( 11%)]  Loss: 71.467606 (70.4512)  Loss_l1:  0.399021 (0.1030)  Loss_traj: 69.472504 (69.9360)  Time: 1.273s,   25.14/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4350/40036 ( 11%)]  Loss: 70.550354 (70.4507)  Loss_l1:  0.013988 (0.1029)  Loss_traj: 70.480415 (69.9363)  Time: 1.304s,   24.53/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [4400/40036 ( 11%)]  Loss: 69.958321 (70.4495)  Loss_l1:  0.070554 (0.1029)  Loss_traj: 69.605553 (69.9353)  Time: 1.233s,   25.95/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [4450/40036 ( 11%)]  Loss: 71.170029 (70.4467)  Loss_l1:  0.203960 (0.1029)  Loss_traj: 70.150223 (69.9323)  Time: 1.327s,   24.11/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4500/40036 ( 11%)]  Loss: 72.355263 (70.4492)  Loss_l1:  0.373943 (0.1029)  Loss_traj: 70.485550 (69.9346)  Time: 1.240s,   25.81/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4550/40036 ( 11%)]  Loss: 72.015526 (70.4495)  Loss_l1:  0.109240 (0.1031)  Loss_traj: 71.469322 (69.9339)  Time: 1.330s,   24.07/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4600/40036 ( 11%)]  Loss: 71.037979 (70.4510)  Loss_l1:  0.000160 (0.1031)  Loss_traj: 71.037178 (69.9353)  Time: 1.233s,   25.95/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4650/40036 ( 12%)]  Loss: 71.436340 (70.4517)  Loss_l1:  0.329246 (0.1031)  Loss_traj: 69.790108 (69.9361)  Time: 1.226s,   26.10/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4700/40036 ( 12%)]  Loss: 71.680237 (70.4496)  Loss_l1:  0.052212 (0.1032)  Loss_traj: 71.419174 (69.9335)  Time: 1.319s,   24.26/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [4750/40036 ( 12%)]  Loss: 70.522102 (70.4497)  Loss_l1:  0.004236 (0.1035)  Loss_traj: 70.500923 (69.9323)  Time: 1.238s,   25.84/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4800/40036 ( 12%)]  Loss: 71.147934 (70.4481)  Loss_l1:  0.047286 (0.1036)  Loss_traj: 70.911507 (69.9302)  Time: 1.339s,   23.90/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [4850/40036 ( 12%)]  Loss: 69.906197 (70.4483)  Loss_l1:  0.060540 (0.1035)  Loss_traj: 69.603500 (69.9307)  Time: 1.241s,   25.79/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [4900/40036 ( 12%)]  Loss: 70.191681 (70.4510)  Loss_l1:  0.111456 (0.1036)  Loss_traj: 69.634399 (69.9331)  Time: 1.327s,   24.12/s  (1.289s,   24.83/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [4950/40036 ( 12%)]  Loss: 72.363297 (70.4524)  Loss_l1:  0.069615 (0.1036)  Loss_traj: 72.015221 (69.9345)  Time: 1.225s,   26.12/s  (1.289s,   24.84/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [5000/40036 ( 12%)]  Loss: 70.472961 (70.4543)  Loss_l1:  0.061667 (0.1036)  Loss_traj: 70.164627 (69.9363)  Time: 1.231s,   25.99/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5050/40036 ( 13%)]  Loss: 69.420517 (70.4551)  Loss_l1:  0.057898 (0.1037)  Loss_traj: 69.131027 (69.9368)  Time: 1.338s,   23.92/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.018 (0.014)\n",
      "INFO:root:Train: 0 [5100/40036 ( 13%)]  Loss: 71.171463 (70.4546)  Loss_l1:  0.073646 (0.1035)  Loss_traj: 70.803230 (69.9370)  Time: 1.251s,   25.57/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5150/40036 ( 13%)]  Loss: 70.047577 (70.4522)  Loss_l1:  0.069525 (0.1035)  Loss_traj: 69.699951 (69.9348)  Time: 1.317s,   24.30/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [5200/40036 ( 13%)]  Loss: 71.612144 (70.4536)  Loss_l1:  0.082991 (0.1036)  Loss_traj: 71.197189 (69.9354)  Time: 1.226s,   26.10/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [5250/40036 ( 13%)]  Loss: 71.923973 (70.4553)  Loss_l1:  0.247941 (0.1035)  Loss_traj: 70.684265 (69.9377)  Time: 1.365s,   23.45/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5300/40036 ( 13%)]  Loss: 70.847076 (70.4574)  Loss_l1:  0.049604 (0.1034)  Loss_traj: 70.599060 (69.9402)  Time: 1.324s,   24.17/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5350/40036 ( 13%)]  Loss: 69.778755 (70.4557)  Loss_l1:  0.149032 (0.1034)  Loss_traj: 69.033600 (69.9386)  Time: 1.262s,   25.36/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5400/40036 ( 13%)]  Loss: 71.680504 (70.4562)  Loss_l1:  0.071081 (0.1033)  Loss_traj: 71.325096 (69.9396)  Time: 1.312s,   24.39/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5450/40036 ( 14%)]  Loss: 69.425285 (70.4565)  Loss_l1:  0.103496 (0.1035)  Loss_traj: 68.907806 (69.9390)  Time: 1.238s,   25.85/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5500/40036 ( 14%)]  Loss: 70.272789 (70.4553)  Loss_l1:  0.042519 (0.1035)  Loss_traj: 70.060188 (69.9380)  Time: 1.324s,   24.18/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5550/40036 ( 14%)]  Loss: 71.755623 (70.4566)  Loss_l1:  0.150914 (0.1036)  Loss_traj: 71.001053 (69.9385)  Time: 1.222s,   26.18/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [5600/40036 ( 14%)]  Loss: 70.752983 (70.4560)  Loss_l1:  0.066111 (0.1037)  Loss_traj: 70.422424 (69.9375)  Time: 1.319s,   24.26/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5650/40036 ( 14%)]  Loss: 69.632790 (70.4527)  Loss_l1:  0.071111 (0.1037)  Loss_traj: 69.277237 (69.9343)  Time: 1.350s,   23.71/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [5700/40036 ( 14%)]  Loss: 69.831329 (70.4522)  Loss_l1:  0.130178 (0.1035)  Loss_traj: 69.180443 (69.9346)  Time: 1.238s,   25.84/s  (1.288s,   24.84/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5750/40036 ( 14%)]  Loss: 73.291313 (70.4506)  Loss_l1:  0.076939 (0.1033)  Loss_traj: 72.906616 (69.9343)  Time: 1.321s,   24.23/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [5800/40036 ( 14%)]  Loss: 69.552139 (70.4523)  Loss_l1:  0.112905 (0.1033)  Loss_traj: 68.987617 (69.9356)  Time: 1.227s,   26.08/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5850/40036 ( 15%)]  Loss: 70.859764 (70.4526)  Loss_l1:  0.040515 (0.1032)  Loss_traj: 70.657188 (69.9366)  Time: 1.326s,   24.14/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [5900/40036 ( 15%)]  Loss: 70.636475 (70.4519)  Loss_l1:  0.156448 (0.1033)  Loss_traj: 69.854233 (69.9356)  Time: 1.222s,   26.19/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [5950/40036 ( 15%)]  Loss: 68.533607 (70.4530)  Loss_l1:  0.003495 (0.1033)  Loss_traj: 68.516129 (69.9363)  Time: 1.321s,   24.22/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [6000/40036 ( 15%)]  Loss: 69.926247 (70.4542)  Loss_l1:  0.067334 (0.1035)  Loss_traj: 69.589577 (69.9366)  Time: 1.282s,   24.97/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6050/40036 ( 15%)]  Loss: 68.964691 (70.4547)  Loss_l1:  0.028242 (0.1034)  Loss_traj: 68.823479 (69.9376)  Time: 1.238s,   25.84/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [6100/40036 ( 15%)]  Loss: 70.186562 (70.4543)  Loss_l1:  0.046535 (0.1035)  Loss_traj: 69.953888 (69.9368)  Time: 1.348s,   23.74/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6150/40036 ( 15%)]  Loss: 70.583466 (70.4549)  Loss_l1:  0.040579 (0.1036)  Loss_traj: 70.380569 (69.9370)  Time: 1.226s,   26.10/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.018 (0.014)\n",
      "INFO:root:Train: 0 [6200/40036 ( 15%)]  Loss: 69.150772 (70.4555)  Loss_l1:  0.053282 (0.1038)  Loss_traj: 68.884361 (69.9366)  Time: 1.338s,   23.92/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6250/40036 ( 16%)]  Loss: 71.201477 (70.4551)  Loss_l1:  0.111987 (0.1037)  Loss_traj: 70.641541 (69.9368)  Time: 1.247s,   25.67/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6300/40036 ( 16%)]  Loss: 69.975578 (70.4546)  Loss_l1:  0.190239 (0.1036)  Loss_traj: 69.024384 (69.9366)  Time: 1.373s,   23.31/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6350/40036 ( 16%)]  Loss: 69.965584 (70.4528)  Loss_l1:  0.034288 (0.1035)  Loss_traj: 69.794144 (69.9353)  Time: 1.244s,   25.72/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [6400/40036 ( 16%)]  Loss: 71.575516 (70.4531)  Loss_l1:  0.193162 (0.1036)  Loss_traj: 70.609703 (69.9350)  Time: 1.224s,   26.15/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [6450/40036 ( 16%)]  Loss: 70.866333 (70.4536)  Loss_l1:  0.066960 (0.1034)  Loss_traj: 70.531532 (69.9364)  Time: 1.328s,   24.10/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'sRGB' 41 1\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'gAMA' 54 4\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'cHRM' 70 32\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 114 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 135 65401\n",
      "INFO:root:Train: 0 [6500/40036 ( 16%)]  Loss: 70.358070 (70.4536)  Loss_l1:  0.112419 (0.1034)  Loss_traj: 69.795975 (69.9366)  Time: 1.262s,   25.35/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6550/40036 ( 16%)]  Loss: 70.564369 (70.4525)  Loss_l1:  0.156296 (0.1033)  Loss_traj: 69.782890 (69.9358)  Time: 1.329s,   24.07/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6600/40036 ( 16%)]  Loss: 70.649887 (70.4535)  Loss_l1:  0.142936 (0.1034)  Loss_traj: 69.935204 (69.9364)  Time: 1.247s,   25.66/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6650/40036 ( 17%)]  Loss: 70.609848 (70.4544)  Loss_l1:  0.016793 (0.1035)  Loss_traj: 70.525879 (69.9367)  Time: 1.318s,   24.28/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [6700/40036 ( 17%)]  Loss: 70.147697 (70.4544)  Loss_l1:  0.108518 (0.1035)  Loss_traj: 69.605110 (69.9370)  Time: 1.337s,   23.94/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6750/40036 ( 17%)]  Loss: 70.968124 (70.4547)  Loss_l1:  0.144979 (0.1036)  Loss_traj: 70.243225 (69.9369)  Time: 1.245s,   25.70/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6800/40036 ( 17%)]  Loss: 72.067230 (70.4552)  Loss_l1:  0.154140 (0.1035)  Loss_traj: 71.296532 (69.9379)  Time: 1.291s,   24.78/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [6850/40036 ( 17%)]  Loss: 71.692520 (70.4546)  Loss_l1:  0.296824 (0.1034)  Loss_traj: 70.208405 (69.9376)  Time: 1.230s,   26.02/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6900/40036 ( 17%)]  Loss: 69.772789 (70.4566)  Loss_l1:  0.005575 (0.1035)  Loss_traj: 69.744919 (69.9389)  Time: 1.353s,   23.65/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [6950/40036 ( 17%)]  Loss: 70.784912 (70.4580)  Loss_l1:  0.180937 (0.1036)  Loss_traj: 69.880226 (69.9398)  Time: 1.224s,   26.14/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7000/40036 ( 17%)]  Loss: 70.305733 (70.4585)  Loss_l1:  0.079363 (0.1038)  Loss_traj: 69.908920 (69.9397)  Time: 1.336s,   23.95/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7050/40036 ( 18%)]  Loss: 70.803871 (70.4585)  Loss_l1:  0.131317 (0.1036)  Loss_traj: 70.147285 (69.9402)  Time: 1.289s,   24.83/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7100/40036 ( 18%)]  Loss: 68.660309 (70.4586)  Loss_l1:  0.112126 (0.1036)  Loss_traj: 68.099678 (69.9405)  Time: 1.263s,   25.33/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [7150/40036 ( 18%)]  Loss: 70.841972 (70.4594)  Loss_l1:  0.203855 (0.1035)  Loss_traj: 69.822693 (69.9419)  Time: 1.286s,   24.87/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7200/40036 ( 18%)]  Loss: 70.981430 (70.4595)  Loss_l1:  0.174696 (0.1034)  Loss_traj: 70.107948 (69.9423)  Time: 1.228s,   26.07/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7250/40036 ( 18%)]  Loss: 70.126266 (70.4587)  Loss_l1:  0.110390 (0.1035)  Loss_traj: 69.574318 (69.9411)  Time: 1.342s,   23.84/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7300/40036 ( 18%)]  Loss: 71.107010 (70.4600)  Loss_l1:  0.123724 (0.1035)  Loss_traj: 70.488388 (69.9425)  Time: 1.235s,   25.91/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7350/40036 ( 18%)]  Loss: 69.331566 (70.4605)  Loss_l1:  0.126865 (0.1036)  Loss_traj: 68.697243 (69.9424)  Time: 1.317s,   24.30/s  (1.288s,   24.85/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7400/40036 ( 18%)]  Loss: 72.510780 (70.4622)  Loss_l1:  0.125362 (0.1036)  Loss_traj: 71.883972 (69.9443)  Time: 1.294s,   24.73/s  (1.287s,   24.85/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7450/40036 ( 19%)]  Loss: 70.210648 (70.4603)  Loss_l1:  0.102618 (0.1036)  Loss_traj: 69.697556 (69.9425)  Time: 1.216s,   26.32/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7500/40036 ( 19%)]  Loss: 69.921890 (70.4614)  Loss_l1:  0.035913 (0.1036)  Loss_traj: 69.742325 (69.9435)  Time: 1.353s,   23.64/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7550/40036 ( 19%)]  Loss: 70.078346 (70.4621)  Loss_l1:  0.076595 (0.1036)  Loss_traj: 69.695366 (69.9442)  Time: 1.262s,   25.36/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7600/40036 ( 19%)]  Loss: 71.774193 (70.4625)  Loss_l1:  0.332466 (0.1035)  Loss_traj: 70.111862 (69.9448)  Time: 1.380s,   23.19/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7650/40036 ( 19%)]  Loss: 70.208061 (70.4622)  Loss_l1:  0.005492 (0.1035)  Loss_traj: 70.180603 (69.9446)  Time: 1.237s,   25.86/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7700/40036 ( 19%)]  Loss: 71.371758 (70.4617)  Loss_l1:  0.014285 (0.1034)  Loss_traj: 71.300331 (69.9446)  Time: 1.320s,   24.24/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7750/40036 ( 19%)]  Loss: 71.137085 (70.4620)  Loss_l1:  0.195120 (0.1034)  Loss_traj: 70.161484 (69.9452)  Time: 1.293s,   24.75/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [7800/40036 ( 19%)]  Loss: 69.685226 (70.4641)  Loss_l1:  0.009037 (0.1035)  Loss_traj: 69.640038 (69.9466)  Time: 1.236s,   25.89/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7850/40036 ( 20%)]  Loss: 69.797539 (70.4638)  Loss_l1:  0.017599 (0.1036)  Loss_traj: 69.709541 (69.9459)  Time: 1.310s,   24.44/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [7900/40036 ( 20%)]  Loss: 69.557503 (70.4635)  Loss_l1:  0.167725 (0.1036)  Loss_traj: 68.718880 (69.9456)  Time: 1.223s,   26.16/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [7950/40036 ( 20%)]  Loss: 70.302559 (70.4619)  Loss_l1:  0.002479 (0.1036)  Loss_traj: 70.290161 (69.9439)  Time: 1.343s,   23.83/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8000/40036 ( 20%)]  Loss: 69.737579 (70.4620)  Loss_l1:  0.033728 (0.1037)  Loss_traj: 69.568939 (69.9436)  Time: 1.215s,   26.34/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8050/40036 ( 20%)]  Loss: 69.684685 (70.4609)  Loss_l1:  0.054700 (0.1036)  Loss_traj: 69.411179 (69.9428)  Time: 1.336s,   23.95/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8100/40036 ( 20%)]  Loss: 71.305168 (70.4609)  Loss_l1:  0.057690 (0.1036)  Loss_traj: 71.016716 (69.9427)  Time: 1.315s,   24.33/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8150/40036 ( 20%)]  Loss: 70.273941 (70.4595)  Loss_l1:  0.126598 (0.1036)  Loss_traj: 69.640953 (69.9415)  Time: 1.250s,   25.59/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8200/40036 ( 20%)]  Loss: 69.131340 (70.4589)  Loss_l1:  0.048607 (0.1036)  Loss_traj: 68.888306 (69.9410)  Time: 1.313s,   24.37/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8250/40036 ( 21%)]  Loss: 72.285141 (70.4592)  Loss_l1:  0.173492 (0.1035)  Loss_traj: 71.417679 (69.9415)  Time: 1.217s,   26.30/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8300/40036 ( 21%)]  Loss: 69.978065 (70.4586)  Loss_l1:  0.072335 (0.1035)  Loss_traj: 69.616394 (69.9411)  Time: 1.349s,   23.73/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8350/40036 ( 21%)]  Loss: 70.909149 (70.4594)  Loss_l1:  0.037658 (0.1036)  Loss_traj: 70.720856 (69.9414)  Time: 1.241s,   25.78/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8400/40036 ( 21%)]  Loss: 70.899940 (70.4592)  Loss_l1:  0.094362 (0.1036)  Loss_traj: 70.428131 (69.9414)  Time: 1.357s,   23.58/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8450/40036 ( 21%)]  Loss: 70.122536 (70.4604)  Loss_l1:  0.013563 (0.1036)  Loss_traj: 70.054726 (69.9423)  Time: 1.315s,   24.33/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8500/40036 ( 21%)]  Loss: 70.497826 (70.4618)  Loss_l1:  0.149955 (0.1037)  Loss_traj: 69.748055 (69.9435)  Time: 1.228s,   26.05/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8550/40036 ( 21%)]  Loss: 70.978203 (70.4612)  Loss_l1:  0.049123 (0.1036)  Loss_traj: 70.732590 (69.9433)  Time: 1.299s,   24.63/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8600/40036 ( 21%)]  Loss: 70.566711 (70.4612)  Loss_l1:  0.047820 (0.1037)  Loss_traj: 70.327614 (69.9428)  Time: 1.253s,   25.53/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8650/40036 ( 22%)]  Loss: 70.784813 (70.4616)  Loss_l1:  0.133281 (0.1037)  Loss_traj: 70.118408 (69.9429)  Time: 1.319s,   24.26/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8700/40036 ( 22%)]  Loss: 70.122704 (70.4617)  Loss_l1:  0.222873 (0.1039)  Loss_traj: 69.008339 (69.9425)  Time: 1.248s,   25.65/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8750/40036 ( 22%)]  Loss: 72.483910 (70.4605)  Loss_l1:  0.222743 (0.1037)  Loss_traj: 71.370193 (69.9419)  Time: 1.314s,   24.35/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8800/40036 ( 22%)]  Loss: 69.953468 (70.4616)  Loss_l1:  0.107375 (0.1038)  Loss_traj: 69.416595 (69.9425)  Time: 1.247s,   25.66/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [8850/40036 ( 22%)]  Loss: 70.163834 (70.4608)  Loss_l1:  0.068520 (0.1038)  Loss_traj: 69.821236 (69.9420)  Time: 1.254s,   25.51/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8900/40036 ( 22%)]  Loss: 70.534378 (70.4620)  Loss_l1:  0.041849 (0.1036)  Loss_traj: 70.325134 (69.9442)  Time: 1.279s,   25.02/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [8950/40036 ( 22%)]  Loss: 67.415565 (70.4605)  Loss_l1:  0.001326 (0.1035)  Loss_traj: 67.408936 (69.9432)  Time: 1.241s,   25.79/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9000/40036 ( 22%)]  Loss: 71.311989 (70.4615)  Loss_l1:  0.098422 (0.1035)  Loss_traj: 70.819878 (69.9441)  Time: 1.334s,   23.98/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9050/40036 ( 23%)]  Loss: 69.437538 (70.4622)  Loss_l1:  0.016178 (0.1034)  Loss_traj: 69.356644 (69.9453)  Time: 1.249s,   25.62/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9100/40036 ( 23%)]  Loss: 68.410538 (70.4628)  Loss_l1:  0.076804 (0.1034)  Loss_traj: 68.026520 (69.9456)  Time: 1.346s,   23.77/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9150/40036 ( 23%)]  Loss: 71.327049 (70.4647)  Loss_l1:  0.150093 (0.1035)  Loss_traj: 70.576584 (69.9471)  Time: 1.386s,   23.09/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [9200/40036 ( 23%)]  Loss: 68.242630 (70.4646)  Loss_l1:  0.011482 (0.1036)  Loss_traj: 68.185219 (69.9467)  Time: 1.251s,   25.58/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9250/40036 ( 23%)]  Loss: 69.390961 (70.4655)  Loss_l1:  0.069916 (0.1036)  Loss_traj: 69.041382 (69.9475)  Time: 1.300s,   24.62/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9300/40036 ( 23%)]  Loss: 72.510750 (70.4649)  Loss_l1:  0.073159 (0.1037)  Loss_traj: 72.144958 (69.9465)  Time: 1.222s,   26.18/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9350/40036 ( 23%)]  Loss: 70.485596 (70.4665)  Loss_l1:  0.208672 (0.1037)  Loss_traj: 69.442238 (69.9479)  Time: 1.331s,   24.03/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9400/40036 ( 23%)]  Loss: 69.732613 (70.4656)  Loss_l1:  0.096559 (0.1038)  Loss_traj: 69.249817 (69.9467)  Time: 1.244s,   25.73/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9450/40036 ( 24%)]  Loss: 69.402794 (70.4647)  Loss_l1:  0.028341 (0.1038)  Loss_traj: 69.261093 (69.9457)  Time: 1.328s,   24.09/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9500/40036 ( 24%)]  Loss: 70.010551 (70.4645)  Loss_l1:  0.220673 (0.1039)  Loss_traj: 68.907188 (69.9452)  Time: 1.343s,   23.83/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9550/40036 ( 24%)]  Loss: 69.884048 (70.4651)  Loss_l1:  0.281290 (0.1039)  Loss_traj: 68.477600 (69.9456)  Time: 1.246s,   25.68/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9600/40036 ( 24%)]  Loss: 72.986526 (70.4664)  Loss_l1:  0.233405 (0.1040)  Loss_traj: 71.819504 (69.9466)  Time: 1.322s,   24.21/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9650/40036 ( 24%)]  Loss: 70.066711 (70.4660)  Loss_l1:  0.043210 (0.1039)  Loss_traj: 69.850662 (69.9467)  Time: 1.248s,   25.64/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [9700/40036 ( 24%)]  Loss: 70.952713 (70.4656)  Loss_l1:  0.026998 (0.1039)  Loss_traj: 70.817719 (69.9462)  Time: 1.322s,   24.20/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [9750/40036 ( 24%)]  Loss: 69.856926 (70.4649)  Loss_l1:  0.029785 (0.1039)  Loss_traj: 69.708000 (69.9456)  Time: 1.244s,   25.73/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9800/40036 ( 24%)]  Loss: 71.247215 (70.4646)  Loss_l1:  0.176879 (0.1038)  Loss_traj: 70.362823 (69.9454)  Time: 1.330s,   24.06/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9850/40036 ( 25%)]  Loss: 69.823845 (70.4636)  Loss_l1:  0.105173 (0.1038)  Loss_traj: 69.297981 (69.9448)  Time: 1.297s,   24.67/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [9900/40036 ( 25%)]  Loss: 69.633438 (70.4637)  Loss_l1:  0.063480 (0.1038)  Loss_traj: 69.316040 (69.9445)  Time: 1.223s,   26.17/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [9950/40036 ( 25%)]  Loss: 69.340683 (70.4626)  Loss_l1:  0.022565 (0.1038)  Loss_traj: 69.227859 (69.9434)  Time: 1.337s,   23.94/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10000/40036 ( 25%)]  Loss: 69.620667 (70.4620)  Loss_l1:  0.191653 (0.1039)  Loss_traj: 68.662399 (69.9426)  Time: 1.245s,   25.71/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10050/40036 ( 25%)]  Loss: 71.997658 (70.4624)  Loss_l1:  0.261016 (0.1039)  Loss_traj: 70.692581 (69.9428)  Time: 1.341s,   23.87/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10100/40036 ( 25%)]  Loss: 71.608521 (70.4611)  Loss_l1:  0.061405 (0.1039)  Loss_traj: 71.301498 (69.9416)  Time: 1.233s,   25.94/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10150/40036 ( 25%)]  Loss: 71.085670 (70.4606)  Loss_l1:  0.160523 (0.1038)  Loss_traj: 70.283051 (69.9415)  Time: 1.318s,   24.29/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10200/40036 ( 25%)]  Loss: 71.932732 (70.4609)  Loss_l1:  0.147283 (0.1038)  Loss_traj: 71.196312 (69.9419)  Time: 1.309s,   24.45/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10250/40036 ( 26%)]  Loss: 71.079170 (70.4605)  Loss_l1:  0.001953 (0.1038)  Loss_traj: 71.069405 (69.9414)  Time: 1.228s,   26.05/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10300/40036 ( 26%)]  Loss: 70.706261 (70.4609)  Loss_l1:  0.106851 (0.1039)  Loss_traj: 70.172005 (69.9414)  Time: 1.312s,   24.39/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10350/40036 ( 26%)]  Loss: 69.920723 (70.4608)  Loss_l1:  0.249980 (0.1039)  Loss_traj: 68.670822 (69.9411)  Time: 1.222s,   26.19/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10400/40036 ( 26%)]  Loss: 69.086586 (70.4610)  Loss_l1:  0.082474 (0.1040)  Loss_traj: 68.674217 (69.9412)  Time: 1.318s,   24.28/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10450/40036 ( 26%)]  Loss: 70.533920 (70.4611)  Loss_l1:  0.003972 (0.1039)  Loss_traj: 70.514061 (69.9414)  Time: 1.246s,   25.68/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10500/40036 ( 26%)]  Loss: 69.711578 (70.4604)  Loss_l1:  0.133556 (0.1040)  Loss_traj: 69.043800 (69.9402)  Time: 1.321s,   24.22/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10550/40036 ( 26%)]  Loss: 70.954552 (70.4608)  Loss_l1:  0.056517 (0.1040)  Loss_traj: 70.671967 (69.9407)  Time: 1.274s,   25.12/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10600/40036 ( 26%)]  Loss: 71.586006 (70.4614)  Loss_l1:  0.103221 (0.1040)  Loss_traj: 71.069901 (69.9415)  Time: 1.252s,   25.56/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10650/40036 ( 27%)]  Loss: 72.472054 (70.4627)  Loss_l1:  0.095393 (0.1040)  Loss_traj: 71.995087 (69.9428)  Time: 1.326s,   24.14/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10700/40036 ( 27%)]  Loss: 70.451820 (70.4624)  Loss_l1:  0.122325 (0.1040)  Loss_traj: 69.840195 (69.9424)  Time: 1.250s,   25.59/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10750/40036 ( 27%)]  Loss: 71.745232 (70.4630)  Loss_l1:  0.136455 (0.1041)  Loss_traj: 71.062958 (69.9427)  Time: 1.340s,   23.87/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10800/40036 ( 27%)]  Loss: 69.419258 (70.4632)  Loss_l1:  0.071518 (0.1040)  Loss_traj: 69.061668 (69.9430)  Time: 1.229s,   26.03/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10850/40036 ( 27%)]  Loss: 72.063568 (70.4642)  Loss_l1:  0.015278 (0.1039)  Loss_traj: 71.987175 (69.9444)  Time: 1.329s,   24.07/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [10900/40036 ( 27%)]  Loss: 69.768204 (70.4646)  Loss_l1:  0.255764 (0.1039)  Loss_traj: 68.489380 (69.9450)  Time: 1.268s,   25.24/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [10950/40036 ( 27%)]  Loss: 70.688797 (70.4647)  Loss_l1:  0.045446 (0.1039)  Loss_traj: 70.461571 (69.9449)  Time: 1.223s,   26.16/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11000/40036 ( 27%)]  Loss: 69.813843 (70.4654)  Loss_l1:  0.052706 (0.1040)  Loss_traj: 69.550316 (69.9456)  Time: 1.345s,   23.79/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11050/40036 ( 28%)]  Loss: 69.809280 (70.4645)  Loss_l1:  0.105417 (0.1039)  Loss_traj: 69.282196 (69.9450)  Time: 1.237s,   25.86/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11100/40036 ( 28%)]  Loss: 71.165131 (70.4646)  Loss_l1:  0.021168 (0.1039)  Loss_traj: 71.059288 (69.9451)  Time: 1.333s,   24.00/s  (1.287s,   24.86/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11150/40036 ( 28%)]  Loss: 70.387611 (70.4659)  Loss_l1:  0.244684 (0.1039)  Loss_traj: 69.164192 (69.9463)  Time: 1.247s,   25.67/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11200/40036 ( 28%)]  Loss: 70.390762 (70.4645)  Loss_l1:  0.060280 (0.1038)  Loss_traj: 70.089363 (69.9455)  Time: 1.356s,   23.60/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11250/40036 ( 28%)]  Loss: 72.110695 (70.4645)  Loss_l1:  0.381085 (0.1038)  Loss_traj: 70.205269 (69.9455)  Time: 1.313s,   24.36/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11300/40036 ( 28%)]  Loss: 68.479790 (70.4637)  Loss_l1:  0.100026 (0.1037)  Loss_traj: 67.979660 (69.9451)  Time: 1.232s,   25.98/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11350/40036 ( 28%)]  Loss: 69.137070 (70.4637)  Loss_l1:  0.083741 (0.1037)  Loss_traj: 68.718361 (69.9452)  Time: 1.302s,   24.58/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11400/40036 ( 28%)]  Loss: 70.445213 (70.4642)  Loss_l1:  0.054459 (0.1036)  Loss_traj: 70.172920 (69.9463)  Time: 1.245s,   25.71/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11450/40036 ( 29%)]  Loss: 70.942513 (70.4640)  Loss_l1:  0.036698 (0.1036)  Loss_traj: 70.759026 (69.9460)  Time: 1.377s,   23.24/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11500/40036 ( 29%)]  Loss: 72.197426 (70.4645)  Loss_l1:  0.082074 (0.1037)  Loss_traj: 71.787056 (69.9462)  Time: 1.228s,   26.06/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11550/40036 ( 29%)]  Loss: 68.977074 (70.4643)  Loss_l1:  0.030681 (0.1036)  Loss_traj: 68.823669 (69.9463)  Time: 1.320s,   24.25/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11600/40036 ( 29%)]  Loss: 71.481674 (70.4652)  Loss_l1:  0.203906 (0.1036)  Loss_traj: 70.462143 (69.9470)  Time: 1.240s,   25.80/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11650/40036 ( 29%)]  Loss: 69.039482 (70.4651)  Loss_l1:  0.127379 (0.1037)  Loss_traj: 68.402588 (69.9467)  Time: 1.220s,   26.22/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11700/40036 ( 29%)]  Loss: 68.829056 (70.4656)  Loss_l1:  0.182525 (0.1037)  Loss_traj: 67.916428 (69.9471)  Time: 1.344s,   23.80/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11750/40036 ( 29%)]  Loss: 69.677414 (70.4660)  Loss_l1:  0.020522 (0.1037)  Loss_traj: 69.574806 (69.9474)  Time: 1.217s,   26.30/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [11800/40036 ( 29%)]  Loss: 71.208130 (70.4657)  Loss_l1:  0.109338 (0.1037)  Loss_traj: 70.661438 (69.9474)  Time: 1.310s,   24.43/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11850/40036 ( 30%)]  Loss: 70.811081 (70.4660)  Loss_l1:  0.072798 (0.1037)  Loss_traj: 70.447090 (69.9474)  Time: 1.234s,   25.92/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [11900/40036 ( 30%)]  Loss: 70.163719 (70.4668)  Loss_l1:  0.111116 (0.1038)  Loss_traj: 69.608139 (69.9479)  Time: 1.348s,   23.74/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [11950/40036 ( 30%)]  Loss: 71.465233 (70.4663)  Loss_l1:  0.092288 (0.1038)  Loss_traj: 71.003792 (69.9473)  Time: 1.220s,   26.22/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12000/40036 ( 30%)]  Loss: 70.419220 (70.4667)  Loss_l1:  0.125775 (0.1038)  Loss_traj: 69.790344 (69.9477)  Time: 1.245s,   25.71/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12050/40036 ( 30%)]  Loss: 70.917084 (70.4660)  Loss_l1:  0.274735 (0.1038)  Loss_traj: 69.543404 (69.9471)  Time: 1.317s,   24.31/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12100/40036 ( 30%)]  Loss: 69.880356 (70.4661)  Loss_l1:  0.200497 (0.1037)  Loss_traj: 68.877869 (69.9475)  Time: 1.277s,   25.05/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12150/40036 ( 30%)]  Loss: 70.912354 (70.4660)  Loss_l1:  0.364747 (0.1038)  Loss_traj: 69.088615 (69.9472)  Time: 1.321s,   24.22/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12200/40036 ( 30%)]  Loss: 71.426926 (70.4660)  Loss_l1:  0.420114 (0.1038)  Loss_traj: 69.326355 (69.9472)  Time: 1.243s,   25.74/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12250/40036 ( 31%)]  Loss: 72.099358 (70.4654)  Loss_l1:  0.036888 (0.1038)  Loss_traj: 71.914917 (69.9462)  Time: 1.333s,   24.00/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12300/40036 ( 31%)]  Loss: 70.794403 (70.4669)  Loss_l1:  0.009431 (0.1038)  Loss_traj: 70.747246 (69.9480)  Time: 1.311s,   24.41/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12350/40036 ( 31%)]  Loss: 71.185249 (70.4669)  Loss_l1:  0.028425 (0.1037)  Loss_traj: 71.043121 (69.9482)  Time: 1.226s,   26.09/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12400/40036 ( 31%)]  Loss: 69.840744 (70.4666)  Loss_l1:  0.015001 (0.1038)  Loss_traj: 69.765739 (69.9478)  Time: 1.305s,   24.51/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12450/40036 ( 31%)]  Loss: 70.142380 (70.4674)  Loss_l1:  0.066360 (0.1037)  Loss_traj: 69.810577 (69.9489)  Time: 1.215s,   26.33/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12500/40036 ( 31%)]  Loss: 69.053604 (70.4669)  Loss_l1:  0.049588 (0.1037)  Loss_traj: 68.805664 (69.9481)  Time: 1.342s,   23.85/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12550/40036 ( 31%)]  Loss: 69.736214 (70.4664)  Loss_l1:  0.133069 (0.1037)  Loss_traj: 69.070869 (69.9477)  Time: 1.248s,   25.64/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12600/40036 ( 31%)]  Loss: 74.103889 (70.4651)  Loss_l1:  0.247073 (0.1037)  Loss_traj: 72.868523 (69.9468)  Time: 1.344s,   23.81/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [12650/40036 ( 32%)]  Loss: 69.697929 (70.4658)  Loss_l1:  0.012230 (0.1036)  Loss_traj: 69.636780 (69.9476)  Time: 1.311s,   24.40/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [12700/40036 ( 32%)]  Loss: 69.319710 (70.4658)  Loss_l1:  0.089278 (0.1036)  Loss_traj: 68.873322 (69.9477)  Time: 1.232s,   25.97/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12750/40036 ( 32%)]  Loss: 71.067032 (70.4666)  Loss_l1:  0.008219 (0.1036)  Loss_traj: 71.025940 (69.9487)  Time: 1.348s,   23.74/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12800/40036 ( 32%)]  Loss: 70.972191 (70.4665)  Loss_l1:  0.042573 (0.1037)  Loss_traj: 70.759323 (69.9480)  Time: 1.227s,   26.09/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "/home/cutz/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "INFO:root:Train: 0 [12850/40036 ( 32%)]  Loss: 69.952751 (70.4669)  Loss_l1:  0.012940 (0.1037)  Loss_traj: 69.888054 (69.9482)  Time: 1.346s,   23.77/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12900/40036 ( 32%)]  Loss: 69.568939 (70.4667)  Loss_l1:  0.112179 (0.1037)  Loss_traj: 69.008041 (69.9482)  Time: 1.249s,   25.61/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [12950/40036 ( 32%)]  Loss: 71.168991 (70.4670)  Loss_l1:  0.016039 (0.1038)  Loss_traj: 71.088799 (69.9481)  Time: 1.338s,   23.91/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13000/40036 ( 32%)]  Loss: 70.212761 (70.4668)  Loss_l1:  0.199374 (0.1037)  Loss_traj: 69.215889 (69.9482)  Time: 1.217s,   26.29/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13050/40036 ( 33%)]  Loss: 71.127823 (70.4661)  Loss_l1:  0.089481 (0.1037)  Loss_traj: 70.680420 (69.9477)  Time: 1.224s,   26.14/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13100/40036 ( 33%)]  Loss: 70.947960 (70.4656)  Loss_l1:  0.188694 (0.1036)  Loss_traj: 70.004494 (69.9476)  Time: 1.379s,   23.21/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13150/40036 ( 33%)]  Loss: 70.868317 (70.4660)  Loss_l1:  0.169127 (0.1036)  Loss_traj: 70.022682 (69.9478)  Time: 1.240s,   25.80/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13200/40036 ( 33%)]  Loss: 69.345909 (70.4667)  Loss_l1:  0.003385 (0.1036)  Loss_traj: 69.328987 (69.9487)  Time: 1.367s,   23.41/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [13250/40036 ( 33%)]  Loss: 69.628914 (70.4659)  Loss_l1:  0.023607 (0.1036)  Loss_traj: 69.510880 (69.9480)  Time: 1.243s,   25.74/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13300/40036 ( 33%)]  Loss: 70.658325 (70.4657)  Loss_l1:  0.189417 (0.1036)  Loss_traj: 69.711243 (69.9476)  Time: 1.319s,   24.26/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13350/40036 ( 33%)]  Loss: 72.004097 (70.4653)  Loss_l1:  0.132430 (0.1036)  Loss_traj: 71.341949 (69.9474)  Time: 1.337s,   23.94/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13400/40036 ( 33%)]  Loss: 69.779053 (70.4655)  Loss_l1:  0.103008 (0.1035)  Loss_traj: 69.264015 (69.9480)  Time: 1.221s,   26.21/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13450/40036 ( 34%)]  Loss: 69.167923 (70.4639)  Loss_l1:  0.066363 (0.1035)  Loss_traj: 68.836105 (69.9463)  Time: 1.320s,   24.25/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13500/40036 ( 34%)]  Loss: 69.162468 (70.4648)  Loss_l1:  0.071151 (0.1035)  Loss_traj: 68.806709 (69.9473)  Time: 1.258s,   25.43/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13550/40036 ( 34%)]  Loss: 69.661797 (70.4651)  Loss_l1:  0.106864 (0.1035)  Loss_traj: 69.127480 (69.9478)  Time: 1.326s,   24.13/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13600/40036 ( 34%)]  Loss: 71.237663 (70.4649)  Loss_l1:  0.298754 (0.1036)  Loss_traj: 69.743896 (69.9471)  Time: 1.235s,   25.92/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13650/40036 ( 34%)]  Loss: 73.243568 (70.4646)  Loss_l1:  0.139779 (0.1035)  Loss_traj: 72.544678 (69.9471)  Time: 1.341s,   23.86/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13700/40036 ( 34%)]  Loss: 70.362801 (70.4653)  Loss_l1:  0.224077 (0.1035)  Loss_traj: 69.242416 (69.9477)  Time: 1.246s,   25.67/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [13750/40036 ( 34%)]  Loss: 70.769913 (70.4653)  Loss_l1:  0.161578 (0.1035)  Loss_traj: 69.962021 (69.9476)  Time: 1.246s,   25.68/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13800/40036 ( 34%)]  Loss: 73.102577 (70.4658)  Loss_l1:  0.129209 (0.1035)  Loss_traj: 72.456535 (69.9482)  Time: 1.299s,   24.64/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [13850/40036 ( 35%)]  Loss: 72.125305 (70.4663)  Loss_l1:  0.058206 (0.1035)  Loss_traj: 71.834274 (69.9488)  Time: 1.237s,   25.87/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13900/40036 ( 35%)]  Loss: 68.917267 (70.4654)  Loss_l1:  0.018199 (0.1035)  Loss_traj: 68.826271 (69.9481)  Time: 1.375s,   23.27/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [13950/40036 ( 35%)]  Loss: 71.069069 (70.4649)  Loss_l1:  0.009897 (0.1035)  Loss_traj: 71.019585 (69.9473)  Time: 1.220s,   26.22/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14000/40036 ( 35%)]  Loss: 70.915848 (70.4653)  Loss_l1:  0.217815 (0.1036)  Loss_traj: 69.826775 (69.9475)  Time: 1.320s,   24.25/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14050/40036 ( 35%)]  Loss: 69.953514 (70.4659)  Loss_l1:  0.096692 (0.1036)  Loss_traj: 69.470055 (69.9480)  Time: 1.318s,   24.29/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14100/40036 ( 35%)]  Loss: 70.189156 (70.4663)  Loss_l1:  0.140520 (0.1036)  Loss_traj: 69.486557 (69.9483)  Time: 1.251s,   25.57/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14150/40036 ( 35%)]  Loss: 72.412758 (70.4661)  Loss_l1:  0.028364 (0.1036)  Loss_traj: 72.270943 (69.9481)  Time: 1.346s,   23.78/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14200/40036 ( 35%)]  Loss: 70.024261 (70.4663)  Loss_l1:  0.082856 (0.1036)  Loss_traj: 69.609978 (69.9485)  Time: 1.236s,   25.88/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14250/40036 ( 36%)]  Loss: 69.264503 (70.4653)  Loss_l1:  0.089679 (0.1036)  Loss_traj: 68.816109 (69.9472)  Time: 1.349s,   23.72/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14300/40036 ( 36%)]  Loss: 70.706482 (70.4654)  Loss_l1:  0.188582 (0.1036)  Loss_traj: 69.763573 (69.9476)  Time: 1.256s,   25.48/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14350/40036 ( 36%)]  Loss: 70.210220 (70.4658)  Loss_l1:  0.128530 (0.1035)  Loss_traj: 69.567574 (69.9481)  Time: 1.331s,   24.05/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14400/40036 ( 36%)]  Loss: 71.338531 (70.4656)  Loss_l1:  0.072009 (0.1036)  Loss_traj: 70.978485 (69.9478)  Time: 1.318s,   24.28/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14450/40036 ( 36%)]  Loss: 69.927467 (70.4660)  Loss_l1:  0.152928 (0.1035)  Loss_traj: 69.162827 (69.9484)  Time: 1.243s,   25.75/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14500/40036 ( 36%)]  Loss: 70.701927 (70.4660)  Loss_l1:  0.071887 (0.1035)  Loss_traj: 70.342491 (69.9483)  Time: 1.284s,   24.92/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14550/40036 ( 36%)]  Loss: 71.362656 (70.4671)  Loss_l1:  0.293273 (0.1036)  Loss_traj: 69.896294 (69.9489)  Time: 1.236s,   25.90/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14600/40036 ( 36%)]  Loss: 70.330444 (70.4676)  Loss_l1:  0.054249 (0.1036)  Loss_traj: 70.059204 (69.9496)  Time: 1.332s,   24.03/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [14650/40036 ( 37%)]  Loss: 69.723541 (70.4676)  Loss_l1:  0.058471 (0.1036)  Loss_traj: 69.431183 (69.9499)  Time: 1.229s,   26.03/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14700/40036 ( 37%)]  Loss: 70.709465 (70.4677)  Loss_l1:  0.216871 (0.1036)  Loss_traj: 69.625107 (69.9498)  Time: 1.322s,   24.20/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14750/40036 ( 37%)]  Loss: 68.915001 (70.4677)  Loss_l1:  0.001327 (0.1036)  Loss_traj: 68.908363 (69.9497)  Time: 1.297s,   24.66/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14800/40036 ( 37%)]  Loss: 71.183006 (70.4676)  Loss_l1:  0.226587 (0.1035)  Loss_traj: 70.050072 (69.9500)  Time: 1.226s,   26.11/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [14850/40036 ( 37%)]  Loss: 71.031151 (70.4680)  Loss_l1:  0.218136 (0.1036)  Loss_traj: 69.940468 (69.9501)  Time: 1.334s,   23.98/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14900/40036 ( 37%)]  Loss: 70.957100 (70.4679)  Loss_l1:  0.068660 (0.1036)  Loss_traj: 70.613800 (69.9498)  Time: 1.227s,   26.07/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [14950/40036 ( 37%)]  Loss: 70.591667 (70.4681)  Loss_l1:  0.130400 (0.1036)  Loss_traj: 69.939667 (69.9502)  Time: 1.340s,   23.88/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.018 (0.014)\n",
      "INFO:root:Train: 0 [15000/40036 ( 37%)]  Loss: 69.308731 (70.4682)  Loss_l1:  0.041773 (0.1036)  Loss_traj: 69.099861 (69.9501)  Time: 1.217s,   26.29/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15050/40036 ( 38%)]  Loss: 69.526657 (70.4685)  Loss_l1:  0.081465 (0.1037)  Loss_traj: 69.119331 (69.9500)  Time: 1.314s,   24.35/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15100/40036 ( 38%)]  Loss: 72.669807 (70.4693)  Loss_l1:  0.242441 (0.1037)  Loss_traj: 71.457603 (69.9505)  Time: 1.315s,   24.34/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15150/40036 ( 38%)]  Loss: 71.393715 (70.4689)  Loss_l1:  0.003896 (0.1037)  Loss_traj: 71.374237 (69.9505)  Time: 1.240s,   25.81/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15200/40036 ( 38%)]  Loss: 70.352676 (70.4686)  Loss_l1:  0.136747 (0.1036)  Loss_traj: 69.668945 (69.9505)  Time: 1.334s,   24.00/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15250/40036 ( 38%)]  Loss: 71.145523 (70.4690)  Loss_l1:  0.136271 (0.1037)  Loss_traj: 70.464165 (69.9504)  Time: 1.222s,   26.18/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15300/40036 ( 38%)]  Loss: 68.445351 (70.4685)  Loss_l1:  0.000652 (0.1038)  Loss_traj: 68.442093 (69.9497)  Time: 1.331s,   24.04/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15350/40036 ( 38%)]  Loss: 71.531105 (70.4683)  Loss_l1:  0.065823 (0.1037)  Loss_traj: 71.201988 (69.9498)  Time: 1.281s,   24.98/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15400/40036 ( 38%)]  Loss: 70.561699 (70.4685)  Loss_l1:  0.043795 (0.1037)  Loss_traj: 70.342728 (69.9498)  Time: 1.317s,   24.30/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15450/40036 ( 39%)]  Loss: 70.961449 (70.4685)  Loss_l1:  0.175694 (0.1038)  Loss_traj: 70.082977 (69.9497)  Time: 1.310s,   24.43/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15500/40036 ( 39%)]  Loss: 70.235916 (70.4691)  Loss_l1:  0.065797 (0.1038)  Loss_traj: 69.906929 (69.9502)  Time: 1.262s,   25.36/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15550/40036 ( 39%)]  Loss: 72.610771 (70.4700)  Loss_l1:  0.005633 (0.1038)  Loss_traj: 72.582611 (69.9509)  Time: 1.290s,   24.81/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15600/40036 ( 39%)]  Loss: 68.669258 (70.4708)  Loss_l1:  0.127653 (0.1038)  Loss_traj: 68.030991 (69.9516)  Time: 1.228s,   26.06/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15650/40036 ( 39%)]  Loss: 68.607796 (70.4707)  Loss_l1:  0.134551 (0.1039)  Loss_traj: 67.935043 (69.9513)  Time: 1.317s,   24.29/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15700/40036 ( 39%)]  Loss: 70.299797 (70.4709)  Loss_l1:  0.111065 (0.1039)  Loss_traj: 69.744476 (69.9512)  Time: 1.253s,   25.54/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15750/40036 ( 39%)]  Loss: 70.387268 (70.4712)  Loss_l1:  0.173578 (0.1039)  Loss_traj: 69.519379 (69.9515)  Time: 1.321s,   24.23/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [15800/40036 ( 39%)]  Loss: 70.049385 (70.4712)  Loss_l1:  0.178066 (0.1039)  Loss_traj: 69.159050 (69.9515)  Time: 1.366s,   23.42/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [15850/40036 ( 40%)]  Loss: 70.789787 (70.4713)  Loss_l1:  0.006475 (0.1040)  Loss_traj: 70.757416 (69.9514)  Time: 1.227s,   26.07/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15900/40036 ( 40%)]  Loss: 72.140747 (70.4712)  Loss_l1:  0.279931 (0.1040)  Loss_traj: 70.741089 (69.9510)  Time: 1.332s,   24.02/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [15950/40036 ( 40%)]  Loss: 69.470612 (70.4714)  Loss_l1:  0.048305 (0.1040)  Loss_traj: 69.229088 (69.9512)  Time: 1.231s,   26.00/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16000/40036 ( 40%)]  Loss: 71.507286 (70.4715)  Loss_l1:  0.147337 (0.1041)  Loss_traj: 70.770599 (69.9512)  Time: 1.320s,   24.24/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16050/40036 ( 40%)]  Loss: 70.714127 (70.4713)  Loss_l1:  0.206214 (0.1040)  Loss_traj: 69.683060 (69.9513)  Time: 1.241s,   25.79/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16100/40036 ( 40%)]  Loss: 71.754601 (70.4710)  Loss_l1:  0.115123 (0.1039)  Loss_traj: 71.178986 (69.9513)  Time: 1.316s,   24.31/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16150/40036 ( 40%)]  Loss: 69.375504 (70.4699)  Loss_l1:  0.108003 (0.1039)  Loss_traj: 68.835487 (69.9505)  Time: 1.287s,   24.85/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16200/40036 ( 40%)]  Loss: 69.308945 (70.4699)  Loss_l1:  0.029199 (0.1039)  Loss_traj: 69.162949 (69.9505)  Time: 1.249s,   25.62/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16250/40036 ( 41%)]  Loss: 70.554619 (70.4695)  Loss_l1:  0.145208 (0.1039)  Loss_traj: 69.828583 (69.9502)  Time: 1.340s,   23.88/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16300/40036 ( 41%)]  Loss: 71.650948 (70.4695)  Loss_l1:  0.203195 (0.1039)  Loss_traj: 70.634972 (69.9500)  Time: 1.237s,   25.88/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16350/40036 ( 41%)]  Loss: 69.213089 (70.4689)  Loss_l1:  0.161460 (0.1039)  Loss_traj: 68.405792 (69.9494)  Time: 1.358s,   23.56/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16400/40036 ( 41%)]  Loss: 70.488701 (70.4684)  Loss_l1:  0.060772 (0.1039)  Loss_traj: 70.184837 (69.9489)  Time: 1.240s,   25.80/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16450/40036 ( 41%)]  Loss: 71.107491 (70.4692)  Loss_l1:  0.189710 (0.1039)  Loss_traj: 70.158943 (69.9496)  Time: 1.366s,   23.43/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16500/40036 ( 41%)]  Loss: 72.887184 (70.4698)  Loss_l1:  0.024796 (0.1039)  Loss_traj: 72.763206 (69.9502)  Time: 1.340s,   23.89/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16550/40036 ( 41%)]  Loss: 70.359039 (70.4696)  Loss_l1:  0.103466 (0.1039)  Loss_traj: 69.841713 (69.9501)  Time: 1.236s,   25.90/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16600/40036 ( 41%)]  Loss: 71.057655 (70.4694)  Loss_l1:  0.153335 (0.1038)  Loss_traj: 70.290985 (69.9502)  Time: 1.338s,   23.92/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16650/40036 ( 42%)]  Loss: 68.625694 (70.4697)  Loss_l1:  0.071525 (0.1039)  Loss_traj: 68.268066 (69.9503)  Time: 1.226s,   26.11/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16700/40036 ( 42%)]  Loss: 71.940842 (70.4700)  Loss_l1:  0.136749 (0.1039)  Loss_traj: 71.257095 (69.9506)  Time: 1.314s,   24.35/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16750/40036 ( 42%)]  Loss: 71.091187 (70.4696)  Loss_l1:  0.064436 (0.1039)  Loss_traj: 70.769005 (69.9503)  Time: 1.221s,   26.21/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16800/40036 ( 42%)]  Loss: 69.905846 (70.4699)  Loss_l1:  0.180029 (0.1038)  Loss_traj: 69.005699 (69.9509)  Time: 1.362s,   23.50/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16850/40036 ( 42%)]  Loss: 71.064613 (70.4699)  Loss_l1:  0.011740 (0.1038)  Loss_traj: 71.005913 (69.9508)  Time: 1.303s,   24.55/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [16900/40036 ( 42%)]  Loss: 69.929558 (70.4702)  Loss_l1:  0.090149 (0.1039)  Loss_traj: 69.478813 (69.9508)  Time: 1.230s,   26.03/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [16950/40036 ( 42%)]  Loss: 71.862518 (70.4702)  Loss_l1:  0.194487 (0.1039)  Loss_traj: 70.890083 (69.9508)  Time: 1.354s,   23.64/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17000/40036 ( 42%)]  Loss: 71.255058 (70.4700)  Loss_l1:  0.106466 (0.1039)  Loss_traj: 70.722733 (69.9505)  Time: 1.265s,   25.30/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17050/40036 ( 43%)]  Loss: 71.465927 (70.4695)  Loss_l1:  0.243212 (0.1039)  Loss_traj: 70.249870 (69.9499)  Time: 1.344s,   23.81/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17100/40036 ( 43%)]  Loss: 70.804253 (70.4688)  Loss_l1:  0.042602 (0.1039)  Loss_traj: 70.591240 (69.9494)  Time: 1.240s,   25.81/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17150/40036 ( 43%)]  Loss: 71.091850 (70.4691)  Loss_l1:  0.183313 (0.1039)  Loss_traj: 70.175285 (69.9498)  Time: 1.336s,   23.95/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17200/40036 ( 43%)]  Loss: 71.350380 (70.4693)  Loss_l1:  0.020435 (0.1038)  Loss_traj: 71.248207 (69.9501)  Time: 1.350s,   23.71/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17250/40036 ( 43%)]  Loss: 70.476089 (70.4689)  Loss_l1:  0.030265 (0.1038)  Loss_traj: 70.324768 (69.9498)  Time: 1.241s,   25.78/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17300/40036 ( 43%)]  Loss: 69.960030 (70.4689)  Loss_l1:  0.027743 (0.1038)  Loss_traj: 69.821312 (69.9497)  Time: 1.353s,   23.66/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17350/40036 ( 43%)]  Loss: 69.544968 (70.4693)  Loss_l1:  0.092292 (0.1039)  Loss_traj: 69.083504 (69.9500)  Time: 1.230s,   26.01/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17400/40036 ( 43%)]  Loss: 70.040085 (70.4690)  Loss_l1:  0.026573 (0.1039)  Loss_traj: 69.907219 (69.9497)  Time: 1.348s,   23.73/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17450/40036 ( 44%)]  Loss: 71.157745 (70.4690)  Loss_l1:  0.085455 (0.1039)  Loss_traj: 70.730469 (69.9495)  Time: 1.250s,   25.60/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17500/40036 ( 44%)]  Loss: 70.989075 (70.4695)  Loss_l1:  0.105202 (0.1039)  Loss_traj: 70.463066 (69.9501)  Time: 1.361s,   23.52/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17550/40036 ( 44%)]  Loss: 70.812485 (70.4692)  Loss_l1:  0.050279 (0.1038)  Loss_traj: 70.561089 (69.9500)  Time: 1.298s,   24.65/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17600/40036 ( 44%)]  Loss: 70.901489 (70.4698)  Loss_l1:  0.186321 (0.1039)  Loss_traj: 69.969887 (69.9504)  Time: 1.245s,   25.70/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17650/40036 ( 44%)]  Loss: 71.668465 (70.4700)  Loss_l1:  0.202104 (0.1039)  Loss_traj: 70.657944 (69.9504)  Time: 1.333s,   24.01/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17700/40036 ( 44%)]  Loss: 69.717758 (70.4698)  Loss_l1:  0.115457 (0.1039)  Loss_traj: 69.140472 (69.9503)  Time: 1.273s,   25.14/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [17750/40036 ( 44%)]  Loss: 69.154465 (70.4698)  Loss_l1:  0.152939 (0.1039)  Loss_traj: 68.389771 (69.9504)  Time: 1.320s,   24.24/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17800/40036 ( 44%)]  Loss: 71.154907 (70.4701)  Loss_l1:  0.188513 (0.1039)  Loss_traj: 70.212341 (69.9506)  Time: 1.226s,   26.10/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [17850/40036 ( 45%)]  Loss: 68.795959 (70.4702)  Loss_l1:  0.056439 (0.1040)  Loss_traj: 68.513763 (69.9504)  Time: 1.340s,   23.88/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [17900/40036 ( 45%)]  Loss: 70.604286 (70.4702)  Loss_l1:  0.160450 (0.1040)  Loss_traj: 69.802040 (69.9503)  Time: 1.319s,   24.27/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [17950/40036 ( 45%)]  Loss: 68.677887 (70.4698)  Loss_l1:  0.056835 (0.1040)  Loss_traj: 68.393715 (69.9497)  Time: 1.231s,   25.98/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18000/40036 ( 45%)]  Loss: 69.976257 (70.4699)  Loss_l1:  0.033471 (0.1040)  Loss_traj: 69.808899 (69.9499)  Time: 1.300s,   24.61/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18050/40036 ( 45%)]  Loss: 70.008514 (70.4689)  Loss_l1:  0.082147 (0.1040)  Loss_traj: 69.597778 (69.9490)  Time: 1.226s,   26.11/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18100/40036 ( 45%)]  Loss: 71.736954 (70.4694)  Loss_l1:  0.105813 (0.1039)  Loss_traj: 71.207893 (69.9497)  Time: 1.323s,   24.18/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18150/40036 ( 45%)]  Loss: 70.260460 (70.4693)  Loss_l1:  0.097874 (0.1040)  Loss_traj: 69.771088 (69.9495)  Time: 1.232s,   25.97/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18200/40036 ( 45%)]  Loss: 70.496643 (70.4688)  Loss_l1:  0.153716 (0.1039)  Loss_traj: 69.728065 (69.9491)  Time: 1.317s,   24.30/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18250/40036 ( 46%)]  Loss: 69.882004 (70.4689)  Loss_l1:  0.140329 (0.1040)  Loss_traj: 69.180359 (69.9490)  Time: 1.333s,   24.01/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18300/40036 ( 46%)]  Loss: 71.472610 (70.4692)  Loss_l1:  0.167568 (0.1039)  Loss_traj: 70.634773 (69.9495)  Time: 1.228s,   26.06/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18350/40036 ( 46%)]  Loss: 73.257385 (70.4689)  Loss_l1:  0.182695 (0.1040)  Loss_traj: 72.343910 (69.9491)  Time: 1.340s,   23.88/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18400/40036 ( 46%)]  Loss: 71.449265 (70.4684)  Loss_l1:  0.257632 (0.1040)  Loss_traj: 70.161102 (69.9486)  Time: 1.216s,   26.32/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18450/40036 ( 46%)]  Loss: 70.823112 (70.4680)  Loss_l1:  0.260297 (0.1039)  Loss_traj: 69.521629 (69.9483)  Time: 1.315s,   24.33/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18500/40036 ( 46%)]  Loss: 69.428528 (70.4685)  Loss_l1:  0.184480 (0.1040)  Loss_traj: 68.506126 (69.9487)  Time: 1.220s,   26.24/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18550/40036 ( 46%)]  Loss: 69.489601 (70.4691)  Loss_l1:  0.084043 (0.1040)  Loss_traj: 69.069382 (69.9490)  Time: 1.361s,   23.51/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18600/40036 ( 46%)]  Loss: 71.420647 (70.4699)  Loss_l1:  0.005910 (0.1040)  Loss_traj: 71.391098 (69.9496)  Time: 1.322s,   24.20/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18650/40036 ( 47%)]  Loss: 70.049004 (70.4703)  Loss_l1:  0.158309 (0.1041)  Loss_traj: 69.257462 (69.9499)  Time: 1.226s,   26.11/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18700/40036 ( 47%)]  Loss: 70.440643 (70.4708)  Loss_l1:  0.141999 (0.1040)  Loss_traj: 69.730644 (69.9507)  Time: 1.352s,   23.68/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18750/40036 ( 47%)]  Loss: 71.276886 (70.4705)  Loss_l1:  0.108021 (0.1040)  Loss_traj: 70.736778 (69.9505)  Time: 1.217s,   26.29/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18800/40036 ( 47%)]  Loss: 70.299034 (70.4702)  Loss_l1:  0.004289 (0.1040)  Loss_traj: 70.277588 (69.9503)  Time: 1.347s,   23.75/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18850/40036 ( 47%)]  Loss: 71.615509 (70.4714)  Loss_l1:  0.099231 (0.1040)  Loss_traj: 71.119354 (69.9513)  Time: 1.227s,   26.09/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [18900/40036 ( 47%)]  Loss: 70.135605 (70.4712)  Loss_l1:  0.138795 (0.1040)  Loss_traj: 69.441635 (69.9512)  Time: 1.325s,   24.16/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [18950/40036 ( 47%)]  Loss: 70.107689 (70.4706)  Loss_l1:  0.040202 (0.1039)  Loss_traj: 69.906677 (69.9509)  Time: 1.355s,   23.62/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [19000/40036 ( 47%)]  Loss: 70.664551 (70.4704)  Loss_l1:  0.239899 (0.1039)  Loss_traj: 69.465057 (69.9510)  Time: 1.223s,   26.17/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [19050/40036 ( 48%)]  Loss: 71.270081 (70.4709)  Loss_l1:  0.188537 (0.1039)  Loss_traj: 70.327400 (69.9512)  Time: 1.327s,   24.11/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [19100/40036 ( 48%)]  Loss: 70.649345 (70.4710)  Loss_l1:  0.028203 (0.1039)  Loss_traj: 70.508331 (69.9517)  Time: 1.243s,   25.73/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [19150/40036 ( 48%)]  Loss: 69.579376 (70.4707)  Loss_l1:  0.082202 (0.1039)  Loss_traj: 69.168365 (69.9512)  Time: 1.348s,   23.73/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19200/40036 ( 48%)]  Loss: 69.409630 (70.4706)  Loss_l1:  0.017415 (0.1039)  Loss_traj: 69.322556 (69.9511)  Time: 1.227s,   26.08/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19250/40036 ( 48%)]  Loss: 70.235832 (70.4708)  Loss_l1:  0.174535 (0.1039)  Loss_traj: 69.363159 (69.9511)  Time: 1.365s,   23.45/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19300/40036 ( 48%)]  Loss: 70.346939 (70.4711)  Loss_l1:  0.159211 (0.1040)  Loss_traj: 69.550888 (69.9511)  Time: 1.325s,   24.15/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [19350/40036 ( 48%)]  Loss: 71.198532 (70.4712)  Loss_l1:  0.092532 (0.1040)  Loss_traj: 70.735870 (69.9511)  Time: 1.219s,   26.24/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19400/40036 ( 48%)]  Loss: 69.936035 (70.4714)  Loss_l1:  0.080044 (0.1041)  Loss_traj: 69.535812 (69.9511)  Time: 1.330s,   24.05/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19450/40036 ( 49%)]  Loss: 69.888931 (70.4713)  Loss_l1:  0.003818 (0.1041)  Loss_traj: 69.869843 (69.9509)  Time: 1.245s,   25.71/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19500/40036 ( 49%)]  Loss: 71.341812 (70.4706)  Loss_l1:  0.101393 (0.1041)  Loss_traj: 70.834846 (69.9503)  Time: 1.341s,   23.85/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19550/40036 ( 49%)]  Loss: 72.132698 (70.4711)  Loss_l1:  0.086718 (0.1041)  Loss_traj: 71.699112 (69.9508)  Time: 1.236s,   25.90/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19600/40036 ( 49%)]  Loss: 71.715080 (70.4710)  Loss_l1:  0.138472 (0.1041)  Loss_traj: 71.022720 (69.9507)  Time: 1.331s,   24.05/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19650/40036 ( 49%)]  Loss: 70.897858 (70.4710)  Loss_l1:  0.042676 (0.1040)  Loss_traj: 70.684479 (69.9509)  Time: 1.245s,   25.70/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19700/40036 ( 49%)]  Loss: 70.025322 (70.4711)  Loss_l1:  0.127185 (0.1040)  Loss_traj: 69.389397 (69.9509)  Time: 1.230s,   26.01/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19750/40036 ( 49%)]  Loss: 71.311852 (70.4713)  Loss_l1:  0.114604 (0.1040)  Loss_traj: 70.738831 (69.9512)  Time: 1.328s,   24.10/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [19800/40036 ( 49%)]  Loss: 69.340683 (70.4707)  Loss_l1:  0.131546 (0.1040)  Loss_traj: 68.682953 (69.9508)  Time: 1.222s,   26.19/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [19850/40036 ( 50%)]  Loss: 74.642838 (70.4714)  Loss_l1:  0.449502 (0.1040)  Loss_traj: 72.395325 (69.9514)  Time: 1.341s,   23.87/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [19900/40036 ( 50%)]  Loss: 69.594162 (70.4712)  Loss_l1:  0.011955 (0.1039)  Loss_traj: 69.534386 (69.9515)  Time: 1.241s,   25.79/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [19950/40036 ( 50%)]  Loss: 71.226128 (70.4717)  Loss_l1:  0.213047 (0.1040)  Loss_traj: 70.160889 (69.9519)  Time: 1.318s,   24.28/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [20000/40036 ( 50%)]  Loss: 71.565529 (70.4718)  Loss_l1:  0.179130 (0.1040)  Loss_traj: 70.669884 (69.9520)  Time: 1.329s,   24.07/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [20050/40036 ( 50%)]  Loss: 70.159515 (70.4720)  Loss_l1:  0.059007 (0.1039)  Loss_traj: 69.864479 (69.9523)  Time: 1.226s,   26.10/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [20100/40036 ( 50%)]  Loss: 70.094704 (70.4713)  Loss_l1:  0.073207 (0.1040)  Loss_traj: 69.728668 (69.9515)  Time: 1.300s,   24.62/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [20150/40036 ( 50%)]  Loss: 70.681175 (70.4705)  Loss_l1:  0.068815 (0.1039)  Loss_traj: 70.337097 (69.9509)  Time: 1.231s,   26.00/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [20200/40036 ( 50%)]  Loss: 70.770126 (70.4713)  Loss_l1:  0.143550 (0.1038)  Loss_traj: 70.052376 (69.9521)  Time: 1.378s,   23.23/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [20250/40036 ( 51%)]  Loss: 70.506538 (70.4716)  Loss_l1:  0.033888 (0.1039)  Loss_traj: 70.337097 (69.9523)  Time: 1.243s,   25.74/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [20300/40036 ( 51%)]  Loss: 70.673210 (70.4717)  Loss_l1:  0.068015 (0.1038)  Loss_traj: 70.333138 (69.9527)  Time: 1.346s,   23.77/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [20350/40036 ( 51%)]  Loss: 69.875008 (70.4715)  Loss_l1:  0.070746 (0.1038)  Loss_traj: 69.521278 (69.9527)  Time: 1.249s,   25.62/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [20400/40036 ( 51%)]  Loss: 71.424683 (70.4714)  Loss_l1:  0.130496 (0.1037)  Loss_traj: 70.772202 (69.9527)  Time: 1.234s,   25.94/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [20450/40036 ( 51%)]  Loss: 71.874718 (70.4710)  Loss_l1:  0.168783 (0.1038)  Loss_traj: 71.030800 (69.9522)  Time: 1.336s,   23.95/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [20500/40036 ( 51%)]  Loss: 69.954628 (70.4705)  Loss_l1:  0.062004 (0.1037)  Loss_traj: 69.644608 (69.9518)  Time: 1.252s,   25.56/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [20550/40036 ( 51%)]  Loss: 70.870811 (70.4709)  Loss_l1:  0.052545 (0.1037)  Loss_traj: 70.608086 (69.9523)  Time: 1.331s,   24.05/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [20600/40036 ( 51%)]  Loss: 70.872765 (70.4707)  Loss_l1:  0.188689 (0.1038)  Loss_traj: 69.929321 (69.9519)  Time: 1.229s,   26.04/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [20650/40036 ( 52%)]  Loss: 71.566971 (70.4706)  Loss_l1:  0.151607 (0.1037)  Loss_traj: 70.808937 (69.9520)  Time: 1.331s,   24.04/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [20700/40036 ( 52%)]  Loss: 70.191444 (70.4703)  Loss_l1:  0.102444 (0.1037)  Loss_traj: 69.679222 (69.9516)  Time: 1.244s,   25.71/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [20750/40036 ( 52%)]  Loss: 70.293411 (70.4701)  Loss_l1:  0.081726 (0.1038)  Loss_traj: 69.884781 (69.9513)  Time: 1.259s,   25.42/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [20800/40036 ( 52%)]  Loss: 69.613205 (70.4699)  Loss_l1:  0.033866 (0.1037)  Loss_traj: 69.443878 (69.9511)  Time: 1.340s,   23.87/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [20850/40036 ( 52%)]  Loss: 71.318375 (70.4702)  Loss_l1:  0.031841 (0.1038)  Loss_traj: 71.159164 (69.9512)  Time: 1.233s,   25.95/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [20900/40036 ( 52%)]  Loss: 70.542747 (70.4708)  Loss_l1:  0.107792 (0.1038)  Loss_traj: 70.003784 (69.9519)  Time: 1.347s,   23.76/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [20950/40036 ( 52%)]  Loss: 68.695503 (70.4708)  Loss_l1:  0.031761 (0.1038)  Loss_traj: 68.536697 (69.9521)  Time: 1.260s,   25.40/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21000/40036 ( 52%)]  Loss: 68.901146 (70.4708)  Loss_l1:  0.003558 (0.1038)  Loss_traj: 68.883354 (69.9518)  Time: 1.325s,   24.14/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21050/40036 ( 53%)]  Loss: 71.009636 (70.4708)  Loss_l1:  0.064682 (0.1038)  Loss_traj: 70.686226 (69.9517)  Time: 1.223s,   26.17/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.019 (0.014)\n",
      "INFO:root:Train: 0 [21100/40036 ( 53%)]  Loss: 69.263885 (70.4711)  Loss_l1:  0.010407 (0.1038)  Loss_traj: 69.211853 (69.9520)  Time: 1.243s,   25.75/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21150/40036 ( 53%)]  Loss: 71.168694 (70.4716)  Loss_l1:  0.149266 (0.1039)  Loss_traj: 70.422363 (69.9523)  Time: 1.338s,   23.92/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21200/40036 ( 53%)]  Loss: 70.401276 (70.4721)  Loss_l1:  0.055172 (0.1039)  Loss_traj: 70.125412 (69.9527)  Time: 1.232s,   25.98/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21250/40036 ( 53%)]  Loss: 70.012596 (70.4713)  Loss_l1:  0.020416 (0.1039)  Loss_traj: 69.910515 (69.9519)  Time: 1.327s,   24.12/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [21300/40036 ( 53%)]  Loss: 70.503189 (70.4717)  Loss_l1:  0.027264 (0.1039)  Loss_traj: 70.366867 (69.9522)  Time: 1.241s,   25.78/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21350/40036 ( 53%)]  Loss: 71.488594 (70.4721)  Loss_l1:  0.364372 (0.1039)  Loss_traj: 69.666733 (69.9527)  Time: 1.316s,   24.32/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21400/40036 ( 53%)]  Loss: 70.685959 (70.4725)  Loss_l1:  0.214893 (0.1039)  Loss_traj: 69.611496 (69.9531)  Time: 1.323s,   24.19/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [21450/40036 ( 54%)]  Loss: 70.807709 (70.4723)  Loss_l1:  0.184713 (0.1039)  Loss_traj: 69.884148 (69.9530)  Time: 1.238s,   25.86/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21500/40036 ( 54%)]  Loss: 70.380310 (70.4722)  Loss_l1:  0.077628 (0.1039)  Loss_traj: 69.992172 (69.9528)  Time: 1.337s,   23.93/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21550/40036 ( 54%)]  Loss: 69.379608 (70.4725)  Loss_l1:  0.088008 (0.1039)  Loss_traj: 68.939568 (69.9532)  Time: 1.243s,   25.74/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21600/40036 ( 54%)]  Loss: 70.807327 (70.4724)  Loss_l1:  0.065244 (0.1038)  Loss_traj: 70.481102 (69.9532)  Time: 1.350s,   23.71/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21650/40036 ( 54%)]  Loss: 68.746254 (70.4724)  Loss_l1:  0.043323 (0.1038)  Loss_traj: 68.529640 (69.9532)  Time: 1.224s,   26.14/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.019 (0.014)\n",
      "INFO:root:Train: 0 [21700/40036 ( 54%)]  Loss: 69.599915 (70.4724)  Loss_l1:  0.089832 (0.1038)  Loss_traj: 69.150757 (69.9531)  Time: 1.348s,   23.74/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21750/40036 ( 54%)]  Loss: 71.710922 (70.4723)  Loss_l1:  0.138218 (0.1038)  Loss_traj: 71.019836 (69.9531)  Time: 1.350s,   23.70/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [21800/40036 ( 54%)]  Loss: 69.736786 (70.4720)  Loss_l1:  0.081258 (0.1039)  Loss_traj: 69.330498 (69.9525)  Time: 1.222s,   26.18/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [21850/40036 ( 55%)]  Loss: 70.117882 (70.4723)  Loss_l1:  0.042635 (0.1039)  Loss_traj: 69.904709 (69.9526)  Time: 1.317s,   24.31/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [21900/40036 ( 55%)]  Loss: 71.031013 (70.4723)  Loss_l1:  0.045706 (0.1039)  Loss_traj: 70.802483 (69.9527)  Time: 1.237s,   25.87/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [21950/40036 ( 55%)]  Loss: 72.534409 (70.4719)  Loss_l1:  0.105577 (0.1040)  Loss_traj: 72.006523 (69.9521)  Time: 1.330s,   24.06/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [22000/40036 ( 55%)]  Loss: 70.637833 (70.4720)  Loss_l1:  0.172687 (0.1040)  Loss_traj: 69.774399 (69.9522)  Time: 1.254s,   25.52/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [22050/40036 ( 55%)]  Loss: 69.264000 (70.4721)  Loss_l1:  0.105041 (0.1040)  Loss_traj: 68.738800 (69.9522)  Time: 1.355s,   23.62/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [22100/40036 ( 55%)]  Loss: 70.844780 (70.4723)  Loss_l1:  0.093346 (0.1040)  Loss_traj: 70.378052 (69.9525)  Time: 1.357s,   23.58/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [22150/40036 ( 55%)]  Loss: 71.691978 (70.4720)  Loss_l1:  0.101440 (0.1039)  Loss_traj: 71.184776 (69.9523)  Time: 1.226s,   26.10/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [22200/40036 ( 55%)]  Loss: 69.451782 (70.4718)  Loss_l1:  0.012898 (0.1039)  Loss_traj: 69.387291 (69.9521)  Time: 1.318s,   24.28/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [22250/40036 ( 56%)]  Loss: 68.766289 (70.4716)  Loss_l1:  0.036305 (0.1040)  Loss_traj: 68.584763 (69.9517)  Time: 1.230s,   26.02/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [22300/40036 ( 56%)]  Loss: 70.297272 (70.4713)  Loss_l1:  0.071512 (0.1039)  Loss_traj: 69.939713 (69.9516)  Time: 1.342s,   23.85/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [22350/40036 ( 56%)]  Loss: 70.231705 (70.4711)  Loss_l1:  0.157052 (0.1040)  Loss_traj: 69.446442 (69.9513)  Time: 1.229s,   26.04/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [22400/40036 ( 56%)]  Loss: 69.034737 (70.4707)  Loss_l1:  0.123305 (0.1040)  Loss_traj: 68.418213 (69.9509)  Time: 1.345s,   23.79/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22450/40036 ( 56%)]  Loss: 70.197083 (70.4706)  Loss_l1:  0.056043 (0.1040)  Loss_traj: 69.916870 (69.9508)  Time: 1.360s,   23.52/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22500/40036 ( 56%)]  Loss: 72.834305 (70.4713)  Loss_l1:  0.110640 (0.1040)  Loss_traj: 72.281105 (69.9515)  Time: 1.218s,   26.26/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [22550/40036 ( 56%)]  Loss: 70.871223 (70.4712)  Loss_l1:  0.130621 (0.1040)  Loss_traj: 70.218117 (69.9513)  Time: 1.288s,   24.84/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22600/40036 ( 56%)]  Loss: 71.252457 (70.4711)  Loss_l1:  0.160960 (0.1040)  Loss_traj: 70.447655 (69.9513)  Time: 1.244s,   25.73/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22650/40036 ( 57%)]  Loss: 70.242592 (70.4711)  Loss_l1:  0.044406 (0.1039)  Loss_traj: 70.020561 (69.9515)  Time: 1.338s,   23.92/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22700/40036 ( 57%)]  Loss: 71.532028 (70.4709)  Loss_l1:  0.274288 (0.1039)  Loss_traj: 70.160591 (69.9514)  Time: 1.225s,   26.13/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22750/40036 ( 57%)]  Loss: 70.645973 (70.4710)  Loss_l1:  0.214557 (0.1039)  Loss_traj: 69.573189 (69.9515)  Time: 1.350s,   23.70/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22800/40036 ( 57%)]  Loss: 73.046608 (70.4708)  Loss_l1:  0.143351 (0.1039)  Loss_traj: 72.329849 (69.9512)  Time: 1.298s,   24.65/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.020 (0.014)\n",
      "INFO:root:Train: 0 [22850/40036 ( 57%)]  Loss: 72.613571 (70.4704)  Loss_l1:  0.296512 (0.1039)  Loss_traj: 71.131012 (69.9507)  Time: 1.228s,   26.05/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [22900/40036 ( 57%)]  Loss: 70.747566 (70.4705)  Loss_l1:  0.172905 (0.1039)  Loss_traj: 69.883041 (69.9509)  Time: 1.363s,   23.48/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [22950/40036 ( 57%)]  Loss: 68.954926 (70.4703)  Loss_l1:  0.078218 (0.1039)  Loss_traj: 68.563835 (69.9506)  Time: 1.224s,   26.14/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23000/40036 ( 57%)]  Loss: 71.755356 (70.4703)  Loss_l1:  0.111239 (0.1039)  Loss_traj: 71.199158 (69.9506)  Time: 1.328s,   24.09/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23050/40036 ( 58%)]  Loss: 70.496269 (70.4704)  Loss_l1:  0.030565 (0.1040)  Loss_traj: 70.343445 (69.9505)  Time: 1.245s,   25.70/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23100/40036 ( 58%)]  Loss: 69.295532 (70.4701)  Loss_l1:  0.067783 (0.1040)  Loss_traj: 68.956619 (69.9501)  Time: 1.350s,   23.71/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23150/40036 ( 58%)]  Loss: 73.326645 (70.4702)  Loss_l1:  0.242258 (0.1040)  Loss_traj: 72.115356 (69.9503)  Time: 1.305s,   24.52/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23200/40036 ( 58%)]  Loss: 69.808914 (70.4701)  Loss_l1:  0.031691 (0.1039)  Loss_traj: 69.650459 (69.9505)  Time: 1.245s,   25.70/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23250/40036 ( 58%)]  Loss: 69.712204 (70.4702)  Loss_l1:  0.242697 (0.1039)  Loss_traj: 68.498718 (69.9505)  Time: 1.330s,   24.06/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23300/40036 ( 58%)]  Loss: 71.900169 (70.4700)  Loss_l1:  0.161810 (0.1040)  Loss_traj: 71.091118 (69.9503)  Time: 1.215s,   26.34/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23350/40036 ( 58%)]  Loss: 71.518478 (70.4698)  Loss_l1:  0.039215 (0.1040)  Loss_traj: 71.322403 (69.9499)  Time: 1.358s,   23.57/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23400/40036 ( 58%)]  Loss: 70.050522 (70.4702)  Loss_l1:  0.005587 (0.1040)  Loss_traj: 70.022583 (69.9504)  Time: 1.222s,   26.19/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23450/40036 ( 59%)]  Loss: 69.623207 (70.4703)  Loss_l1:  0.009032 (0.1040)  Loss_traj: 69.578049 (69.9504)  Time: 1.315s,   24.33/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23500/40036 ( 59%)]  Loss: 70.875633 (70.4704)  Loss_l1:  0.208658 (0.1040)  Loss_traj: 69.832344 (69.9505)  Time: 1.231s,   25.99/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23550/40036 ( 59%)]  Loss: 69.677353 (70.4701)  Loss_l1:  0.147909 (0.1040)  Loss_traj: 68.937805 (69.9501)  Time: 1.226s,   26.10/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23600/40036 ( 59%)]  Loss: 70.850868 (70.4706)  Loss_l1:  0.050978 (0.1040)  Loss_traj: 70.595978 (69.9504)  Time: 1.341s,   23.86/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [23650/40036 ( 59%)]  Loss: 69.503761 (70.4708)  Loss_l1:  0.041418 (0.1041)  Loss_traj: 69.296669 (69.9506)  Time: 1.277s,   25.05/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [23700/40036 ( 59%)]  Loss: 68.747040 (70.4702)  Loss_l1:  0.067375 (0.1040)  Loss_traj: 68.410164 (69.9501)  Time: 1.305s,   24.52/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [23750/40036 ( 59%)]  Loss: 70.336159 (70.4701)  Loss_l1:  0.031091 (0.1040)  Loss_traj: 70.180702 (69.9501)  Time: 1.228s,   26.05/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [23800/40036 ( 59%)]  Loss: 70.480827 (70.4697)  Loss_l1:  0.023912 (0.1040)  Loss_traj: 70.361267 (69.9497)  Time: 1.325s,   24.15/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [23850/40036 ( 60%)]  Loss: 69.552467 (70.4694)  Loss_l1:  0.187546 (0.1040)  Loss_traj: 68.614738 (69.9496)  Time: 1.319s,   24.27/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [23900/40036 ( 60%)]  Loss: 69.020576 (70.4689)  Loss_l1:  0.079241 (0.1040)  Loss_traj: 68.624374 (69.9490)  Time: 1.247s,   25.66/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [23950/40036 ( 60%)]  Loss: 70.372490 (70.4689)  Loss_l1:  0.208462 (0.1039)  Loss_traj: 69.330185 (69.9492)  Time: 1.328s,   24.10/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24000/40036 ( 60%)]  Loss: 71.047569 (70.4691)  Loss_l1:  0.067710 (0.1039)  Loss_traj: 70.709015 (69.9494)  Time: 1.260s,   25.41/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24050/40036 ( 60%)]  Loss: 68.896744 (70.4691)  Loss_l1:  0.002335 (0.1040)  Loss_traj: 68.885071 (69.9493)  Time: 1.346s,   23.78/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24100/40036 ( 60%)]  Loss: 69.833618 (70.4685)  Loss_l1:  0.161358 (0.1039)  Loss_traj: 69.026825 (69.9488)  Time: 1.247s,   25.66/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24150/40036 ( 60%)]  Loss: 70.492035 (70.4688)  Loss_l1:  0.003290 (0.1039)  Loss_traj: 70.475586 (69.9491)  Time: 1.342s,   23.85/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24200/40036 ( 60%)]  Loss: 71.119843 (70.4688)  Loss_l1:  0.009198 (0.1039)  Loss_traj: 71.073853 (69.9494)  Time: 1.299s,   24.63/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24250/40036 ( 61%)]  Loss: 71.586159 (70.4691)  Loss_l1:  0.037360 (0.1039)  Loss_traj: 71.399361 (69.9497)  Time: 1.224s,   26.15/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24300/40036 ( 61%)]  Loss: 71.796165 (70.4693)  Loss_l1:  0.051167 (0.1039)  Loss_traj: 71.540329 (69.9499)  Time: 1.306s,   24.50/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [24350/40036 ( 61%)]  Loss: 70.595741 (70.4691)  Loss_l1:  0.077755 (0.1039)  Loss_traj: 70.206963 (69.9497)  Time: 1.232s,   25.96/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [24400/40036 ( 61%)]  Loss: 72.138298 (70.4693)  Loss_l1:  0.174662 (0.1038)  Loss_traj: 71.264992 (69.9501)  Time: 1.337s,   23.94/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [24450/40036 ( 61%)]  Loss: 70.901802 (70.4693)  Loss_l1:  0.084114 (0.1038)  Loss_traj: 70.481232 (69.9502)  Time: 1.216s,   26.32/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [24500/40036 ( 61%)]  Loss: 71.207558 (70.4692)  Loss_l1:  0.158861 (0.1038)  Loss_traj: 70.413254 (69.9501)  Time: 1.343s,   23.84/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [24550/40036 ( 61%)]  Loss: 68.409546 (70.4691)  Loss_l1:  0.049268 (0.1038)  Loss_traj: 68.163208 (69.9500)  Time: 1.295s,   24.70/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [24600/40036 ( 61%)]  Loss: 70.818993 (70.4689)  Loss_l1:  0.077488 (0.1038)  Loss_traj: 70.431549 (69.9499)  Time: 1.240s,   25.80/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [24650/40036 ( 62%)]  Loss: 69.234200 (70.4689)  Loss_l1:  0.097051 (0.1038)  Loss_traj: 68.748947 (69.9497)  Time: 1.313s,   24.38/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [24700/40036 ( 62%)]  Loss: 68.433350 (70.4683)  Loss_l1:  0.019850 (0.1039)  Loss_traj: 68.334099 (69.9490)  Time: 1.231s,   25.99/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [24750/40036 ( 62%)]  Loss: 71.364395 (70.4688)  Loss_l1:  0.049354 (0.1039)  Loss_traj: 71.117630 (69.9494)  Time: 1.344s,   23.81/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [24800/40036 ( 62%)]  Loss: 69.935249 (70.4687)  Loss_l1:  0.097860 (0.1039)  Loss_traj: 69.445946 (69.9494)  Time: 1.252s,   25.56/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [24850/40036 ( 62%)]  Loss: 69.317780 (70.4684)  Loss_l1:  0.147993 (0.1039)  Loss_traj: 68.577812 (69.9491)  Time: 1.329s,   24.07/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [24900/40036 ( 62%)]  Loss: 69.513435 (70.4683)  Loss_l1:  0.094349 (0.1039)  Loss_traj: 69.041687 (69.9488)  Time: 1.319s,   24.26/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [24950/40036 ( 62%)]  Loss: 71.054787 (70.4678)  Loss_l1:  0.031531 (0.1039)  Loss_traj: 70.897133 (69.9484)  Time: 1.228s,   26.06/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25000/40036 ( 62%)]  Loss: 70.322327 (70.4672)  Loss_l1:  0.007564 (0.1038)  Loss_traj: 70.284508 (69.9479)  Time: 1.320s,   24.24/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.018 (0.014)\n",
      "INFO:root:Train: 0 [25050/40036 ( 63%)]  Loss: 70.984596 (70.4672)  Loss_l1:  0.072328 (0.1038)  Loss_traj: 70.622955 (69.9479)  Time: 1.258s,   25.43/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25100/40036 ( 63%)]  Loss: 68.346245 (70.4673)  Loss_l1:  0.010112 (0.1038)  Loss_traj: 68.295685 (69.9482)  Time: 1.343s,   23.83/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [25150/40036 ( 63%)]  Loss: 69.341721 (70.4677)  Loss_l1:  0.003779 (0.1038)  Loss_traj: 69.322823 (69.9486)  Time: 1.228s,   26.06/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25200/40036 ( 63%)]  Loss: 71.227249 (70.4675)  Loss_l1:  0.168799 (0.1038)  Loss_traj: 70.383255 (69.9484)  Time: 1.320s,   24.24/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25250/40036 ( 63%)]  Loss: 69.547218 (70.4673)  Loss_l1:  0.038376 (0.1038)  Loss_traj: 69.355339 (69.9482)  Time: 1.297s,   24.67/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25300/40036 ( 63%)]  Loss: 71.523170 (70.4673)  Loss_l1:  0.060313 (0.1038)  Loss_traj: 71.221611 (69.9483)  Time: 1.246s,   25.68/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25350/40036 ( 63%)]  Loss: 69.916336 (70.4673)  Loss_l1:  0.093432 (0.1038)  Loss_traj: 69.449173 (69.9483)  Time: 1.307s,   24.49/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25400/40036 ( 63%)]  Loss: 71.327850 (70.4674)  Loss_l1:  0.103164 (0.1039)  Loss_traj: 70.812035 (69.9481)  Time: 1.229s,   26.05/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25450/40036 ( 64%)]  Loss: 68.926117 (70.4673)  Loss_l1:  0.116738 (0.1039)  Loss_traj: 68.342430 (69.9480)  Time: 1.323s,   24.20/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25500/40036 ( 64%)]  Loss: 69.043922 (70.4671)  Loss_l1:  0.004403 (0.1039)  Loss_traj: 69.021904 (69.9477)  Time: 1.226s,   26.10/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25550/40036 ( 64%)]  Loss: 69.972740 (70.4668)  Loss_l1:  0.009763 (0.1039)  Loss_traj: 69.923920 (69.9475)  Time: 1.326s,   24.14/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [25600/40036 ( 64%)]  Loss: 71.429474 (70.4666)  Loss_l1:  0.012673 (0.1039)  Loss_traj: 71.366112 (69.9473)  Time: 1.346s,   23.77/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25650/40036 ( 64%)]  Loss: 70.189728 (70.4672)  Loss_l1:  0.046315 (0.1039)  Loss_traj: 69.958153 (69.9477)  Time: 1.233s,   25.95/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25700/40036 ( 64%)]  Loss: 70.584183 (70.4672)  Loss_l1:  0.020368 (0.1039)  Loss_traj: 70.482346 (69.9476)  Time: 1.286s,   24.88/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25750/40036 ( 64%)]  Loss: 69.816322 (70.4671)  Loss_l1:  0.135849 (0.1039)  Loss_traj: 69.137077 (69.9473)  Time: 1.218s,   26.26/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25800/40036 ( 64%)]  Loss: 68.924576 (70.4671)  Loss_l1:  0.040794 (0.1040)  Loss_traj: 68.720604 (69.9473)  Time: 1.323s,   24.19/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [25850/40036 ( 65%)]  Loss: 69.724663 (70.4674)  Loss_l1:  0.172831 (0.1040)  Loss_traj: 68.860512 (69.9476)  Time: 1.225s,   26.13/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25900/40036 ( 65%)]  Loss: 71.843719 (70.4678)  Loss_l1:  0.165653 (0.1040)  Loss_traj: 71.015457 (69.9480)  Time: 1.340s,   23.88/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [25950/40036 ( 65%)]  Loss: 68.666862 (70.4676)  Loss_l1:  0.129379 (0.1040)  Loss_traj: 68.019966 (69.9478)  Time: 1.354s,   23.64/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26000/40036 ( 65%)]  Loss: 69.353714 (70.4673)  Loss_l1:  0.065023 (0.1040)  Loss_traj: 69.028595 (69.9475)  Time: 1.211s,   26.43/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26050/40036 ( 65%)]  Loss: 68.308189 (70.4678)  Loss_l1:  0.026067 (0.1040)  Loss_traj: 68.177856 (69.9479)  Time: 1.347s,   23.75/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26100/40036 ( 65%)]  Loss: 68.050537 (70.4674)  Loss_l1:  0.002301 (0.1039)  Loss_traj: 68.039032 (69.9477)  Time: 1.252s,   25.57/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26150/40036 ( 65%)]  Loss: 69.127197 (70.4672)  Loss_l1:  0.026076 (0.1039)  Loss_traj: 68.996819 (69.9475)  Time: 1.365s,   23.45/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [26200/40036 ( 65%)]  Loss: 70.469528 (70.4672)  Loss_l1:  0.081360 (0.1039)  Loss_traj: 70.062729 (69.9475)  Time: 1.240s,   25.81/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26250/40036 ( 66%)]  Loss: 70.595840 (70.4674)  Loss_l1:  0.236521 (0.1039)  Loss_traj: 69.413239 (69.9477)  Time: 1.322s,   24.20/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26300/40036 ( 66%)]  Loss: 70.003769 (70.4675)  Loss_l1:  0.001816 (0.1040)  Loss_traj: 69.994690 (69.9477)  Time: 1.347s,   23.76/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26350/40036 ( 66%)]  Loss: 71.188354 (70.4678)  Loss_l1:  0.038681 (0.1040)  Loss_traj: 70.994949 (69.9479)  Time: 1.234s,   25.94/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26400/40036 ( 66%)]  Loss: 69.433220 (70.4675)  Loss_l1:  0.042953 (0.1040)  Loss_traj: 69.218452 (69.9477)  Time: 1.319s,   24.27/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26450/40036 ( 66%)]  Loss: 70.536598 (70.4678)  Loss_l1:  0.136031 (0.1040)  Loss_traj: 69.856445 (69.9479)  Time: 1.252s,   25.57/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26500/40036 ( 66%)]  Loss: 70.894913 (70.4678)  Loss_l1:  0.115140 (0.1040)  Loss_traj: 70.319214 (69.9479)  Time: 1.341s,   23.86/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [26550/40036 ( 66%)]  Loss: 69.909660 (70.4678)  Loss_l1:  0.176958 (0.1040)  Loss_traj: 69.024872 (69.9479)  Time: 1.240s,   25.80/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [26600/40036 ( 66%)]  Loss: 72.610596 (70.4675)  Loss_l1:  0.279834 (0.1040)  Loss_traj: 71.211426 (69.9478)  Time: 1.347s,   23.76/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [26650/40036 ( 67%)]  Loss: 70.556786 (70.4678)  Loss_l1:  0.028575 (0.1039)  Loss_traj: 70.413910 (69.9481)  Time: 1.257s,   25.46/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [26700/40036 ( 67%)]  Loss: 69.363945 (70.4679)  Loss_l1:  0.113015 (0.1039)  Loss_traj: 68.798866 (69.9484)  Time: 1.248s,   25.64/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [26750/40036 ( 67%)]  Loss: 70.705490 (70.4680)  Loss_l1:  0.027986 (0.1039)  Loss_traj: 70.565559 (69.9483)  Time: 1.320s,   24.25/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [26800/40036 ( 67%)]  Loss: 70.784142 (70.4681)  Loss_l1:  0.123922 (0.1039)  Loss_traj: 70.164528 (69.9485)  Time: 1.233s,   25.96/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [26850/40036 ( 67%)]  Loss: 70.031525 (70.4680)  Loss_l1:  0.056901 (0.1039)  Loss_traj: 69.747017 (69.9483)  Time: 1.351s,   23.69/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [26900/40036 ( 67%)]  Loss: 70.617226 (70.4678)  Loss_l1:  0.018965 (0.1039)  Loss_traj: 70.522400 (69.9483)  Time: 1.226s,   26.11/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [26950/40036 ( 67%)]  Loss: 69.803818 (70.4679)  Loss_l1:  0.236926 (0.1040)  Loss_traj: 68.619186 (69.9481)  Time: 1.336s,   23.96/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27000/40036 ( 67%)]  Loss: 70.260880 (70.4681)  Loss_l1:  0.017149 (0.1039)  Loss_traj: 70.175133 (69.9483)  Time: 1.307s,   24.48/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27050/40036 ( 68%)]  Loss: 70.435150 (70.4678)  Loss_l1:  0.203831 (0.1040)  Loss_traj: 69.415993 (69.9480)  Time: 1.233s,   25.95/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27100/40036 ( 68%)]  Loss: 70.142242 (70.4681)  Loss_l1:  0.052528 (0.1040)  Loss_traj: 69.879608 (69.9481)  Time: 1.306s,   24.49/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27150/40036 ( 68%)]  Loss: 69.570290 (70.4681)  Loss_l1:  0.053997 (0.1041)  Loss_traj: 69.300301 (69.9478)  Time: 1.244s,   25.73/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27200/40036 ( 68%)]  Loss: 70.682510 (70.4683)  Loss_l1:  0.026598 (0.1040)  Loss_traj: 70.549522 (69.9482)  Time: 1.333s,   24.01/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27250/40036 ( 68%)]  Loss: 70.922935 (70.4683)  Loss_l1:  0.018074 (0.1041)  Loss_traj: 70.832565 (69.9481)  Time: 1.224s,   26.15/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27300/40036 ( 68%)]  Loss: 72.021896 (70.4685)  Loss_l1:  0.112108 (0.1040)  Loss_traj: 71.461357 (69.9482)  Time: 1.314s,   24.34/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [27350/40036 ( 68%)]  Loss: 68.635590 (70.4685)  Loss_l1:  0.065726 (0.1040)  Loss_traj: 68.306961 (69.9483)  Time: 1.354s,   23.64/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [27400/40036 ( 68%)]  Loss: 69.556381 (70.4685)  Loss_l1:  0.076026 (0.1041)  Loss_traj: 69.176254 (69.9482)  Time: 1.231s,   25.99/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [27450/40036 ( 69%)]  Loss: 71.089165 (70.4684)  Loss_l1:  0.223722 (0.1040)  Loss_traj: 69.970551 (69.9482)  Time: 1.277s,   25.05/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [27500/40036 ( 69%)]  Loss: 69.068115 (70.4684)  Loss_l1:  0.034824 (0.1040)  Loss_traj: 68.893997 (69.9483)  Time: 1.225s,   26.12/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [27550/40036 ( 69%)]  Loss: 71.529129 (70.4687)  Loss_l1:  0.009381 (0.1040)  Loss_traj: 71.482224 (69.9485)  Time: 1.340s,   23.88/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27600/40036 ( 69%)]  Loss: 69.257904 (70.4686)  Loss_l1:  0.115225 (0.1040)  Loss_traj: 68.681778 (69.9484)  Time: 1.233s,   25.94/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [27650/40036 ( 69%)]  Loss: 72.130562 (70.4688)  Loss_l1:  0.148373 (0.1040)  Loss_traj: 71.388695 (69.9485)  Time: 1.330s,   24.06/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [27700/40036 ( 69%)]  Loss: 71.233032 (70.4691)  Loss_l1:  0.108523 (0.1041)  Loss_traj: 70.690414 (69.9488)  Time: 1.307s,   24.48/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [27750/40036 ( 69%)]  Loss: 70.484627 (70.4692)  Loss_l1:  0.084338 (0.1041)  Loss_traj: 70.062935 (69.9488)  Time: 1.240s,   25.82/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27800/40036 ( 69%)]  Loss: 70.226028 (70.4687)  Loss_l1:  0.059335 (0.1041)  Loss_traj: 69.929352 (69.9484)  Time: 1.322s,   24.21/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [27850/40036 ( 70%)]  Loss: 70.037170 (70.4685)  Loss_l1:  0.094765 (0.1040)  Loss_traj: 69.563347 (69.9482)  Time: 1.258s,   25.44/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [27900/40036 ( 70%)]  Loss: 67.889305 (70.4688)  Loss_l1:  0.109444 (0.1040)  Loss_traj: 67.342087 (69.9486)  Time: 1.339s,   23.91/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [27950/40036 ( 70%)]  Loss: 69.672432 (70.4690)  Loss_l1:  0.035997 (0.1040)  Loss_traj: 69.492447 (69.9489)  Time: 1.242s,   25.76/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28000/40036 ( 70%)]  Loss: 69.723480 (70.4686)  Loss_l1:  0.112788 (0.1040)  Loss_traj: 69.159538 (69.9485)  Time: 1.325s,   24.14/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28050/40036 ( 70%)]  Loss: 70.986496 (70.4682)  Loss_l1:  0.168144 (0.1041)  Loss_traj: 70.145775 (69.9479)  Time: 1.345s,   23.80/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [28100/40036 ( 70%)]  Loss: 72.495987 (70.4684)  Loss_l1:  0.236102 (0.1040)  Loss_traj: 71.315475 (69.9482)  Time: 1.245s,   25.71/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28150/40036 ( 70%)]  Loss: 70.385544 (70.4678)  Loss_l1:  0.118751 (0.1040)  Loss_traj: 69.791794 (69.9479)  Time: 1.327s,   24.11/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28200/40036 ( 70%)]  Loss: 70.951370 (70.4679)  Loss_l1:  0.016502 (0.1040)  Loss_traj: 70.868858 (69.9480)  Time: 1.241s,   25.78/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28250/40036 ( 71%)]  Loss: 69.673775 (70.4678)  Loss_l1:  0.035722 (0.1040)  Loss_traj: 69.495171 (69.9479)  Time: 1.340s,   23.88/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28300/40036 ( 71%)]  Loss: 69.937157 (70.4675)  Loss_l1:  0.097712 (0.1040)  Loss_traj: 69.448601 (69.9476)  Time: 1.236s,   25.89/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28350/40036 ( 71%)]  Loss: 68.938309 (70.4675)  Loss_l1:  0.038448 (0.1040)  Loss_traj: 68.746071 (69.9477)  Time: 1.319s,   24.25/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28400/40036 ( 71%)]  Loss: 70.122421 (70.4675)  Loss_l1:  0.074753 (0.1040)  Loss_traj: 69.748657 (69.9476)  Time: 1.240s,   25.81/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28450/40036 ( 71%)]  Loss: 71.138565 (70.4679)  Loss_l1:  0.151008 (0.1040)  Loss_traj: 70.383530 (69.9480)  Time: 1.219s,   26.26/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28500/40036 ( 71%)]  Loss: 70.059174 (70.4675)  Loss_l1:  0.033224 (0.1040)  Loss_traj: 69.893059 (69.9476)  Time: 1.316s,   24.32/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28550/40036 ( 71%)]  Loss: 70.490044 (70.4679)  Loss_l1:  0.091514 (0.1040)  Loss_traj: 70.032478 (69.9480)  Time: 1.251s,   25.59/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28600/40036 ( 71%)]  Loss: 70.573570 (70.4675)  Loss_l1:  0.182402 (0.1040)  Loss_traj: 69.661560 (69.9476)  Time: 1.359s,   23.54/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28650/40036 ( 72%)]  Loss: 68.038002 (70.4677)  Loss_l1:  0.080170 (0.1040)  Loss_traj: 67.637154 (69.9477)  Time: 1.246s,   25.68/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28700/40036 ( 72%)]  Loss: 69.970192 (70.4679)  Loss_l1:  0.026924 (0.1040)  Loss_traj: 69.835571 (69.9477)  Time: 1.348s,   23.74/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28750/40036 ( 72%)]  Loss: 71.792267 (70.4679)  Loss_l1:  0.010125 (0.1041)  Loss_traj: 71.741638 (69.9477)  Time: 1.301s,   24.59/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [28800/40036 ( 72%)]  Loss: 70.160934 (70.4677)  Loss_l1:  0.125781 (0.1041)  Loss_traj: 69.532028 (69.9472)  Time: 1.248s,   25.64/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [28850/40036 ( 72%)]  Loss: 69.286308 (70.4676)  Loss_l1:  0.053072 (0.1041)  Loss_traj: 69.020950 (69.9473)  Time: 1.323s,   24.18/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [28900/40036 ( 72%)]  Loss: 70.094063 (70.4674)  Loss_l1:  0.126807 (0.1041)  Loss_traj: 69.460022 (69.9470)  Time: 1.224s,   26.14/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [28950/40036 ( 72%)]  Loss: 70.434677 (70.4672)  Loss_l1:  0.052156 (0.1040)  Loss_traj: 70.173897 (69.9470)  Time: 1.340s,   23.87/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [29000/40036 ( 72%)]  Loss: 70.236694 (70.4667)  Loss_l1:  0.062130 (0.1040)  Loss_traj: 69.926048 (69.9465)  Time: 1.255s,   25.50/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29050/40036 ( 73%)]  Loss: 70.576881 (70.4666)  Loss_l1:  0.477350 (0.1041)  Loss_traj: 68.190132 (69.9463)  Time: 1.340s,   23.88/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29100/40036 ( 73%)]  Loss: 70.574303 (70.4665)  Loss_l1:  0.025425 (0.1041)  Loss_traj: 70.447174 (69.9463)  Time: 1.342s,   23.85/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29150/40036 ( 73%)]  Loss: 69.425529 (70.4660)  Loss_l1:  0.190034 (0.1040)  Loss_traj: 68.475365 (69.9458)  Time: 1.237s,   25.88/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29200/40036 ( 73%)]  Loss: 69.966309 (70.4658)  Loss_l1:  0.002408 (0.1040)  Loss_traj: 69.954269 (69.9457)  Time: 1.308s,   24.47/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29250/40036 ( 73%)]  Loss: 68.814423 (70.4661)  Loss_l1:  0.149438 (0.1040)  Loss_traj: 68.067230 (69.9459)  Time: 1.215s,   26.33/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29300/40036 ( 73%)]  Loss: 71.313995 (70.4661)  Loss_l1:  0.210560 (0.1040)  Loss_traj: 70.261192 (69.9460)  Time: 1.348s,   23.75/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29350/40036 ( 73%)]  Loss: 71.451569 (70.4666)  Loss_l1:  0.219404 (0.1041)  Loss_traj: 70.354546 (69.9462)  Time: 1.249s,   25.62/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29400/40036 ( 73%)]  Loss: 70.630608 (70.4667)  Loss_l1:  0.050134 (0.1041)  Loss_traj: 70.379936 (69.9464)  Time: 1.346s,   23.77/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29450/40036 ( 74%)]  Loss: 69.547050 (70.4666)  Loss_l1:  0.169832 (0.1041)  Loss_traj: 68.697891 (69.9463)  Time: 1.227s,   26.09/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29500/40036 ( 74%)]  Loss: 69.415688 (70.4666)  Loss_l1:  0.021687 (0.1041)  Loss_traj: 69.307251 (69.9462)  Time: 1.221s,   26.20/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29550/40036 ( 74%)]  Loss: 71.868889 (70.4665)  Loss_l1:  0.154365 (0.1041)  Loss_traj: 71.097061 (69.9461)  Time: 1.326s,   24.13/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29600/40036 ( 74%)]  Loss: 70.940086 (70.4670)  Loss_l1:  0.217301 (0.1041)  Loss_traj: 69.853584 (69.9466)  Time: 1.233s,   25.96/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [29650/40036 ( 74%)]  Loss: 68.637207 (70.4667)  Loss_l1:  0.064026 (0.1041)  Loss_traj: 68.317078 (69.9463)  Time: 1.337s,   23.93/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29700/40036 ( 74%)]  Loss: 73.478859 (70.4669)  Loss_l1:  0.335715 (0.1041)  Loss_traj: 71.800285 (69.9462)  Time: 1.234s,   25.93/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29750/40036 ( 74%)]  Loss: 70.224754 (70.4672)  Loss_l1:  0.008471 (0.1042)  Loss_traj: 70.182404 (69.9463)  Time: 1.331s,   24.05/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29800/40036 ( 74%)]  Loss: 72.006271 (70.4671)  Loss_l1:  0.278715 (0.1042)  Loss_traj: 70.612694 (69.9462)  Time: 1.323s,   24.19/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [29850/40036 ( 75%)]  Loss: 70.723785 (70.4673)  Loss_l1:  0.030739 (0.1042)  Loss_traj: 70.570091 (69.9465)  Time: 1.247s,   25.67/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29900/40036 ( 75%)]  Loss: 71.177231 (70.4672)  Loss_l1:  0.043150 (0.1042)  Loss_traj: 70.961479 (69.9464)  Time: 1.350s,   23.70/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [29950/40036 ( 75%)]  Loss: 70.418198 (70.4673)  Loss_l1:  0.017238 (0.1041)  Loss_traj: 70.332008 (69.9466)  Time: 1.257s,   25.45/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30000/40036 ( 75%)]  Loss: 68.364944 (70.4675)  Loss_l1:  0.028222 (0.1041)  Loss_traj: 68.223831 (69.9468)  Time: 1.337s,   23.93/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30050/40036 ( 75%)]  Loss: 69.494858 (70.4679)  Loss_l1:  0.071704 (0.1042)  Loss_traj: 69.136337 (69.9470)  Time: 1.235s,   25.90/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30100/40036 ( 75%)]  Loss: 69.593719 (70.4680)  Loss_l1:  0.093243 (0.1042)  Loss_traj: 69.127502 (69.9470)  Time: 1.328s,   24.09/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30150/40036 ( 75%)]  Loss: 69.799026 (70.4682)  Loss_l1:  0.065490 (0.1042)  Loss_traj: 69.471573 (69.9471)  Time: 1.317s,   24.29/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30200/40036 ( 75%)]  Loss: 70.831963 (70.4681)  Loss_l1:  0.178495 (0.1042)  Loss_traj: 69.939491 (69.9472)  Time: 1.236s,   25.90/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30250/40036 ( 76%)]  Loss: 69.798904 (70.4686)  Loss_l1:  0.108055 (0.1042)  Loss_traj: 69.258629 (69.9477)  Time: 1.292s,   24.76/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30300/40036 ( 76%)]  Loss: 68.623314 (70.4685)  Loss_l1:  0.111395 (0.1042)  Loss_traj: 68.066338 (69.9476)  Time: 1.230s,   26.01/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30350/40036 ( 76%)]  Loss: 70.021011 (70.4683)  Loss_l1:  0.159759 (0.1042)  Loss_traj: 69.222214 (69.9474)  Time: 1.366s,   23.42/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [30400/40036 ( 76%)]  Loss: 69.635811 (70.4679)  Loss_l1:  0.138296 (0.1042)  Loss_traj: 68.944328 (69.9470)  Time: 1.237s,   25.87/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30450/40036 ( 76%)]  Loss: 71.479324 (70.4677)  Loss_l1:  0.008068 (0.1042)  Loss_traj: 71.438980 (69.9466)  Time: 1.343s,   23.83/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [30500/40036 ( 76%)]  Loss: 69.093163 (70.4677)  Loss_l1:  0.009501 (0.1042)  Loss_traj: 69.045662 (69.9466)  Time: 1.373s,   23.30/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30550/40036 ( 76%)]  Loss: 71.084145 (70.4678)  Loss_l1:  0.133008 (0.1042)  Loss_traj: 70.419106 (69.9466)  Time: 1.214s,   26.36/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30600/40036 ( 76%)]  Loss: 70.547165 (70.4683)  Loss_l1:  0.052027 (0.1042)  Loss_traj: 70.287033 (69.9471)  Time: 1.316s,   24.31/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30650/40036 ( 77%)]  Loss: 69.229424 (70.4676)  Loss_l1:  0.078821 (0.1042)  Loss_traj: 68.835320 (69.9464)  Time: 1.225s,   26.13/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30700/40036 ( 77%)]  Loss: 69.264587 (70.4678)  Loss_l1:  0.078286 (0.1042)  Loss_traj: 68.873161 (69.9467)  Time: 1.348s,   23.74/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [30750/40036 ( 77%)]  Loss: 69.521873 (70.4675)  Loss_l1:  0.103143 (0.1042)  Loss_traj: 69.006157 (69.9464)  Time: 1.220s,   26.23/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30800/40036 ( 77%)]  Loss: 73.015411 (70.4676)  Loss_l1:  0.295205 (0.1042)  Loss_traj: 71.539383 (69.9465)  Time: 1.327s,   24.12/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30850/40036 ( 77%)]  Loss: 70.773125 (70.4676)  Loss_l1:  0.061755 (0.1042)  Loss_traj: 70.464348 (69.9465)  Time: 1.343s,   23.83/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30900/40036 ( 77%)]  Loss: 70.019104 (70.4676)  Loss_l1:  0.136838 (0.1042)  Loss_traj: 69.334915 (69.9467)  Time: 1.235s,   25.92/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [30950/40036 ( 77%)]  Loss: 72.016312 (70.4680)  Loss_l1:  0.165636 (0.1042)  Loss_traj: 71.188133 (69.9470)  Time: 1.337s,   23.93/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [31000/40036 ( 77%)]  Loss: 71.468330 (70.4678)  Loss_l1:  0.225078 (0.1042)  Loss_traj: 70.342941 (69.9467)  Time: 1.299s,   24.63/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [31050/40036 ( 78%)]  Loss: 70.618515 (70.4680)  Loss_l1:  0.063591 (0.1042)  Loss_traj: 70.300560 (69.9469)  Time: 1.324s,   24.16/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [31100/40036 ( 78%)]  Loss: 71.561516 (70.4681)  Loss_l1:  0.185200 (0.1042)  Loss_traj: 70.635513 (69.9469)  Time: 1.249s,   25.61/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [31150/40036 ( 78%)]  Loss: 70.563629 (70.4682)  Loss_l1:  0.154082 (0.1042)  Loss_traj: 69.793221 (69.9471)  Time: 1.327s,   24.11/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [31200/40036 ( 78%)]  Loss: 69.535759 (70.4683)  Loss_l1:  0.114718 (0.1042)  Loss_traj: 68.962166 (69.9473)  Time: 1.338s,   23.92/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [31250/40036 ( 78%)]  Loss: 70.583450 (70.4684)  Loss_l1:  0.033918 (0.1042)  Loss_traj: 70.413857 (69.9475)  Time: 1.229s,   26.04/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 0 [31300/40036 ( 78%)]  Loss: 70.400826 (70.4687)  Loss_l1:  0.084966 (0.1042)  Loss_traj: 69.975990 (69.9477)  Time: 1.354s,   23.63/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [31350/40036 ( 78%)]  Loss: 69.095253 (70.4686)  Loss_l1:  0.048296 (0.1042)  Loss_traj: 68.853775 (69.9478)  Time: 1.259s,   25.42/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [31400/40036 ( 78%)]  Loss: 69.531898 (70.4688)  Loss_l1:  0.015703 (0.1042)  Loss_traj: 69.453384 (69.9479)  Time: 1.333s,   24.01/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [31450/40036 ( 79%)]  Loss: 69.862518 (70.4685)  Loss_l1:  0.026531 (0.1042)  Loss_traj: 69.729866 (69.9476)  Time: 1.224s,   26.15/s  (1.286s,   24.88/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [31500/40036 ( 79%)]  Loss: 71.482986 (70.4682)  Loss_l1:  0.155737 (0.1042)  Loss_traj: 70.704300 (69.9474)  Time: 1.339s,   23.91/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [31550/40036 ( 79%)]  Loss: 70.810135 (70.4683)  Loss_l1:  0.306838 (0.1042)  Loss_traj: 69.275940 (69.9475)  Time: 1.339s,   23.90/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [31600/40036 ( 79%)]  Loss: 70.059631 (70.4686)  Loss_l1:  0.063309 (0.1042)  Loss_traj: 69.743088 (69.9477)  Time: 1.229s,   26.05/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.017 (0.014)\n",
      "INFO:root:Train: 0 [31650/40036 ( 79%)]  Loss: 70.838867 (70.4689)  Loss_l1:  0.205341 (0.1042)  Loss_traj: 69.812164 (69.9481)  Time: 1.353s,   23.65/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [31700/40036 ( 79%)]  Loss: 69.706131 (70.4689)  Loss_l1:  0.095173 (0.1042)  Loss_traj: 69.230263 (69.9482)  Time: 1.255s,   25.51/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [31750/40036 ( 79%)]  Loss: 68.823326 (70.4689)  Loss_l1:  0.043002 (0.1042)  Loss_traj: 68.608315 (69.9480)  Time: 1.337s,   23.93/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [31800/40036 ( 79%)]  Loss: 70.054619 (70.4690)  Loss_l1:  0.111583 (0.1042)  Loss_traj: 69.496704 (69.9481)  Time: 1.262s,   25.35/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 0 [31850/40036 ( 80%)]  Loss: 68.696602 (70.4690)  Loss_l1:  0.071646 (0.1042)  Loss_traj: 68.338371 (69.9481)  Time: 1.318s,   24.29/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [31900/40036 ( 80%)]  Loss: 69.813904 (70.4690)  Loss_l1:  0.006412 (0.1042)  Loss_traj: 69.781845 (69.9479)  Time: 1.333s,   24.01/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [31950/40036 ( 80%)]  Loss: 69.629738 (70.4690)  Loss_l1:  0.117338 (0.1042)  Loss_traj: 69.043045 (69.9479)  Time: 1.249s,   25.63/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32000/40036 ( 80%)]  Loss: 70.060341 (70.4694)  Loss_l1:  0.095062 (0.1043)  Loss_traj: 69.585030 (69.9481)  Time: 1.305s,   24.52/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32050/40036 ( 80%)]  Loss: 70.214996 (70.4694)  Loss_l1:  0.206098 (0.1043)  Loss_traj: 69.184509 (69.9480)  Time: 1.229s,   26.03/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 0 [32100/40036 ( 80%)]  Loss: 70.154449 (70.4695)  Loss_l1:  0.034827 (0.1043)  Loss_traj: 69.980316 (69.9481)  Time: 1.351s,   23.69/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32150/40036 ( 80%)]  Loss: 69.139061 (70.4691)  Loss_l1:  0.018276 (0.1043)  Loss_traj: 69.047684 (69.9476)  Time: 1.253s,   25.54/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32200/40036 ( 80%)]  Loss: 71.471039 (70.4687)  Loss_l1:  0.137419 (0.1043)  Loss_traj: 70.783943 (69.9471)  Time: 1.312s,   24.40/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32250/40036 ( 81%)]  Loss: 70.820259 (70.4687)  Loss_l1:  0.243512 (0.1043)  Loss_traj: 69.602699 (69.9471)  Time: 1.355s,   23.62/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 0 [32300/40036 ( 81%)]  Loss: 70.597809 (70.4685)  Loss_l1:  0.082584 (0.1043)  Loss_traj: 70.184891 (69.9468)  Time: 1.231s,   25.98/s  (1.286s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32350/40036 ( 81%)]  Loss: 69.330154 (70.4681)  Loss_l1:  0.001276 (0.1043)  Loss_traj: 69.323769 (69.9465)  Time: 1.371s,   23.35/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32400/40036 ( 81%)]  Loss: 70.785034 (70.4679)  Loss_l1:  0.067784 (0.1043)  Loss_traj: 70.446114 (69.9464)  Time: 1.232s,   25.97/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32450/40036 ( 81%)]  Loss: 69.925629 (70.4678)  Loss_l1:  0.169804 (0.1043)  Loss_traj: 69.076607 (69.9464)  Time: 1.322s,   24.21/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32500/40036 ( 81%)]  Loss: 69.697601 (70.4674)  Loss_l1:  0.047241 (0.1043)  Loss_traj: 69.461395 (69.9459)  Time: 1.235s,   25.91/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32550/40036 ( 81%)]  Loss: 69.181404 (70.4674)  Loss_l1:  0.129369 (0.1043)  Loss_traj: 68.534561 (69.9460)  Time: 1.372s,   23.32/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.019 (0.014)\n",
      "INFO:root:Train: 0 [32600/40036 ( 81%)]  Loss: 68.498062 (70.4673)  Loss_l1:  0.278767 (0.1043)  Loss_traj: 67.104225 (69.9459)  Time: 1.237s,   25.86/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32650/40036 ( 82%)]  Loss: 69.313828 (70.4672)  Loss_l1:  0.060860 (0.1043)  Loss_traj: 69.009529 (69.9458)  Time: 1.225s,   26.12/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32700/40036 ( 82%)]  Loss: 73.001549 (70.4673)  Loss_l1:  0.138926 (0.1043)  Loss_traj: 72.306915 (69.9459)  Time: 1.300s,   24.61/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32750/40036 ( 82%)]  Loss: 72.420753 (70.4672)  Loss_l1:  0.307204 (0.1043)  Loss_traj: 70.884735 (69.9457)  Time: 1.225s,   26.13/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32800/40036 ( 82%)]  Loss: 71.053932 (70.4672)  Loss_l1:  0.053349 (0.1043)  Loss_traj: 70.787186 (69.9457)  Time: 1.333s,   24.01/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n",
      "INFO:root:Train: 0 [32850/40036 ( 82%)]  Loss: 68.923691 (70.4678)  Loss_l1:  0.083705 (0.1043)  Loss_traj: 68.505165 (69.9463)  Time: 1.230s,   26.01/s  (1.287s,   24.87/s)  LR: 1.000e-04  Data: 0.015 (0.014)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4d916440d386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_l1_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         use_amp=use_amp, model_ema=model_ema)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-0838e4a6b2fb>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, model_ns, model_raw, loader, optimizer, loss_fn, loss_traj_fn, args, lr_scheduler, saver, output_dir, use_amp, model_ema)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    if args.distributed:\n",
    "        loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "    train_metrics = train_epoch(\n",
    "        epoch, model, model_ns, model_raw, train_loader, optimizer, train_loss_fn, loss_l1_fn, args,\n",
    "        lr_scheduler=lr_scheduler, saver=saver, output_dir=output_dir,\n",
    "        use_amp=use_amp, model_ema=model_ema)\n",
    "\n",
    "    if (epoch+1)%5 == 0:\n",
    "        eval_metrics = val_epoch(model_raw, model, val_loader, validate_loss_fn, args)\n",
    "\n",
    "    if model_ema is not None and not args.model_ema_force_cpu:\n",
    "        if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "            distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "        ema_eval_metrics = validate(\n",
    "            model_ema.ema, loader_eval, validate_loss_fn, args, log_suffix=' (EMA)')\n",
    "        eval_metrics = ema_eval_metrics\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        # step LR for next epoch\n",
    "        lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    if (epoch+1)%10 == 0:\n",
    "        update_summary(\n",
    "            epoch, train_metrics, eval_metrics, os.path.join(output_dir, 'summary.csv'),\n",
    "            write_header=best_metric is None)\n",
    "\n",
    "    if saver is not None:\n",
    "    # save proper checkpoint with eval metric\n",
    "        save_metric = eval_metrics[eval_metric]\n",
    "        best_metric, best_epoch = saver.save_checkpoint(\n",
    "            model, optimizer, args,\n",
    "            epoch=epoch, model_ema=model_ema, metric=save_metric, use_amp=use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0056,  0.0056,  0.0056,  ...,  0.4851,  0.4337,  0.4508],\n",
      "          [ 0.0056,  0.0056,  0.0056,  ...,  0.4851,  0.4337,  0.4337],\n",
      "          [ 0.0056,  0.0056,  0.0056,  ...,  0.4679,  0.4679,  0.4679],\n",
      "          ...,\n",
      "          [ 1.4783,  1.4783,  1.4783,  ...,  2.0263,  2.0434,  2.0434],\n",
      "          [ 1.4440,  1.4440,  1.4440,  ...,  2.0092,  2.0092,  2.0092],\n",
      "          [ 1.4440,  1.4440,  1.4440,  ...,  1.9578,  1.9578,  1.9578]],\n",
      "\n",
      "         [[ 1.5182,  1.5182,  1.5182,  ...,  1.7458,  1.7458,  1.7108],\n",
      "          [ 1.5182,  1.5182,  1.5182,  ...,  1.7458,  1.7458,  1.7458],\n",
      "          [ 1.5182,  1.5182,  1.5182,  ...,  1.7633,  1.7633,  1.7633],\n",
      "          ...,\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          ...,\n",
      "          [-1.7173, -1.7173, -1.7173,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.7522, -1.7522, -1.7522,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.7522, -1.7522, -1.7522,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-0.9192, -0.9534, -0.9705,  ..., -0.8678, -0.8678, -0.8849],\n",
      "          [-0.9192, -0.9363, -0.9534,  ..., -0.8507, -0.8849, -0.9020],\n",
      "          [-0.9192, -0.9192, -0.9192,  ..., -0.8678, -0.8849, -0.9020],\n",
      "          ...,\n",
      "          [-0.4397, -0.4397, -0.4226,  ..., -0.5253, -0.5253, -0.5253],\n",
      "          [-0.4911, -0.4739, -0.4397,  ..., -0.5253, -0.5253, -0.5424],\n",
      "          [-0.5253, -0.5082, -0.4739,  ..., -0.5082, -0.5253, -0.5253]],\n",
      "\n",
      "         [[-0.8803, -0.8978, -0.9153,  ..., -0.7577, -0.7577, -0.7752],\n",
      "          [-0.8978, -0.8978, -0.9153,  ..., -0.7402, -0.7752, -0.7927],\n",
      "          [-0.9153, -0.9153, -0.9153,  ..., -0.7577, -0.7752, -0.7927],\n",
      "          ...,\n",
      "          [-0.3725, -0.3901, -0.4076,  ..., -0.4076, -0.4076, -0.4076],\n",
      "          [-0.4076, -0.4076, -0.4076,  ..., -0.4076, -0.4076, -0.4251],\n",
      "          [-0.4251, -0.4076, -0.4076,  ..., -0.3901, -0.4076, -0.4076]],\n",
      "\n",
      "         [[-0.7413, -0.7761, -0.7936,  ..., -0.5844, -0.5844, -0.5844],\n",
      "          [-0.7587, -0.7761, -0.7936,  ..., -0.5670, -0.5844, -0.6193],\n",
      "          [-0.7761, -0.7761, -0.7761,  ..., -0.5495, -0.6018, -0.6367],\n",
      "          ...,\n",
      "          [-0.2184, -0.2184, -0.2532,  ..., -0.2184, -0.2358, -0.2532],\n",
      "          [-0.2532, -0.2358, -0.2358,  ..., -0.2010, -0.2358, -0.2707],\n",
      "          [-0.2881, -0.2707, -0.2532,  ..., -0.1835, -0.2184, -0.2532]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3070,  1.3413,  1.4098,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 1.1872,  1.2728,  1.4612,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 0.9474,  1.1015,  1.4783,  ...,  2.2489,  2.2489,  2.1462],\n",
      "          ...,\n",
      "          [-0.0116, -0.0287, -0.0801,  ...,  1.3413,  1.4098,  1.3927],\n",
      "          [ 0.1426,  0.1083, -0.0116,  ...,  1.2385,  1.3413,  1.3927],\n",
      "          [ 0.2282,  0.1597,  0.0227,  ...,  1.1872,  1.3070,  1.3927]],\n",
      "\n",
      "         [[ 1.3256,  1.2556,  1.1506,  ...,  2.0434,  1.9209,  1.8333],\n",
      "          [ 1.3081,  1.2556,  1.3081,  ...,  1.9559,  1.9209,  1.8683],\n",
      "          [ 1.2031,  1.2556,  1.4657,  ...,  1.7633,  1.8333,  1.9384],\n",
      "          ...,\n",
      "          [ 0.4153,  0.4678,  0.5903,  ...,  1.3081,  1.2206,  1.2556],\n",
      "          [ 0.5728,  0.5728,  0.6429,  ...,  1.2206,  1.2556,  1.3081],\n",
      "          [ 0.6078,  0.6078,  0.6604,  ...,  1.2031,  1.2556,  1.3081]],\n",
      "\n",
      "         [[ 0.5659,  0.4614,  0.2522,  ...,  0.3219,  0.2522,  0.1476],\n",
      "          [ 0.7054,  0.6356,  0.3916,  ...,  0.5659,  0.4962,  0.4265],\n",
      "          [ 1.0539,  0.9494,  0.6879,  ...,  0.9842,  0.9145,  0.8622],\n",
      "          ...,\n",
      "          [-0.0964,  0.0605,  0.3393,  ...,  0.7925,  0.6182,  0.5136],\n",
      "          [ 0.1302,  0.1476,  0.3219,  ...,  0.6182,  0.5659,  0.5136],\n",
      "          [ 0.2522,  0.2173,  0.2696,  ...,  0.4614,  0.5659,  0.5659]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0801, -0.0801, -0.0972,  ...,  2.2318,  2.2318,  2.2318],\n",
      "          [-0.0629, -0.0629, -0.0972,  ...,  2.2147,  2.2147,  2.2318],\n",
      "          [ 0.0398,  0.0227, -0.0972,  ...,  2.2147,  2.2147,  2.2147],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.0837,  ...,  0.7248,  0.7419,  0.7762],\n",
      "          [-2.1179, -2.1008, -2.0665,  ...,  0.7077,  0.7248,  0.7419],\n",
      "          [-2.1179, -2.1008, -2.0665,  ...,  0.7077,  0.7248,  0.7419]],\n",
      "\n",
      "         [[-0.2500, -0.1975, -0.1450,  ...,  2.4111,  2.4111,  2.4111],\n",
      "          [-0.1975, -0.1800, -0.1450,  ...,  2.3936,  2.3936,  2.4111],\n",
      "          [-0.0749, -0.0749, -0.1450,  ...,  2.3936,  2.3936,  2.3936],\n",
      "          ...,\n",
      "          [-2.0182, -2.0182, -2.0007,  ...,  0.5728,  0.5903,  0.6078],\n",
      "          [-2.0182, -2.0007, -1.9657,  ...,  0.5378,  0.5728,  0.5903],\n",
      "          [-2.0182, -2.0007, -1.9657,  ...,  0.5378,  0.5728,  0.5903]],\n",
      "\n",
      "         [[ 0.0431,  0.0431,  0.0431,  ...,  1.6291,  1.6465,  1.6640],\n",
      "          [ 0.0605,  0.0605,  0.0431,  ...,  1.6117,  1.6117,  1.6291],\n",
      "          [ 0.1651,  0.1476,  0.0431,  ...,  1.5245,  1.5245,  1.5245],\n",
      "          ...,\n",
      "          [-1.8044, -1.7870, -1.7696,  ...,  0.2871,  0.3219,  0.3393],\n",
      "          [-1.7870, -1.7696, -1.7522,  ...,  0.3219,  0.3393,  0.3568],\n",
      "          [-1.7870, -1.7696, -1.7522,  ...,  0.3219,  0.3393,  0.3568]]],\n",
      "\n",
      "\n",
      "        [[[-0.4568, -0.4054, -0.3369,  ...,  0.3309,  0.3138,  0.3138],\n",
      "          [-0.5253, -0.4739, -0.4054,  ...,  0.3309,  0.3309,  0.3309],\n",
      "          [-0.5938, -0.5938, -0.5596,  ...,  0.3481,  0.3481,  0.3481],\n",
      "          ...,\n",
      "          [-0.6281, -0.6281, -0.6109,  ..., -0.0972, -0.0972, -0.0972],\n",
      "          [-0.6794, -0.6794, -0.6623,  ..., -0.1143, -0.0629, -0.0629],\n",
      "          [-0.7479, -0.7479, -0.6794,  ..., -0.1143, -0.0629, -0.0629]],\n",
      "\n",
      "         [[-0.4251, -0.3901, -0.3375,  ...,  0.2927,  0.2927,  0.2927],\n",
      "          [-0.4601, -0.4426, -0.4076,  ...,  0.2927,  0.2927,  0.2927],\n",
      "          [-0.5476, -0.5301, -0.5301,  ...,  0.3102,  0.3102,  0.3102],\n",
      "          ...,\n",
      "          [-0.8627, -0.8627, -0.8452,  ..., -0.1975, -0.1975, -0.1975],\n",
      "          [-0.8803, -0.8803, -0.8978,  ..., -0.1800, -0.1625, -0.1625],\n",
      "          [-0.8978, -0.8978, -0.9328,  ..., -0.1800, -0.1625, -0.1450]],\n",
      "\n",
      "         [[-0.8807, -0.8458, -0.8110,  ..., -0.6890, -0.7064, -0.7064],\n",
      "          [-0.8981, -0.8807, -0.8807,  ..., -0.6890, -0.6890, -0.6890],\n",
      "          [-0.9678, -0.9678, -0.9504,  ..., -0.6715, -0.6715, -0.6715],\n",
      "          ...,\n",
      "          [-1.0376, -1.0376, -1.0550,  ..., -0.7064, -0.7587, -0.7587],\n",
      "          [-1.0201, -1.0201, -1.0550,  ..., -0.7587, -0.7587, -0.7587],\n",
      "          [-1.0201, -1.0201, -1.0550,  ..., -0.7587, -0.7587, -0.7587]]],\n",
      "\n",
      "\n",
      "        [[[-1.8097, -1.8782, -1.9809,  ..., -1.0904, -0.7822, -0.5767],\n",
      "          [-1.8610, -1.8782, -1.9638,  ..., -1.0904, -0.8678, -0.7137],\n",
      "          [-1.9124, -1.8782, -1.8610,  ..., -1.1932, -1.0733, -0.9705],\n",
      "          ...,\n",
      "          [-1.7412, -1.7412, -1.7069,  ..., -1.5185, -1.5014, -1.4329],\n",
      "          [-1.7240, -1.7069, -1.7240,  ..., -1.6555, -1.5185, -1.4500],\n",
      "          [-1.6898, -1.7069, -1.7240,  ..., -1.6727, -1.5528, -1.4500]],\n",
      "\n",
      "         [[-1.4230, -1.4055, -1.3354,  ..., -0.3550, -0.3200, -0.2850],\n",
      "          [-1.4055, -1.4055, -1.3704,  ..., -0.4076, -0.3200, -0.3200],\n",
      "          [-1.3704, -1.3704, -1.4055,  ..., -0.5301, -0.4251, -0.3550],\n",
      "          ...,\n",
      "          [-1.2654, -1.2829, -1.3354,  ..., -0.6527, -0.6877, -0.7052],\n",
      "          [-1.2654, -1.3354, -1.3529,  ..., -0.6001, -0.7052, -0.8277],\n",
      "          [-1.2829, -1.3354, -1.3704,  ..., -0.5826, -0.7402, -0.8452]],\n",
      "\n",
      "         [[-0.4101, -0.3927, -0.3055,  ...,  0.6531,  1.0017,  1.2805],\n",
      "          [-0.4101, -0.3927, -0.3055,  ...,  0.6008,  0.9319,  1.2108],\n",
      "          [-0.3927, -0.3927, -0.3578,  ...,  0.4614,  0.7925,  0.9842],\n",
      "          ...,\n",
      "          [-0.7761, -0.8458, -0.8458,  ...,  0.1302,  0.2871,  0.3568],\n",
      "          [-0.7587, -0.7761, -0.8633,  ...,  0.2522,  0.2696,  0.3045],\n",
      "          [-0.7238, -0.7761, -0.8807,  ...,  0.2696,  0.2871,  0.2871]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "    print(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.0263,  2.0263,  2.0263,  ...,  1.9064,  1.8379,  1.7865],\n",
      "          [ 2.0263,  2.0263,  2.0263,  ...,  1.7523,  1.7180,  1.7009],\n",
      "          [ 2.0263,  2.0263,  2.0263,  ...,  1.4098,  1.5125,  1.5639],\n",
      "          ...,\n",
      "          [-0.1486, -0.1486, -0.1314,  ..., -0.4226, -0.4568, -0.4739],\n",
      "          [-0.1657, -0.1657, -0.1657,  ..., -0.3712, -0.4397, -0.4739],\n",
      "          [-0.1657, -0.1657, -0.1999,  ..., -0.3541, -0.4397, -0.4739]],\n",
      "\n",
      "         [[ 2.2010,  2.2010,  2.2010,  ...,  0.0476,  0.1001,  0.1176],\n",
      "          [ 2.2010,  2.2010,  2.2010,  ...,  0.0126,  0.0826,  0.1001],\n",
      "          [ 2.2010,  2.2010,  2.2010,  ..., -0.1099, -0.0049,  0.0126],\n",
      "          ...,\n",
      "          [ 0.0126,  0.0126,  0.0301,  ..., -0.3025, -0.3025, -0.2675],\n",
      "          [ 0.0126,  0.0126,  0.0126,  ..., -0.2675, -0.3025, -0.3025],\n",
      "          [ 0.0126,  0.0126, -0.0049,  ..., -0.2675, -0.3025, -0.3200]],\n",
      "\n",
      "         [[ 2.4134,  2.4134,  2.4134,  ...,  1.4374,  1.3851,  1.3851],\n",
      "          [ 2.4134,  2.4134,  2.4134,  ...,  1.3328,  1.3328,  1.3328],\n",
      "          [ 2.4134,  2.4134,  2.4134,  ...,  1.1934,  1.2805,  1.2980],\n",
      "          ...,\n",
      "          [ 0.7402,  0.7402,  0.7402,  ...,  0.1302,  0.1302,  0.1302],\n",
      "          [ 0.7054,  0.7054,  0.7054,  ...,  0.1302,  0.1302,  0.1302],\n",
      "          [ 0.7054,  0.7054,  0.6879,  ...,  0.1302,  0.1302,  0.1302]]],\n",
      "\n",
      "\n",
      "        [[[-1.4843, -1.4843, -1.5014,  ..., -1.1932, -1.1932, -1.1932],\n",
      "          [-1.4843, -1.4843, -1.5014,  ..., -1.1932, -1.1760, -1.1760],\n",
      "          [-1.5014, -1.5014, -1.5014,  ..., -1.1760, -1.1589, -1.1589],\n",
      "          ...,\n",
      "          [-1.0048, -1.0048, -0.9877,  ..., -1.1075, -1.1418, -1.1418],\n",
      "          [-1.0048, -1.0048, -1.0048,  ..., -1.1418, -1.1418, -1.1418],\n",
      "          [-1.0048, -1.0048, -1.0048,  ..., -1.1418, -1.1418, -1.1418]],\n",
      "\n",
      "         [[-1.2654, -1.2654, -1.2829,  ..., -1.0028, -1.0028, -1.0028],\n",
      "          [-1.2654, -1.2654, -1.2829,  ..., -1.0028, -0.9853, -0.9853],\n",
      "          [-1.2829, -1.2829, -1.2829,  ..., -0.9853, -0.9503, -0.9503],\n",
      "          ...,\n",
      "          [-1.2129, -1.2129, -1.1954,  ..., -1.2129, -1.2304, -1.2304],\n",
      "          [-1.2129, -1.2129, -1.2129,  ..., -1.2304, -1.2304, -1.2304],\n",
      "          [-1.2129, -1.2129, -1.2129,  ..., -1.2304, -1.2304, -1.2304]],\n",
      "\n",
      "         [[-1.3861, -1.3861, -1.4036,  ..., -0.9678, -0.9678, -0.9678],\n",
      "          [-1.3861, -1.3861, -1.4036,  ..., -0.9678, -0.9504, -0.9504],\n",
      "          [-1.4036, -1.4036, -1.4036,  ..., -0.9330, -0.9156, -0.9156],\n",
      "          ...,\n",
      "          [-1.2467, -1.2467, -1.2293,  ..., -1.2467, -1.2641, -1.2641],\n",
      "          [-1.2293, -1.2293, -1.2467,  ..., -1.2641, -1.2641, -1.2641],\n",
      "          [-1.2293, -1.2293, -1.2467,  ..., -1.2641, -1.2641, -1.2641]]],\n",
      "\n",
      "\n",
      "        [[[-1.5014, -1.5014, -1.5014,  ..., -1.5014, -1.5014, -1.5014],\n",
      "          [-1.5014, -1.5014, -1.5014,  ..., -1.5014, -1.5014, -1.5014],\n",
      "          [-1.4843, -1.4843, -1.4672,  ..., -1.5014, -1.5014, -1.5014],\n",
      "          ...,\n",
      "          [-1.1760, -1.1760, -1.1760,  ..., -0.7308, -0.7308, -0.7308],\n",
      "          [-1.1760, -1.1760, -1.1760,  ..., -0.7993, -0.7822, -0.7650],\n",
      "          [-1.1760, -1.1760, -1.1760,  ..., -0.8335, -0.7993, -0.7993]],\n",
      "\n",
      "         [[-0.4601, -0.4601, -0.4426,  ..., -0.4076, -0.4076, -0.4076],\n",
      "          [-0.4426, -0.4426, -0.4251,  ..., -0.4076, -0.4076, -0.4076],\n",
      "          [-0.4251, -0.4251, -0.4251,  ..., -0.4076, -0.4076, -0.4076],\n",
      "          ...,\n",
      "          [-0.8627, -0.8627, -0.8627,  ..., -0.6702, -0.6702, -0.6702],\n",
      "          [-0.8627, -0.8627, -0.8627,  ..., -0.7052, -0.6877, -0.6877],\n",
      "          [-0.8627, -0.8627, -0.8627,  ..., -0.7227, -0.7227, -0.7052]],\n",
      "\n",
      "         [[ 0.3219,  0.3219,  0.3219,  ...,  0.3742,  0.3742,  0.3742],\n",
      "          [ 0.3219,  0.3219,  0.3393,  ...,  0.3742,  0.3742,  0.3742],\n",
      "          [ 0.3219,  0.3219,  0.3393,  ...,  0.3742,  0.3742,  0.3742],\n",
      "          ...,\n",
      "          [-0.5670, -0.5670, -0.5670,  ..., -0.6018, -0.5844, -0.5670],\n",
      "          [-0.5670, -0.5670, -0.5670,  ..., -0.6193, -0.5844, -0.5844],\n",
      "          [-0.5670, -0.5670, -0.5670,  ..., -0.6367, -0.6018, -0.5844]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.6042, -1.6213, -1.6555,  ..., -1.4843, -1.4843, -1.4843],\n",
      "          [-1.6042, -1.6213, -1.6384,  ..., -1.4672, -1.4843, -1.4843],\n",
      "          [-1.6213, -1.6213, -1.6384,  ..., -1.4500, -1.4843, -1.4843],\n",
      "          ...,\n",
      "          [-1.3302, -1.3302, -1.3130,  ..., -1.4672, -1.5870, -1.6727],\n",
      "          [-1.2445, -1.2445, -1.2617,  ..., -1.6384, -1.6213, -1.6213],\n",
      "          [-1.1932, -1.2103, -1.2274,  ..., -1.6384, -1.4843, -1.4329]],\n",
      "\n",
      "         [[-1.4930, -1.4930, -1.5105,  ..., -1.3004, -1.3179, -1.3179],\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -1.3004, -1.3179, -1.3179],\n",
      "          [-1.4930, -1.4930, -1.4755,  ..., -1.3004, -1.3179, -1.3354],\n",
      "          ...,\n",
      "          [-1.3004, -1.3004, -1.2829,  ..., -1.3880, -1.5455, -1.6155],\n",
      "          [-1.2829, -1.2829, -1.2654,  ..., -1.6155, -1.6155, -1.6155],\n",
      "          [-1.2829, -1.2829, -1.2479,  ..., -1.6155, -1.4930, -1.4405]],\n",
      "\n",
      "         [[-1.5430, -1.5604, -1.5779,  ..., -1.5604, -1.5430, -1.5430],\n",
      "          [-1.5604, -1.5604, -1.5779,  ..., -1.5430, -1.5430, -1.5604],\n",
      "          [-1.5779, -1.5779, -1.5604,  ..., -1.5256, -1.5604, -1.5779],\n",
      "          ...,\n",
      "          [-1.4559, -1.4559, -1.4733,  ..., -1.4036, -1.4733, -1.5256],\n",
      "          [-1.3861, -1.3861, -1.3861,  ..., -1.5256, -1.5430, -1.5779],\n",
      "          [-1.3513, -1.3513, -1.3513,  ..., -1.5779, -1.5430, -1.5256]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5193,  0.6392,  0.8447,  ...,  0.5193,  0.4679,  0.4508],\n",
      "          [ 0.5536,  0.6392,  0.7933,  ...,  0.6221,  0.4679,  0.3823],\n",
      "          [ 0.6734,  0.6734,  0.7077,  ...,  0.9132,  0.4679,  0.2453],\n",
      "          ...,\n",
      "          [-0.3883,  0.1254,  1.0331,  ..., -1.2617, -1.3987, -1.4672],\n",
      "          [ 0.3823,  1.0844,  2.2489,  ..., -1.4843, -1.1932, -1.0048],\n",
      "          [ 0.6392,  1.4098,  2.2489,  ..., -1.5870, -1.1075, -0.8335]],\n",
      "\n",
      "         [[-0.1975, -0.0399,  0.1702,  ..., -0.2850, -0.3025, -0.3200],\n",
      "          [-0.1275, -0.0399,  0.1001,  ..., -0.1450, -0.3025, -0.3901],\n",
      "          [-0.0049, -0.0049, -0.0399,  ...,  0.1176, -0.3025, -0.5301],\n",
      "          ...,\n",
      "          [-0.7752, -0.4776,  0.1702,  ..., -1.7031, -1.8782, -1.8957],\n",
      "          [-0.3025,  0.3627,  1.6758,  ..., -1.9482, -1.6681, -1.4755],\n",
      "          [-0.1975,  0.6604,  2.2535,  ..., -2.0357, -1.5805, -1.3004]],\n",
      "\n",
      "         [[-0.6367, -0.4798, -0.2707,  ..., -0.9678, -0.9853, -1.0376],\n",
      "          [-0.5670, -0.4798, -0.3404,  ..., -0.8633, -0.9853, -1.0898],\n",
      "          [-0.4450, -0.4450, -0.4450,  ..., -0.5670, -0.9853, -1.2293],\n",
      "          ...,\n",
      "          [-0.8807, -0.7064, -0.3927,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-0.6367, -0.0441,  1.0714,  ..., -1.8044, -1.8044, -1.6650],\n",
      "          [-0.5844,  0.1999,  1.6291,  ..., -1.8044, -1.8044, -1.5256]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3823,  0.3823,  0.3652,  ...,  0.4508,  0.4508,  0.4508],\n",
      "          [ 0.3823,  0.3823,  0.3652,  ...,  0.4508,  0.4508,  0.4508],\n",
      "          [ 0.3823,  0.3823,  0.3652,  ...,  0.2967,  0.2624,  0.2624],\n",
      "          ...,\n",
      "          [-0.3027, -0.3027, -0.3027,  ...,  0.0056,  0.0056,  0.0056],\n",
      "          [-0.3027, -0.3027, -0.3198,  ...,  0.0227,  0.0056,  0.0056],\n",
      "          [-0.3027, -0.3027, -0.3198,  ...,  0.0227,  0.0056,  0.0056]],\n",
      "\n",
      "         [[ 0.5903,  0.5903,  0.5728,  ...,  0.5903,  0.5903,  0.5903],\n",
      "          [ 0.5903,  0.5903,  0.5728,  ...,  0.5903,  0.5903,  0.5903],\n",
      "          [ 0.5728,  0.5728,  0.5553,  ...,  0.4328,  0.3978,  0.3978],\n",
      "          ...,\n",
      "          [-0.0749, -0.0749, -0.1099,  ..., -0.0049, -0.0049, -0.0049],\n",
      "          [-0.0924, -0.0924, -0.1275,  ..., -0.0049, -0.0049, -0.0049],\n",
      "          [-0.0924, -0.0924, -0.1275,  ..., -0.0049, -0.0049, -0.0049]],\n",
      "\n",
      "         [[ 0.7402,  0.7402,  0.7402,  ...,  0.7751,  0.7402,  0.7402],\n",
      "          [ 0.7402,  0.7402,  0.7402,  ...,  0.7751,  0.7402,  0.7402],\n",
      "          [ 0.7402,  0.7402,  0.7402,  ...,  0.5485,  0.4962,  0.4962],\n",
      "          ...,\n",
      "          [-0.5495, -0.5495, -0.5321,  ..., -0.2532, -0.2532, -0.2532],\n",
      "          [-0.5495, -0.5495, -0.5321,  ..., -0.2532, -0.2532, -0.2532],\n",
      "          [-0.5495, -0.5495, -0.5321,  ..., -0.2532, -0.2532, -0.2532]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "    print(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/thesis/timm/models/efficientnet.py\", line 403, in forward\n    x, forward_list = self.forward_features(x)\n  File \"/home/cutz/thesis/timm/models/efficientnet.py\", line 394, in forward_features\n    x = block(x)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n    input = module(input)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/thesis/timm/models/efficientnet_blocks.py\", line 267, in forward\n    x = self.act2(x)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/thesis/timm/models/layers/activations_me.py\", line 54, in forward\n    return SwishJitAutoFn.apply(x)\n  File \"/home/cutz/thesis/timm/models/layers/activations_me.py\", line 37, in forward\n    return swish_jit_fwd(x)\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/cutz/thesis/timm/models/layers/activations_me.py\", line 19, in swish_jit_fwd\n@torch.jit.script\ndef swish_jit_fwd(x):\n    return x.mul(torch.sigmoid(x))\n           ~~~~~ <--- HERE\nRuntimeError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 31.72 GiB total capacity; 30.20 GiB already allocated; 15.56 MiB free; 30.72 GiB reserved in total by PyTorch)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1d2a88c92e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/thesis/timm/models/efficientnet.py\", line 403, in forward\n    x, forward_list = self.forward_features(x)\n  File \"/home/cutz/thesis/timm/models/efficientnet.py\", line 394, in forward_features\n    x = block(x)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n    input = module(input)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/thesis/timm/models/efficientnet_blocks.py\", line 267, in forward\n    x = self.act2(x)\n  File \"/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/cutz/thesis/timm/models/layers/activations_me.py\", line 54, in forward\n    return SwishJitAutoFn.apply(x)\n  File \"/home/cutz/thesis/timm/models/layers/activations_me.py\", line 37, in forward\n    return swish_jit_fwd(x)\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/home/cutz/thesis/timm/models/layers/activations_me.py\", line 19, in swish_jit_fwd\n@torch.jit.script\ndef swish_jit_fwd(x):\n    return x.mul(torch.sigmoid(x))\n           ~~~~~ <--- HERE\nRuntimeError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 31.72 GiB total capacity; 30.20 GiB already allocated; 15.56 MiB free; 30.72 GiB reserved in total by PyTorch)\n\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
