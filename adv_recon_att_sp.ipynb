{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import glob\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from timm.data import Dataset, create_loader, resolve_data_config,  FastCollateMixup, mixup_batch, AugMixDataset\n",
    "from timm.models import create_model, resume_checkpoint, convert_splitbn_model, apply_test_time_pool\n",
    "from timm.utils import *\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from timm.optim import create_optimizer\n",
    "from timm.scheduler import create_scheduler\n",
    "from munch import Munch\n",
    "import yaml\n",
    "import sys\n",
    "from resnet_generator import Generator, Critic\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from transformer import Attention\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "from crd.criterion import CRDLoss\n",
    "from dataset.imagenet import get_dataloader_sample\n",
    "from timm.data import transforms_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from apex import amp\n",
    "# from apex.parallel import DistributedDataParallel as DDP\n",
    "# from apex.parallel import convert_syncbn_model\n",
    "has_apex = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device  True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = '0,1,2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device \" , use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training with a single process on 3 GPUs.\n"
     ]
    }
   ],
   "source": [
    "with open('config/train.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = Munch(config)\n",
    "args.prefetcher = not args.no_prefetcher\n",
    "args.distributed = False\n",
    "args.device = 'cuda'\n",
    "args.world_size = 3\n",
    "args.rank = 0\n",
    "logging.info('Training with a single process on %d GPUs.' % args.num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    args.num_gpu = 1\n",
    "    args.device = 'cuda:%d' % args.local_rank\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://', rank=args.rank, world_size=args.world_size)\n",
    "    args.world_size = torch.distributed.get_world_size()\n",
    "    args.rank = torch.distributed.get_rank()\n",
    "    assert args.rank >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa2f0efae90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(args.seed + args.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ns = timm.create_model('tf_efficientnet_b1_ns', pretrained=True)\n",
    "model_ns = model_ns.cuda()\n",
    "model_ns = torch.nn.DataParallel(model_ns)\n",
    "model_ns = model_ns.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_name = 'module.classifier.'\n",
    "# teacher_model_weights = {}\n",
    "# for name, param in model_ns.named_parameters():\n",
    "#     teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw = timm.create_model('tf_efficientnet_b1', pretrained=True)\n",
    "model_raw = model_raw.cuda()\n",
    "model_raw = torch.nn.DataParallel(model_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g = Generator(args, img_size=args.img_size, max_conv_dim=256)\n",
    "model_g = model_g.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_critic = Critic(args)\n",
    "model_critic = model_critic.cuda()\n",
    "model_critic = torch.nn.DataParallel(model_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:NVIDIA APEX not installed. AMP off.\n"
     ]
    }
   ],
   "source": [
    "use_amp = False\n",
    "if has_apex and args.amp:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    model_raw =  amp.initialize(model_raw)\n",
    "    model_ns = amp.initialize(model_ns)\n",
    "    use_amp = True\n",
    "if args.local_rank == 0:\n",
    "    logging.info('NVIDIA APEX {}. AMP {}.'.format(\n",
    "        'installed' if has_apex else 'not installed', 'on' if use_amp else 'off'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally resume from a checkpoint\n",
    "resume_state = {}\n",
    "resume_epoch = None\n",
    "if args.resume:\n",
    "    resume_state, resume_epoch = resume_checkpoint(model_g, args.resume)\n",
    "if resume_state and not args.no_resume_opt:\n",
    "    if 'optimizer' in resume_state:\n",
    "        if args.local_rank == 0:\n",
    "            logging.info('Restoring Optimizer state from checkpoint')\n",
    "        optimizer.load_state_dict(resume_state['optimizer'])\n",
    "    if use_amp and 'amp' in resume_state and 'load_state_dict' in amp.__dict__:\n",
    "        if args.local_rank == 0:\n",
    "            logging.info('Restoring NVIDIA AMP state from checkpoint')\n",
    "        amp.load_state_dict(resume_state['amp'])\n",
    "del resume_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g = torch.nn.DataParallel(model_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data processing configuration for current model + dataset:\n",
      "INFO:root:\tinput_size: (3, 240, 240)\n",
      "INFO:root:\tinterpolation: bicubic\n",
      "INFO:root:\tmean: (0.485, 0.456, 0.406)\n",
      "INFO:root:\tstd: (0.229, 0.224, 0.225)\n",
      "INFO:root:\tcrop_pct: 0.882\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/home/data/imagenet/train'\n",
    "val_dir = '/home/data/imagenet/val'\n",
    "data_config = resolve_data_config(vars(args), model=model_g, verbose=args.local_rank == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aug_splits = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    if args.sync_bn:\n",
    "        assert not args.split_bn\n",
    "        try:\n",
    "            if has_apex:\n",
    "                model = convert_syncbn_model(model)\n",
    "            else:\n",
    "                model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "            if args.local_rank == 0:\n",
    "                logging.info(\n",
    "                    'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                    'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "        except Exception as e:\n",
    "            logging.error('Failed to enable Synchronized BatchNorm. Install Apex or Torch >= 1.1')\n",
    "    if has_apex:\n",
    "        model = DDP(model, delay_allreduce=True)\n",
    "    else:\n",
    "        if args.local_rank == 0:\n",
    "            logging.info(\"Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP.\")\n",
    "        model = DDP(model, device_ids=[args.local_rank])  # can use device str in Torch >= 1.1\n",
    "    # NOTE: EMA model does not need to be wrapped by DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset(train_dir)\n",
    "val_dataset = Dataset(val_dir, load_bytes=False, class_map='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model created, param count: 3662851\n"
     ]
    }
   ],
   "source": [
    "param_count = sum([m.numel() for m in model_g.parameters()])\n",
    "logging.info('Model created, param count: %d' % (param_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collate_fn = None\n",
    "if args.prefetcher and args.mixup > 0:\n",
    "    assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "    collate_fn = FastCollateMixup(args.mixup, args.smoothing, args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw, test_time_pool = apply_test_time_pool(model_raw, data_config, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms_factory.transforms_imagenet_train(img_size=args.img_size, auto_augment=False, color_jitter=args.color_jitter, interpolation=args.interpolation, re_count=args.recount, re_mode=args.remode, re_prob=args.reprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage1 finished!\n",
      "dataset initialized!\n",
      "num_samples 1281167\n",
      "num_class 1000\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader_sample(transform, dataset='imagenet', batch_size=args.batch_size, is_sample=True, k=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pct = 1.0 if test_time_pool else data_config['crop_pct']\n",
    "val_loader = create_loader(\n",
    "    val_dataset,\n",
    "    input_size=data_config['input_size'],\n",
    "    batch_size=args.batch_size,\n",
    "    is_training=False,\n",
    "    use_prefetcher=args.prefetcher,\n",
    "    interpolation=data_config['interpolation'],\n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=args.workers,\n",
    "    crop_pct=crop_pct,\n",
    "    pin_memory=args.pin_mem,\n",
    "    tf_preprocessing=args.tf_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.jsd:\n",
    "    assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "    train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss()\n",
    "elif args.mixup > 0.:\n",
    "    # smoothing is handled with mixup label transform\n",
    "    train_loss_fn = SoftTargetCrossEntropy()\n",
    "    validate_loss_fn = nn.CrossEntropyLoss()\n",
    "elif args.smoothing:\n",
    "    train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    train_loss_fn = nn.CrossEntropyLoss()\n",
    "    validate_loss_fn = train_loss_fn\n",
    "    \n",
    "crd_loss_fn = CRDLoss(args).cuda()\n",
    "critic_loss_fn = nn.BCELoss()\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): DataParallel(\n",
       "    (module): Generator(\n",
       "      (from_rgb): Conv2d(3, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (encode): ModuleList(\n",
       "        (0): ResBlk(\n",
       "          (actv): LeakyReLU(negative_slope=0.2)\n",
       "          (conv1): Conv2d(68, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(68, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): InstanceNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (conv1x1): Conv2d(68, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): ResBlk(\n",
       "          (actv): LeakyReLU(negative_slope=0.2)\n",
       "          (conv1): Conv2d(136, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(136, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): InstanceNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (conv1x1): Conv2d(136, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): ResBlk(\n",
       "          (actv): LeakyReLU(negative_slope=0.2)\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (decode): ModuleList(\n",
       "        (0): AdainResBlk(\n",
       "          (actv): LeakyReLU(negative_slope=0.2)\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (1): AdainResBlk(\n",
       "          (actv): LeakyReLU(negative_slope=0.2)\n",
       "          (conv1): Conv2d(256, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(136, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(136, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (conv1x1): Conv2d(256, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): AdainResBlk(\n",
       "          (actv): LeakyReLU(negative_slope=0.2)\n",
       "          (conv1): Conv2d(136, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(68, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): InstanceNorm2d(136, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(68, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (conv1x1): Conv2d(136, 68, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (to_rgb): Sequential(\n",
       "        (0): InstanceNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "        (2): Conv2d(68, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DataParallel(\n",
       "    (module): EfficientNet(\n",
       "      (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): SwishMe()\n",
       "      (blocks): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): DepthwiseSeparableConv(\n",
       "            (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): Identity()\n",
       "          )\n",
       "          (1): DepthwiseSeparableConv(\n",
       "            (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "            (bn2): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
       "            (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
       "            (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (4): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1920, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SwishMe()\n",
       "            (conv_dw): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "            (bn2): BatchNorm2d(1920, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SwishMe()\n",
       "            (se): SqueezeExcite(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (conv_reduce): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SwishMe()\n",
       "              (conv_expand): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): SwishMe()\n",
       "      (global_pool): SelectAdaptivePool2d (output_size=1, pool_type=avg)\n",
       "      (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (2): Embed(\n",
       "    (linear): Linear(in_features=1280, out_features=128, bias=True)\n",
       "    (l2norm): Normalize()\n",
       "  )\n",
       "  (3): Embed(\n",
       "    (linear): Linear(in_features=1280, out_features=128, bias=True)\n",
       "    (l2norm): Normalize()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_g)\n",
    "trainable_list.append(model_raw)\n",
    "trainable_list.append(crd_loss_fn.embed_s)\n",
    "trainable_list.append(crd_loss_fn.embed_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = create_optimizer(args, trainable_list)\n",
    "lr_scheduler, num_epochs = create_scheduler(args, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_critic = create_optimizer(args, model_critic)\n",
    "lr_scheduler_critic, _ = create_scheduler(args, optimizer_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scheduled epochs: 200\n"
     ]
    }
   ],
   "source": [
    "if args.local_rank == 0:\n",
    "    logging.info('Scheduled epochs: {}'.format(num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_state = {}\n",
    "resume_epoch = None\n",
    "start_epoch = 0\n",
    "if args.start_epoch is not None:\n",
    "    # a specified start_epoch will always override the resume epoch\n",
    "    start_epoch = args.start_epoch\n",
    "elif resume_epoch is not None:\n",
    "    start_epoch = resume_epoch\n",
    "if lr_scheduler is not None and start_epoch > 0:\n",
    "    lr_scheduler.step(start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = args.eval_metric\n",
    "best_metric = None\n",
    "best_epoch = None\n",
    "saver = None\n",
    "output_dir = ''\n",
    "if args.local_rank == 0:\n",
    "    output_base = args.output if args.output else './output'\n",
    "    exp_name = '-'.join([\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        args.model,\n",
    "        str(data_config['input_size'][-1])\n",
    "    ])\n",
    "    output_dir = get_outdir(output_base, 'train', exp_name)\n",
    "    decreasing = True if eval_metric == 'loss' else False\n",
    "    saver = CheckpointSaver(checkpoint_dir=output_dir, decreasing=decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema = None\n",
    "if args.model_ema:\n",
    "    # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n",
    "    model_ema = ModelEma(\n",
    "        model,\n",
    "        decay=args.model_ema_decay,\n",
    "        device='cpu' if args.model_ema_force_cpu else '',\n",
    "        resume=args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model_g, model_raw, model_ns, model_critic, loader, optimizer, optimizer_critic, loss_fn, crd_loss_fn, critic_loss_fn, args,\n",
    "               lr_scheduler=None, lr_scheduler_critic=None, saver=None, output_dir='', use_amp=False, model_ema=None):\n",
    "\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "    losses_crd = AverageMeter()\n",
    "    losses_ce = AverageMeter()\n",
    "    losses_kd = AverageMeter()\n",
    "    losses_critic = AverageMeter()\n",
    "    losses_g = AverageMeter()\n",
    "    \n",
    "    model_g.train()\n",
    "    model_critic.train()\n",
    "    model_raw.train()\n",
    "    model_ns.eval()\n",
    "    \n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    num_updates = epoch * len(loader)\n",
    "        \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        inputs, target, index, contrast_idx = data\n",
    "        inputs, target, index, contrast_idx = inputs.cuda(), target.cuda(), index.cuda(), contrast_idx.cuda()\n",
    "        last_batch = batch_idx == last_idx\n",
    "        data_time_m.update(time.time() - end)\n",
    "        \n",
    "        g_out = model_g(inputs)\n",
    "        \n",
    "        inputs_z = inputs + g_out\n",
    "        output, f_raw = model_raw(inputs_z)\n",
    "        with torch.no_grad():\n",
    "#             inputs_ns = F.interpolate(inputs, size=300, mode='bicubic')\n",
    "            out_ns, f_ns = model_ns(inputs)\n",
    "            out_ns = out_ns.detach()\n",
    "        \n",
    "        # Critic\n",
    "        model_critic.train()\n",
    "        model_critic.zero_grad()\n",
    "        label = torch.full((f_raw.size(0), ), real_label, device='cuda')\n",
    "        critic_out = model_critic(f_ns).view(-1)\n",
    "        loss_critic_ns = critic_loss_fn(critic_out, label)\n",
    "        loss_critic_ns.backward()\n",
    "        loss_critic_ns_out = loss_critic_ns.item()\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        critic_out = model_critic(f_raw.detach()).view(-1)\n",
    "        loss_critic_raw = critic_loss_fn(critic_out, label)\n",
    "        loss_critic_raw.backward()\n",
    "        loss_critic_raw_out = loss_critic_raw.item()\n",
    "        loss_critic = loss_critic_ns_out + loss_critic_raw_out\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        # KD\n",
    "        p_s = F.log_softmax(output/args.T, dim=1)\n",
    "        p_t = F.softmax(out_ns/args.T, dim=1)\n",
    "        loss_kd = F.kl_div(p_s, p_t, size_average=False) * (args.T ** 2) / output.shape[0]\n",
    "        \n",
    "       # CE\n",
    "        loss_ce = loss_fn(output, target)\n",
    "        \n",
    "        # CRD\n",
    "        loss_crd = crd_loss_fn(f_raw, f_ns, index, contrast_idx)\n",
    "        \n",
    "        # G\n",
    "        model_critic.eval()\n",
    "        label.fill_(real_label)\n",
    "        critic_g_out = model_critic(f_raw).view(-1)\n",
    "        loss_g = critic_loss_fn(critic_g_out, label)\n",
    "        \n",
    "        # overall loss\n",
    "        loss = args.lambda_kd * loss_kd + args.lambda_ce * loss_ce + args.lambda_crd * loss_crd + loss_g\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not args.distributed:\n",
    "            losses_kd.update(loss_kd.item(), inputs.size(0))\n",
    "            losses_ce.update(loss_ce.item(), inputs.size(0))\n",
    "            losses_crd.update(loss_crd.item(), inputs.size(0))\n",
    "            losses_critic.update(loss_critic, inputs.size(0))\n",
    "            losses_g.update(loss_g.item(), inputs.size(0))\n",
    "            losses_m.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "        num_updates += 1\n",
    "\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        if last_batch or batch_idx % args.log_interval == 0:\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            lr = sum(lrl) / len(lrl)\n",
    "\n",
    "            if args.distributed:\n",
    "                reduced_loss = reduce_tensor(loss.data, args.world_size)\n",
    "                losses_m.update(reduced_loss.item(), inputs.size(0))\n",
    "\n",
    "            if args.local_rank == 0:\n",
    "                logging.info(\n",
    "                    'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
    "                    'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
    "                    'Loss_kd: {loss_kd.val:>9.6f} ({loss_kd.avg:>6.4f})  '\n",
    "                    'Loss_ce: {loss_ce.val:>9.6f} ({loss_ce.avg:>6.4f})  '\n",
    "                    'Loss_crd: {loss_crd.val:>9.6f} ({loss_crd.avg:>6.4f})  '\n",
    "                    'Loss_g: {loss_g.val:>9.6f} ({loss_g.avg:>6.4f})  '\n",
    "                    'Loss_critic: {loss_critic.val:>9.6f} ({loss_critic.avg:>6.4f})  '\n",
    "                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
    "                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'LR: {lr:.7f}  '\n",
    "                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
    "                        epoch,\n",
    "                        batch_idx, len(loader),\n",
    "                        100. * batch_idx / last_idx,\n",
    "                        loss=losses_m,\n",
    "                        loss_kd=losses_kd,\n",
    "                        loss_ce=losses_ce,\n",
    "                        loss_crd=losses_crd,\n",
    "                        loss_g=losses_g,\n",
    "                        loss_critic=losses_critic,\n",
    "                        batch_time=batch_time_m,\n",
    "                        rate=inputs.size(0) * args.world_size / batch_time_m.val,\n",
    "                        rate_avg=inputs.size(0) * args.world_size / batch_time_m.avg,\n",
    "                        lr=lr,\n",
    "                        data_time=data_time_m))\n",
    "\n",
    "                if args.save_images and output_dir:\n",
    "                    torchvision.utils.save_image(\n",
    "                        inputs_z,\n",
    "                        os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),\n",
    "                        padding=0,\n",
    "                        normalize=True)\n",
    "\n",
    "        if saver is not None and args.recovery_interval and (\n",
    "                last_batch or (batch_idx + 1) % args.recovery_interval == 0):\n",
    "            saver.save_recovery(\n",
    "                model_raw, optimizer, args, epoch, model_ema=model_ema, use_amp=use_amp, batch_idx=batch_idx)\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
    "            lr_scheduler_critic.step_update(num_updates=num_updates, metric=losses_critic.avg)\n",
    "\n",
    "        end = time.time()\n",
    "        # end for\n",
    "\n",
    "    if hasattr(optimizer, 'sync_lookahead'):\n",
    "        optimizer.sync_lookahead()\n",
    "        \n",
    "\n",
    "    return OrderedDict([('loss', losses_m.avg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model_raw, model, val_loader, criterion, args):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model_raw.eval()\n",
    "    model.eval()\n",
    "#     model_att.eval()\n",
    "    with torch.no_grad():\n",
    "        # warmup, reduce variability of first batch time, especially for comparing torchscript vs non\n",
    "        end = time.time()\n",
    "        for i, (inputs, target) in enumerate(val_loader):\n",
    "            if args.no_prefetcher:\n",
    "                target = target.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            out = model(inputs)\n",
    "            # synthesizing input + generator\n",
    "            inputs_out = inputs + out\n",
    "            # compute output\n",
    "            output, _ = model_raw(inputs_out)\n",
    "#             output = model_att(inputs_out, output)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output.data, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(acc1.item(), inputs.size(0))\n",
    "            top5.update(acc5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.log_freq == 0:\n",
    "                logging.info(\n",
    "                    'Test: [{0:>4d}/{1}]  '\n",
    "                    'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '\n",
    "                    'Acc@1: {top1.val:>7.3f} ({top1.avg:>7.3f})  '\n",
    "                    'Acc@5: {top5.val:>7.3f} ({top5.avg:>7.3f})'.format(\n",
    "                        i, len(val_loader), batch_time=batch_time,\n",
    "                        rate_avg=inputs.size(0) / batch_time.avg,\n",
    "                        loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    results = OrderedDict(\n",
    "        top1=round(top1.avg, 4), top1_err=round(100 - top1.avg, 4),\n",
    "        top5=round(top5.avg, 4), top5_err=round(100 - top5.avg, 4),\n",
    "        param_count=round(param_count / 1e6, 2),\n",
    "        img_size=data_config['input_size'][-1],\n",
    "        cropt_pct=crop_pct,\n",
    "        interpolation=data_config['interpolation'])\n",
    "\n",
    "    logging.info(' * Acc@1 {:.3f} ({:.3f}) Acc@5 {:.3f} ({:.3f})'.format(\n",
    "       results['top1'], results['top1_err'], results['top5'], results['top5_err']))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/ATen/native/TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.\n",
      "/home/cutz/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant Z_v1 is set to 2831728.0\n",
      "normalization constant Z_v2 is set to 2842864.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 1 [   0/8374 (  0%)]  Loss: 21.028940 (21.0289)  Loss_kd:  0.532949 (0.5329)  Loss_ce:  3.193007 (3.1930)  Loss_crd: 20.722170 (20.7222)  Loss_g:  0.725248 (0.7252)  Loss_critic:  1.417938 (1.4179)  Time: 24.072s,   19.07/s  (24.072s,   19.07/s)  LR: 0.0100000  Data: 5.422 (5.422)\n",
      "INFO:root:Train: 1 [  50/8374 (  1%)]  Loss: 19.675608 (19.7001)  Loss_kd:  0.389105 (0.3828)  Loss_ce:  1.981173 (2.1080)  Loss_crd: 20.578053 (20.4913)  Loss_g:  0.842888 (0.8163)  Loss_critic:  1.150489 (1.2411)  Time: 1.176s,  390.41/s  (1.659s,  276.60/s)  LR: 0.0100000  Data: 0.013 (0.119)\n",
      "INFO:root:Train: 1 [ 100/8374 (  1%)]  Loss: 19.489040 (19.6058)  Loss_kd:  0.397458 (0.3817)  Loss_ce:  2.005741 (2.0818)  Loss_crd: 20.416481 (20.4803)  Loss_g:  0.752655 (0.7580)  Loss_critic:  1.813709 (1.4150)  Time: 1.188s,  386.21/s  (1.428s,  321.45/s)  LR: 0.0100000  Data: 0.013 (0.067)\n",
      "INFO:root:Train: 1 [ 150/8374 (  2%)]  Loss: 19.994667 (19.6123)  Loss_kd:  0.431213 (0.3875)  Loss_ce:  2.216190 (2.0734)  Loss_crd: 20.626869 (20.4904)  Loss_g:  0.845770 (0.7591)  Loss_critic:  2.180995 (1.5523)  Time: 1.176s,  390.40/s  (1.350s,  340.09/s)  LR: 0.0100000  Data: 0.015 (0.049)\n",
      "INFO:root:Train: 1 [ 200/8374 (  2%)]  Loss: 19.902691 (19.6719)  Loss_kd:  0.502814 (0.4106)  Loss_ce:  2.491061 (2.1305)  Loss_crd: 20.406990 (20.4786)  Loss_g:  0.583222 (0.7479)  Loss_critic:  5.284451 (2.0411)  Time: 1.200s,  382.46/s  (1.308s,  350.82/s)  LR: 0.0100000  Data: 0.013 (0.040)\n",
      "INFO:root:Train: 1 [ 250/8374 (  3%)]  Loss: 19.446928 (19.6833)  Loss_kd:  0.454916 (0.4252)  Loss_ce:  2.458338 (2.2238)  Loss_crd: 20.470631 (20.4782)  Loss_g:  0.157170 (0.6518)  Loss_critic:  9.064334 (3.1512)  Time: 1.175s,  390.64/s  (1.283s,  357.66/s)  LR: 0.0100000  Data: 0.013 (0.035)\n",
      "INFO:root:Train: 1 [ 300/8374 (  4%)]  Loss: 18.885889 (19.6076)  Loss_kd:  0.451965 (0.4297)  Loss_ce:  2.044649 (2.2376)  Loss_crd: 20.434507 (20.4753)  Loss_g:  0.041667 (0.5601)  Loss_critic: 13.126589 (4.4638)  Time: 1.173s,  391.24/s  (1.266s,  362.54/s)  LR: 0.0100000  Data: 0.013 (0.031)\n",
      "INFO:root:Train: 1 [ 350/8374 (  4%)]  Loss: 19.272238 (19.5505)  Loss_kd:  0.434334 (0.4291)  Loss_ce:  2.364201 (2.2448)  Loss_crd: 20.448107 (20.4647)  Loss_g:  0.115218 (0.5048)  Loss_critic:  8.650868 (5.1986)  Time: 1.176s,  390.27/s  (1.254s,  366.03/s)  LR: 0.0100000  Data: 0.013 (0.028)\n",
      "INFO:root:Train: 1 [ 400/8374 (  5%)]  Loss: 19.577620 (19.5142)  Loss_kd:  0.392796 (0.4270)  Loss_ce:  2.621936 (2.2504)  Loss_crd: 20.466331 (20.4649)  Loss_g:  0.189821 (0.4648)  Loss_critic:  8.368309 (5.6101)  Time: 1.174s,  390.84/s  (1.245s,  368.60/s)  LR: 0.0100000  Data: 0.013 (0.027)\n",
      "INFO:root:Train: 1 [ 450/8374 (  5%)]  Loss: 19.409403 (19.5032)  Loss_kd:  0.475205 (0.4290)  Loss_ce:  2.275673 (2.2668)  Loss_crd: 20.281857 (20.4621)  Loss_g:  0.433041 (0.4377)  Loss_critic:  6.101957 (5.8764)  Time: 1.172s,  391.54/s  (1.238s,  370.70/s)  LR: 0.0100000  Data: 0.013 (0.025)\n",
      "INFO:root:Train: 1 [ 500/8374 (  6%)]  Loss: 19.031395 (19.4889)  Loss_kd:  0.429291 (0.4321)  Loss_ce:  2.037049 (2.2751)  Loss_crd: 20.543652 (20.4603)  Loss_g:  0.130132 (0.4135)  Loss_critic:  8.700692 (6.0986)  Time: 1.213s,  378.35/s  (1.233s,  372.32/s)  LR: 0.0100000  Data: 0.013 (0.024)\n",
      "INFO:root:Train: 1 [ 550/8374 (  7%)]  Loss: 19.155977 (19.4662)  Loss_kd:  0.432529 (0.4314)  Loss_ce:  2.307803 (2.2801)  Loss_crd: 20.505043 (20.4561)  Loss_g:  0.011609 (0.3899)  Loss_critic: 12.623128 (6.4512)  Time: 1.174s,  390.87/s  (1.229s,  373.51/s)  LR: 0.0100000  Data: 0.013 (0.023)\n",
      "INFO:root:Train: 1 [ 600/8374 (  7%)]  Loss: 19.231354 (19.4440)  Loss_kd:  0.424469 (0.4313)  Loss_ce:  2.499209 (2.2834)  Loss_crd: 20.250454 (20.4502)  Loss_g:  0.107314 (0.3692)  Loss_critic:  8.928348 (6.7603)  Time: 1.176s,  390.31/s  (1.225s,  374.79/s)  LR: 0.0100000  Data: 0.013 (0.022)\n",
      "INFO:root:Train: 1 [ 650/8374 (  8%)]  Loss: 19.487753 (19.4141)  Loss_kd:  0.431417 (0.4316)  Loss_ce:  2.473210 (2.2777)  Loss_crd: 20.432003 (20.4445)  Loss_g:  0.237523 (0.3492)  Loss_critic: 10.489492 (7.0639)  Time: 1.181s,  388.81/s  (1.221s,  375.80/s)  LR: 0.0100000  Data: 0.013 (0.021)\n",
      "INFO:root:Train: 1 [ 700/8374 (  8%)]  Loss: 19.207266 (19.3964)  Loss_kd:  0.451441 (0.4327)  Loss_ce:  2.173405 (2.2751)  Loss_crd: 20.470131 (20.4401)  Loss_g:  0.206317 (0.3364)  Loss_critic: 11.038422 (7.3359)  Time: 1.173s,  391.17/s  (1.219s,  376.66/s)  LR: 0.0100000  Data: 0.013 (0.021)\n",
      "INFO:root:Train: 1 [ 750/8374 (  9%)]  Loss: 19.447123 (19.3828)  Loss_kd:  0.448239 (0.4336)  Loss_ce:  2.508593 (2.2762)  Loss_crd: 20.418343 (20.4339)  Loss_g:  0.155618 (0.3258)  Loss_critic: 11.736362 (7.6093)  Time: 1.215s,  377.68/s  (1.216s,  377.45/s)  LR: 0.0100000  Data: 0.013 (0.020)\n",
      "INFO:root:Train: 1 [ 800/8374 ( 10%)]  Loss: 19.240404 (19.3698)  Loss_kd:  0.469216 (0.4337)  Loss_ce:  2.316789 (2.2749)  Loss_crd: 20.307125 (20.4300)  Loss_g:  0.208699 (0.3172)  Loss_critic: 14.004741 (7.9035)  Time: 1.174s,  390.90/s  (1.214s,  378.10/s)  LR: 0.0100000  Data: 0.013 (0.020)\n",
      "INFO:root:Train: 1 [ 850/8374 ( 10%)]  Loss: 19.333014 (19.3532)  Loss_kd:  0.435707 (0.4344)  Loss_ce:  2.417406 (2.2765)  Loss_crd: 20.422840 (20.4224)  Loss_g:  0.141628 (0.3044)  Loss_critic: 13.507323 (8.2960)  Time: 1.173s,  391.34/s  (1.212s,  378.73/s)  LR: 0.0100000  Data: 0.013 (0.019)\n",
      "INFO:root:Train: 1 [ 900/8374 ( 11%)]  Loss: 19.494392 (19.3419)  Loss_kd:  0.441120 (0.4343)  Loss_ce:  2.555050 (2.2787)  Loss_crd: 20.303598 (20.4163)  Loss_g:  0.255343 (0.2959)  Loss_critic: 12.259181 (8.5494)  Time: 1.188s,  386.48/s  (1.210s,  379.27/s)  LR: 0.0100000  Data: 0.013 (0.019)\n",
      "INFO:root:Train: 1 [ 950/8374 ( 11%)]  Loss: 19.064234 (19.3438)  Loss_kd:  0.487019 (0.4357)  Loss_ce:  2.185460 (2.2912)  Loss_crd: 20.404434 (20.4120)  Loss_g:  0.068208 (0.2874)  Loss_critic: 19.202794 (8.9109)  Time: 1.183s,  388.08/s  (1.209s,  379.80/s)  LR: 0.0100000  Data: 0.013 (0.019)\n",
      "INFO:root:Train: 1 [1000/8374 ( 12%)]  Loss: 19.229246 (19.3335)  Loss_kd:  0.453886 (0.4373)  Loss_ce:  2.239568 (2.2912)  Loss_crd: 20.487150 (20.4061)  Loss_g:  0.146070 (0.2802)  Loss_critic: 13.187478 (9.1927)  Time: 1.174s,  390.96/s  (1.207s,  380.21/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 1 [1050/8374 ( 13%)]  Loss: 18.797636 (19.3119)  Loss_kd:  0.406098 (0.4372)  Loss_ce:  2.075425 (2.2866)  Loss_crd: 20.210354 (20.3983)  Loss_g:  0.147828 (0.2696)  Loss_critic: 15.676399 (9.5569)  Time: 1.184s,  387.70/s  (1.206s,  380.56/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 1 [1100/8374 ( 13%)]  Loss: 18.979279 (19.3088)  Loss_kd:  0.469946 (0.4363)  Loss_ce:  2.196924 (2.2940)  Loss_crd: 20.371656 (20.3926)  Loss_g:  0.015084 (0.2645)  Loss_critic: 36.028068 (9.8641)  Time: 1.176s,  390.24/s  (1.205s,  380.96/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 1 [1150/8374 ( 14%)]  Loss: 19.262941 (19.2978)  Loss_kd:  0.452121 (0.4379)  Loss_ce:  2.357919 (2.2916)  Loss_crd: 20.334808 (20.3854)  Loss_g:  0.185056 (0.2600)  Loss_critic: 12.682052 (10.3546)  Time: 1.184s,  387.78/s  (1.204s,  381.33/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 1 [1200/8374 ( 14%)]  Loss: 18.510380 (19.2905)  Loss_kd:  0.422755 (0.4386)  Loss_ce:  1.981871 (2.2939)  Loss_crd: 19.989941 (20.3807)  Loss_g:  0.113800 (0.2533)  Loss_critic: 29.904837 (10.8228)  Time: 1.176s,  390.46/s  (1.203s,  381.67/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 1 [1250/8374 ( 15%)]  Loss: 18.989079 (19.2793)  Loss_kd:  0.432361 (0.4379)  Loss_ce:  2.158656 (2.2936)  Loss_crd: 20.300676 (20.3744)  Loss_g:  0.157520 (0.2483)  Loss_critic: 13.406004 (11.0898)  Time: 1.176s,  390.18/s  (1.202s,  381.94/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 1 [1300/8374 ( 16%)]  Loss: 18.916916 (19.2663)  Loss_kd:  0.429104 (0.4381)  Loss_ce:  2.233302 (2.2916)  Loss_crd: 20.295153 (20.3682)  Loss_g:  0.018388 (0.2420)  Loss_critic: 17.729026 (11.2571)  Time: 1.176s,  390.22/s  (1.201s,  382.20/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 1 [1350/8374 ( 16%)]  Loss: 19.092482 (19.2538)  Loss_kd:  0.434528 (0.4378)  Loss_ce:  2.278958 (2.2898)  Loss_crd: 20.354099 (20.3622)  Loss_g:  0.095717 (0.2363)  Loss_critic: 13.485906 (11.4185)  Time: 1.175s,  390.56/s  (1.200s,  382.43/s)  LR: 0.0100000  Data: 0.013 (0.017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 1 [1400/8374 ( 17%)]  Loss: 18.818636 (19.2411)  Loss_kd:  0.415783 (0.4375)  Loss_ce:  2.375732 (2.2874)  Loss_crd: 19.984623 (20.3563)  Loss_g:  0.039423 (0.2313)  Loss_critic: 13.604064 (11.5019)  Time: 1.182s,  388.35/s  (1.199s,  382.70/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 1 [1450/8374 ( 17%)]  Loss: 18.750465 (19.2324)  Loss_kd:  0.450237 (0.4367)  Loss_ce:  2.303662 (2.2870)  Loss_crd: 19.930775 (20.3503)  Loss_g:  0.051946 (0.2285)  Loss_critic: 16.697525 (11.5447)  Time: 1.171s,  391.94/s  (1.199s,  382.95/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 1 [1500/8374 ( 18%)]  Loss: 18.951246 (19.2230)  Loss_kd:  0.416260 (0.4370)  Loss_ce:  2.378465 (2.2863)  Loss_crd: 20.100838 (20.3441)  Loss_g:  0.075849 (0.2245)  Loss_critic: 12.284160 (11.6823)  Time: 1.173s,  391.32/s  (1.198s,  383.14/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 1 [1550/8374 ( 19%)]  Loss: 19.092564 (19.2131)  Loss_kd:  0.438700 (0.4368)  Loss_ce:  2.345758 (2.2860)  Loss_crd: 20.301056 (20.3382)  Loss_g:  0.067262 (0.2197)  Loss_critic: 19.703545 (11.8679)  Time: 1.217s,  377.05/s  (1.197s,  383.33/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 1 [1600/8374 ( 19%)]  Loss: 19.163132 (19.2076)  Loss_kd:  0.457015 (0.4363)  Loss_ce:  2.542117 (2.2882)  Loss_crd: 20.094501 (20.3321)  Loss_g:  0.088397 (0.2175)  Loss_critic: 17.664498 (11.9418)  Time: 1.173s,  391.34/s  (1.197s,  383.51/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [1650/8374 ( 20%)]  Loss: 18.837219 (19.1983)  Loss_kd:  0.452241 (0.4371)  Loss_ce:  2.042156 (2.2874)  Loss_crd: 20.327377 (20.3263)  Loss_g:  0.080922 (0.2128)  Loss_critic: 14.921954 (12.3146)  Time: 1.176s,  390.32/s  (1.196s,  383.67/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [1700/8374 ( 20%)]  Loss: 19.066935 (19.1891)  Loss_kd:  0.462604 (0.4376)  Loss_ce:  2.310528 (2.2863)  Loss_crd: 20.289074 (20.3197)  Loss_g:  0.062541 (0.2094)  Loss_critic: 20.043874 (12.4490)  Time: 1.173s,  391.20/s  (1.196s,  383.80/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [1750/8374 ( 21%)]  Loss: 19.061571 (19.1773)  Loss_kd:  0.413539 (0.4371)  Loss_ce:  2.388164 (2.2832)  Loss_crd: 20.131865 (20.3148)  Loss_g:  0.154377 (0.2051)  Loss_critic: 14.365631 (12.6638)  Time: 1.190s,  385.65/s  (1.196s,  383.94/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [1800/8374 ( 21%)]  Loss: 18.543045 (19.1671)  Loss_kd:  0.422982 (0.4362)  Loss_ce:  2.074455 (2.2825)  Loss_crd: 19.993355 (20.3085)  Loss_g:  0.050925 (0.2016)  Loss_critic: 18.360410 (12.7180)  Time: 1.173s,  391.36/s  (1.195s,  384.09/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [1850/8374 ( 22%)]  Loss: 18.521563 (19.1552)  Loss_kd:  0.434924 (0.4361)  Loss_ce:  2.106239 (2.2790)  Loss_crd: 19.888180 (20.3025)  Loss_g:  0.069854 (0.1982)  Loss_critic: 13.720412 (12.8201)  Time: 1.178s,  389.65/s  (1.195s,  384.15/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [1900/8374 ( 23%)]  Loss: 18.974556 (19.1452)  Loss_kd:  0.422386 (0.4356)  Loss_ce:  2.411084 (2.2767)  Loss_crd: 20.128727 (20.2969)  Loss_g:  0.038106 (0.1953)  Loss_critic: 15.865095 (12.8409)  Time: 1.214s,  377.96/s  (1.195s,  384.22/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [1950/8374 ( 23%)]  Loss: 18.822952 (19.1378)  Loss_kd:  0.420242 (0.4349)  Loss_ce:  2.296430 (2.2765)  Loss_crd: 19.977570 (20.2914)  Loss_g:  0.124225 (0.1933)  Loss_critic: 13.369711 (12.8778)  Time: 1.189s,  386.18/s  (1.194s,  384.31/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [2000/8374 ( 24%)]  Loss: 19.092258 (19.1322)  Loss_kd:  0.473613 (0.4355)  Loss_ce:  2.334789 (2.2767)  Loss_crd: 20.071564 (20.2855)  Loss_g:  0.226606 (0.1916)  Loss_critic: 13.567859 (12.9190)  Time: 1.174s,  390.89/s  (1.194s,  384.40/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [2050/8374 ( 24%)]  Loss: 19.242083 (19.1307)  Loss_kd:  0.461614 (0.4362)  Loss_ce:  2.444342 (2.2792)  Loss_crd: 20.341612 (20.2799)  Loss_g:  0.062835 (0.1914)  Loss_critic: 17.253815 (12.9473)  Time: 1.174s,  391.12/s  (1.194s,  384.51/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [2100/8374 ( 25%)]  Loss: 19.098644 (19.1247)  Loss_kd:  0.421034 (0.4360)  Loss_ce:  2.252712 (2.2799)  Loss_crd: 20.105148 (20.2742)  Loss_g:  0.340779 (0.1894)  Loss_critic: 13.571147 (13.0622)  Time: 1.175s,  390.78/s  (1.193s,  384.64/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [2150/8374 ( 26%)]  Loss: 18.536728 (19.1176)  Loss_kd:  0.437647 (0.4361)  Loss_ce:  2.020112 (2.2803)  Loss_crd: 20.051826 (20.2685)  Loss_g:  0.037507 (0.1864)  Loss_critic: 24.444160 (13.2339)  Time: 1.183s,  387.93/s  (1.193s,  384.73/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 1 [2200/8374 ( 26%)]  Loss: 18.901802 (19.1092)  Loss_kd:  0.458445 (0.4362)  Loss_ce:  2.383760 (2.2781)  Loss_crd: 20.023041 (20.2623)  Loss_g:  0.041164 (0.1851)  Loss_critic: 16.202260 (13.3153)  Time: 1.173s,  391.30/s  (1.193s,  384.83/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2250/8374 ( 27%)]  Loss: 18.892727 (19.1003)  Loss_kd:  0.409801 (0.4362)  Loss_ce:  2.279807 (2.2765)  Loss_crd: 20.055517 (20.2560)  Loss_g:  0.158705 (0.1827)  Loss_critic: 13.611181 (13.4039)  Time: 1.174s,  391.00/s  (1.193s,  384.90/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2300/8374 ( 27%)]  Loss: 18.746542 (19.0946)  Loss_kd:  0.471869 (0.4366)  Loss_ce:  2.345358 (2.2778)  Loss_crd: 19.790941 (20.2489)  Loss_g:  0.096561 (0.1811)  Loss_critic: 16.588073 (13.4535)  Time: 1.175s,  390.67/s  (1.192s,  384.99/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2350/8374 ( 28%)]  Loss: 18.586056 (19.0891)  Loss_kd:  0.484905 (0.4372)  Loss_ce:  2.137771 (2.2769)  Loss_crd: 19.899437 (20.2437)  Loss_g:  0.043830 (0.1800)  Loss_critic: 22.552234 (13.5314)  Time: 1.175s,  390.69/s  (1.192s,  385.10/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2400/8374 ( 29%)]  Loss: 19.134508 (19.0866)  Loss_kd:  0.462593 (0.4380)  Loss_ce:  2.557034 (2.2796)  Loss_crd: 19.810257 (20.2375)  Loss_g:  0.266676 (0.1790)  Loss_critic: 15.284996 (13.6238)  Time: 1.179s,  389.42/s  (1.192s,  385.16/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2450/8374 ( 29%)]  Loss: 18.924795 (19.0837)  Loss_kd:  0.457575 (0.4382)  Loss_ce:  2.310124 (2.2833)  Loss_crd: 20.053892 (20.2306)  Loss_g:  0.113984 (0.1777)  Loss_critic: 20.506760 (13.6635)  Time: 1.173s,  391.46/s  (1.192s,  385.22/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2500/8374 ( 30%)]  Loss: 18.914043 (19.0786)  Loss_kd:  0.486498 (0.4392)  Loss_ce:  2.264969 (2.2827)  Loss_crd: 20.152683 (20.2248)  Loss_g:  0.040431 (0.1768)  Loss_critic: 18.100658 (13.8670)  Time: 1.172s,  391.55/s  (1.191s,  385.25/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2550/8374 ( 30%)]  Loss: 19.422047 (19.0757)  Loss_kd:  0.455774 (0.4398)  Loss_ce:  2.796664 (2.2854)  Loss_crd: 20.128357 (20.2186)  Loss_g:  0.066924 (0.1757)  Loss_critic: 15.900055 (13.9508)  Time: 1.224s,  375.08/s  (1.191s,  385.34/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2600/8374 ( 31%)]  Loss: 18.595076 (19.0749)  Loss_kd:  0.449582 (0.4401)  Loss_ce:  2.113169 (2.2905)  Loss_crd: 19.943272 (20.2128)  Loss_g:  0.077708 (0.1739)  Loss_critic: 14.912486 (13.9652)  Time: 1.184s,  387.67/s  (1.191s,  385.40/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2650/8374 ( 32%)]  Loss: 18.654612 (19.0705)  Loss_kd:  0.470942 (0.4405)  Loss_ce:  2.258542 (2.2913)  Loss_crd: 19.789093 (20.2068)  Loss_g:  0.093850 (0.1733)  Loss_critic: 15.384714 (13.9575)  Time: 1.175s,  390.50/s  (1.191s,  385.43/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2700/8374 ( 32%)]  Loss: 18.493147 (19.0660)  Loss_kd:  0.429896 (0.4407)  Loss_ce:  2.214381 (2.2923)  Loss_crd: 19.728878 (20.2009)  Loss_g:  0.065766 (0.1722)  Loss_critic: 16.909554 (14.0053)  Time: 1.174s,  391.05/s  (1.191s,  385.45/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2750/8374 ( 33%)]  Loss: 18.549145 (19.0612)  Loss_kd:  0.458275 (0.4409)  Loss_ce:  2.204495 (2.2932)  Loss_crd: 19.699839 (20.1949)  Loss_g:  0.126503 (0.1712)  Loss_critic: 14.806036 (14.0161)  Time: 1.174s,  391.01/s  (1.191s,  385.53/s)  LR: 0.0100000  Data: 0.013 (0.015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 1 [2800/8374 ( 33%)]  Loss: 18.951273 (19.0556)  Loss_kd:  0.434765 (0.4414)  Loss_ce:  2.360312 (2.2934)  Loss_crd: 19.926834 (20.1890)  Loss_g:  0.214730 (0.1695)  Loss_critic: 11.995063 (14.0446)  Time: 1.177s,  389.96/s  (1.190s,  385.57/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2850/8374 ( 34%)]  Loss: 18.678247 (19.0508)  Loss_kd:  0.496733 (0.4416)  Loss_ce:  2.307420 (2.2945)  Loss_crd: 19.751501 (20.1826)  Loss_g:  0.072893 (0.1685)  Loss_critic: 21.805289 (14.0728)  Time: 1.175s,  390.56/s  (1.190s,  385.65/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2900/8374 ( 35%)]  Loss: 19.197048 (19.0488)  Loss_kd:  0.522961 (0.4424)  Loss_ce:  2.681200 (2.2969)  Loss_crd: 19.852974 (20.1762)  Loss_g:  0.110508 (0.1685)  Loss_critic: 17.192480 (14.0947)  Time: 1.175s,  390.74/s  (1.190s,  385.70/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [2950/8374 ( 35%)]  Loss: 18.890476 (19.0456)  Loss_kd:  0.425237 (0.4428)  Loss_ce:  2.642775 (2.2994)  Loss_crd: 19.623512 (20.1702)  Loss_g:  0.123656 (0.1673)  Loss_critic: 14.026688 (14.1306)  Time: 1.174s,  391.11/s  (1.190s,  385.75/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3000/8374 ( 36%)]  Loss: 18.339916 (19.0404)  Loss_kd:  0.437515 (0.4430)  Loss_ce:  2.096409 (2.3002)  Loss_crd: 19.647999 (20.1643)  Loss_g:  0.087593 (0.1658)  Loss_critic: 18.181521 (14.2098)  Time: 1.178s,  389.74/s  (1.190s,  385.80/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3050/8374 ( 36%)]  Loss: 18.602135 (19.0350)  Loss_kd:  0.463885 (0.4432)  Loss_ce:  2.309820 (2.2999)  Loss_crd: 19.702202 (20.1581)  Loss_g:  0.066668 (0.1655)  Loss_critic: 16.700897 (14.2053)  Time: 1.184s,  387.66/s  (1.190s,  385.84/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3100/8374 ( 37%)]  Loss: 19.085054 (19.0326)  Loss_kd:  0.456235 (0.4432)  Loss_ce:  2.461207 (2.3028)  Loss_crd: 20.025501 (20.1519)  Loss_g:  0.147211 (0.1652)  Loss_critic: 11.847108 (14.1964)  Time: 1.176s,  390.37/s  (1.189s,  385.90/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3150/8374 ( 38%)]  Loss: 19.157955 (19.0284)  Loss_kd:  0.443811 (0.4438)  Loss_ce:  2.538808 (2.3033)  Loss_crd: 19.908379 (20.1457)  Loss_g:  0.248633 (0.1647)  Loss_critic: 13.452684 (14.2688)  Time: 1.186s,  386.87/s  (1.189s,  385.94/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3200/8374 ( 38%)]  Loss: 18.903538 (19.0222)  Loss_kd:  0.433875 (0.4439)  Loss_ce:  2.490292 (2.3028)  Loss_crd: 19.685253 (20.1398)  Loss_g:  0.231168 (0.1637)  Loss_critic: 15.451416 (14.3138)  Time: 1.189s,  385.99/s  (1.189s,  386.00/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3250/8374 ( 39%)]  Loss: 18.320320 (19.0185)  Loss_kd:  0.466315 (0.4439)  Loss_ce:  2.173403 (2.3047)  Loss_crd: 19.491955 (20.1338)  Loss_g:  0.087038 (0.1629)  Loss_critic: 19.158566 (14.3462)  Time: 1.192s,  385.09/s  (1.189s,  386.04/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3300/8374 ( 39%)]  Loss: 18.761118 (19.0171)  Loss_kd:  0.475358 (0.4447)  Loss_ce:  2.487121 (2.3067)  Loss_crd: 19.696312 (20.1273)  Loss_g:  0.041590 (0.1638)  Loss_critic: 16.569567 (14.3502)  Time: 1.174s,  390.97/s  (1.189s,  386.07/s)  LR: 0.0100000  Data: 0.014 (0.015)\n",
      "INFO:root:Train: 1 [3350/8374 ( 40%)]  Loss: 19.220181 (19.0161)  Loss_kd:  0.419589 (0.4447)  Loss_ce:  2.735862 (2.3116)  Loss_crd: 19.779205 (20.1211)  Loss_g:  0.241366 (0.1630)  Loss_critic: 14.109758 (14.3883)  Time: 1.174s,  390.85/s  (1.189s,  386.11/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3400/8374 ( 41%)]  Loss: 18.545191 (19.0124)  Loss_kd:  0.488890 (0.4452)  Loss_ce:  2.148998 (2.3130)  Loss_crd: 19.822472 (20.1150)  Loss_g:  0.049324 (0.1621)  Loss_critic: 20.529349 (14.5473)  Time: 1.184s,  387.58/s  (1.189s,  386.14/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3450/8374 ( 41%)]  Loss: 18.861618 (19.0090)  Loss_kd:  0.448824 (0.4457)  Loss_ce:  2.327987 (2.3140)  Loss_crd: 19.909582 (20.1087)  Loss_g:  0.157141 (0.1623)  Loss_critic: 14.707325 (14.5525)  Time: 1.179s,  389.20/s  (1.189s,  386.17/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3500/8374 ( 42%)]  Loss: 19.571871 (19.0254)  Loss_kd:  0.536252 (0.4473)  Loss_ce:  3.230584 (2.3336)  Loss_crd: 19.700747 (20.1030)  Loss_g:  0.044437 (0.1621)  Loss_critic: 19.314713 (14.5715)  Time: 1.175s,  390.49/s  (1.189s,  386.18/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3550/8374 ( 42%)]  Loss: 19.051655 (19.0280)  Loss_kd:  0.514566 (0.4485)  Loss_ce:  2.599523 (2.3404)  Loss_crd: 19.643618 (20.0973)  Loss_g:  0.222671 (0.1613)  Loss_critic: 11.657130 (14.7020)  Time: 1.185s,  387.26/s  (1.188s,  386.22/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3600/8374 ( 43%)]  Loss: 18.709930 (19.0271)  Loss_kd:  0.467810 (0.4494)  Loss_ce:  2.452936 (2.3445)  Loss_crd: 19.590248 (20.0910)  Loss_g:  0.116985 (0.1604)  Loss_critic: 15.307873 (14.7505)  Time: 1.186s,  386.91/s  (1.188s,  386.25/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 1 [3650/8374 ( 44%)]  Loss: 18.894581 (19.0297)  Loss_kd:  0.580864 (0.4498)  Loss_ce:  2.447096 (2.3516)  Loss_crd: 19.800102 (20.0849)  Loss_g:  0.026539 (0.1603)  Loss_critic: 35.770411 (14.7814)  Time: 1.175s,  390.64/s  (1.188s,  386.28/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [3700/8374 ( 44%)]  Loss: 18.995037 (19.0288)  Loss_kd:  0.561990 (0.4510)  Loss_ce:  2.565454 (2.3530)  Loss_crd: 19.489746 (20.0789)  Loss_g:  0.275796 (0.1617)  Loss_critic: 15.910193 (14.8464)  Time: 1.186s,  387.17/s  (1.188s,  386.32/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [3750/8374 ( 45%)]  Loss: 19.067480 (19.0304)  Loss_kd:  0.487082 (0.4518)  Loss_ce:  3.017217 (2.3592)  Loss_crd: 19.242390 (20.0725)  Loss_g:  0.169269 (0.1614)  Loss_critic: 16.101758 (14.8926)  Time: 1.177s,  389.92/s  (1.188s,  386.36/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [3800/8374 ( 45%)]  Loss: 18.623367 (19.0318)  Loss_kd:  0.516404 (0.4526)  Loss_ce:  2.541354 (2.3651)  Loss_crd: 19.376961 (20.0664)  Loss_g:  0.064040 (0.1611)  Loss_critic: 24.934479 (14.9673)  Time: 1.176s,  390.28/s  (1.188s,  386.41/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [3850/8374 ( 46%)]  Loss: 19.192446 (19.0303)  Loss_kd:  0.528060 (0.4532)  Loss_ce:  2.677285 (2.3674)  Loss_crd: 19.816969 (20.0604)  Loss_g:  0.133525 (0.1614)  Loss_critic: 18.586747 (14.9898)  Time: 1.173s,  391.18/s  (1.188s,  386.44/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [3900/8374 ( 47%)]  Loss: 19.250553 (19.0292)  Loss_kd:  0.435292 (0.4534)  Loss_ce:  2.806788 (2.3718)  Loss_crd: 19.563322 (20.0540)  Loss_g:  0.357814 (0.1608)  Loss_critic: 11.205272 (15.0114)  Time: 1.178s,  389.57/s  (1.188s,  386.43/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [3950/8374 ( 47%)]  Loss: 18.918680 (19.0264)  Loss_kd:  0.488389 (0.4542)  Loss_ce:  2.550269 (2.3737)  Loss_crd: 19.583282 (20.0480)  Loss_g:  0.213395 (0.1601)  Loss_critic: 19.625369 (15.0939)  Time: 1.181s,  388.70/s  (1.188s,  386.42/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4000/8374 ( 48%)]  Loss: 18.577673 (19.0236)  Loss_kd:  0.500940 (0.4548)  Loss_ce:  2.410309 (2.3754)  Loss_crd: 19.547173 (20.0420)  Loss_g:  0.028686 (0.1598)  Loss_critic: 40.334936 (15.1967)  Time: 1.174s,  390.99/s  (1.188s,  386.46/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4050/8374 ( 48%)]  Loss: 19.117796 (19.0204)  Loss_kd:  0.459260 (0.4550)  Loss_ce:  2.709629 (2.3772)  Loss_crd: 19.740906 (20.0364)  Loss_g:  0.156183 (0.1591)  Loss_critic: 14.982964 (15.2985)  Time: 1.194s,  384.57/s  (1.188s,  386.48/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4100/8374 ( 49%)]  Loss: 19.066641 (19.0165)  Loss_kd:  0.471571 (0.4552)  Loss_ce:  2.666868 (2.3788)  Loss_crd: 19.567860 (20.0305)  Loss_g:  0.273913 (0.1581)  Loss_critic: 11.229702 (15.3031)  Time: 1.184s,  387.76/s  (1.188s,  386.52/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4150/8374 ( 50%)]  Loss: 18.852968 (19.0126)  Loss_kd:  0.472527 (0.4554)  Loss_ce:  2.661783 (2.3800)  Loss_crd: 19.570354 (20.0246)  Loss_g:  0.062377 (0.1575)  Loss_critic: 17.362702 (15.3042)  Time: 1.184s,  387.58/s  (1.187s,  386.55/s)  LR: 0.0100000  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 1 [4200/8374 ( 50%)]  Loss: 18.987947 (19.0083)  Loss_kd:  0.508996 (0.4556)  Loss_ce:  2.745548 (2.3806)  Loss_crd: 19.601488 (20.0184)  Loss_g:  0.052214 (0.1573)  Loss_critic: 18.278389 (15.2940)  Time: 1.174s,  391.08/s  (1.187s,  386.58/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4250/8374 ( 51%)]  Loss: 19.689116 (19.0077)  Loss_kd:  0.474097 (0.4557)  Loss_ce:  3.347164 (2.3846)  Loss_crd: 19.567535 (20.0122)  Loss_g:  0.213827 (0.1576)  Loss_critic: 11.664228 (15.2813)  Time: 1.172s,  391.55/s  (1.187s,  386.60/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4300/8374 ( 51%)]  Loss: 18.570698 (19.0052)  Loss_kd:  0.487623 (0.4564)  Loss_ce:  2.392738 (2.3868)  Loss_crd: 19.545584 (20.0067)  Loss_g:  0.053871 (0.1566)  Loss_critic: 28.888927 (15.5530)  Time: 1.175s,  390.48/s  (1.187s,  386.62/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4350/8374 ( 52%)]  Loss: 18.375227 (19.0015)  Loss_kd:  0.526682 (0.4568)  Loss_ce:  2.223133 (2.3874)  Loss_crd: 19.309418 (20.0006)  Loss_g:  0.177877 (0.1567)  Loss_critic: 18.965665 (15.5741)  Time: 1.173s,  391.27/s  (1.187s,  386.66/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4400/8374 ( 53%)]  Loss: 19.159731 (18.9977)  Loss_kd:  0.436302 (0.4569)  Loss_ce:  2.691558 (2.3889)  Loss_crd: 19.699997 (19.9947)  Loss_g:  0.271874 (0.1560)  Loss_critic: 15.925909 (15.6389)  Time: 1.172s,  391.61/s  (1.187s,  386.69/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4450/8374 ( 53%)]  Loss: 18.604200 (18.9929)  Loss_kd:  0.460559 (0.4569)  Loss_ce:  2.450165 (2.3900)  Loss_crd: 19.525715 (19.9886)  Loss_g:  0.072905 (0.1551)  Loss_critic: 24.378648 (15.7341)  Time: 1.185s,  387.29/s  (1.187s,  386.71/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4500/8374 ( 54%)]  Loss: 18.370594 (18.9878)  Loss_kd:  0.460214 (0.4573)  Loss_ce:  2.372340 (2.3895)  Loss_crd: 19.405869 (19.9822)  Loss_g:  0.013343 (0.1552)  Loss_critic: 18.711701 (15.7507)  Time: 1.176s,  390.37/s  (1.187s,  386.74/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4550/8374 ( 54%)]  Loss: 19.391071 (18.9856)  Loss_kd:  0.447637 (0.4570)  Loss_ce:  3.135633 (2.3928)  Loss_crd: 19.512615 (19.9762)  Loss_g:  0.197709 (0.1549)  Loss_critic: 11.605609 (15.7607)  Time: 1.255s,  365.60/s  (1.187s,  386.76/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4600/8374 ( 55%)]  Loss: 18.202154 (18.9811)  Loss_kd:  0.466724 (0.4576)  Loss_ce:  2.159308 (2.3935)  Loss_crd: 19.423334 (19.9701)  Loss_g:  0.037455 (0.1539)  Loss_critic: 29.275136 (15.9792)  Time: 1.177s,  390.12/s  (1.187s,  386.78/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4650/8374 ( 56%)]  Loss: 18.229954 (18.9758)  Loss_kd:  0.434598 (0.4577)  Loss_ce:  2.442530 (2.3931)  Loss_crd: 19.111532 (19.9639)  Loss_g:  0.063600 (0.1540)  Loss_critic: 17.227908 (15.9926)  Time: 1.174s,  391.09/s  (1.187s,  386.79/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4700/8374 ( 56%)]  Loss: 18.513750 (18.9725)  Loss_kd:  0.506566 (0.4576)  Loss_ce:  2.371273 (2.3951)  Loss_crd: 19.497599 (19.9579)  Loss_g:  0.037833 (0.1534)  Loss_critic: 15.879207 (15.9866)  Time: 1.176s,  390.17/s  (1.187s,  386.83/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4750/8374 ( 57%)]  Loss: 18.681351 (18.9677)  Loss_kd:  0.493662 (0.4579)  Loss_ce:  2.660293 (2.3949)  Loss_crd: 19.284098 (19.9521)  Loss_g:  0.100117 (0.1532)  Loss_critic: 17.720678 (15.9841)  Time: 1.174s,  391.06/s  (1.186s,  386.87/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4800/8374 ( 57%)]  Loss: 18.838024 (18.9623)  Loss_kd:  0.424668 (0.4578)  Loss_ce:  2.615854 (2.3948)  Loss_crd: 19.514921 (19.9465)  Loss_g:  0.185564 (0.1525)  Loss_critic: 12.659074 (16.0069)  Time: 1.172s,  391.73/s  (1.186s,  386.89/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4850/8374 ( 58%)]  Loss: 18.136019 (18.9566)  Loss_kd:  0.437425 (0.4578)  Loss_ce:  2.110748 (2.3947)  Loss_crd: 19.445902 (19.9405)  Loss_g:  0.031123 (0.1517)  Loss_critic: 19.338550 (16.0423)  Time: 1.176s,  390.23/s  (1.186s,  386.90/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4900/8374 ( 59%)]  Loss: 18.454926 (18.9501)  Loss_kd:  0.447389 (0.4575)  Loss_ce:  2.454489 (2.3940)  Loss_crd: 19.393120 (19.9343)  Loss_g:  0.038551 (0.1510)  Loss_critic: 23.866987 (16.0471)  Time: 1.176s,  390.24/s  (1.186s,  386.94/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [4950/8374 ( 59%)]  Loss: 18.523731 (18.9451)  Loss_kd:  0.482777 (0.4575)  Loss_ce:  2.481617 (2.3938)  Loss_crd: 19.282982 (19.9283)  Loss_g:  0.132953 (0.1512)  Loss_critic: 14.799324 (16.0465)  Time: 1.180s,  389.05/s  (1.186s,  386.95/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5000/8374 ( 60%)]  Loss: 18.632648 (18.9397)  Loss_kd:  0.425583 (0.4575)  Loss_ce:  2.450921 (2.3939)  Loss_crd: 19.577274 (19.9226)  Loss_g:  0.094324 (0.1502)  Loss_critic: 15.707803 (16.0712)  Time: 1.186s,  386.88/s  (1.186s,  386.97/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5050/8374 ( 60%)]  Loss: 18.373875 (18.9358)  Loss_kd:  0.496395 (0.4576)  Loss_ce:  2.445018 (2.3949)  Loss_crd: 19.277420 (19.9168)  Loss_g:  0.010526 (0.1499)  Loss_critic: 38.036667 (16.1276)  Time: 1.241s,  369.89/s  (1.186s,  387.00/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5100/8374 ( 61%)]  Loss: 18.407206 (18.9311)  Loss_kd:  0.477720 (0.4577)  Loss_ce:  2.433064 (2.3946)  Loss_crd: 19.167622 (19.9110)  Loss_g:  0.162323 (0.1499)  Loss_critic: 16.467513 (16.1588)  Time: 1.260s,  364.19/s  (1.186s,  387.01/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5150/8374 ( 62%)]  Loss: 18.838047 (18.9274)  Loss_kd:  0.426422 (0.4576)  Loss_ce:  2.588118 (2.3965)  Loss_crd: 19.497091 (19.9049)  Loss_g:  0.225835 (0.1493)  Loss_critic: 15.233088 (16.1800)  Time: 1.175s,  390.49/s  (1.186s,  387.01/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5200/8374 ( 62%)]  Loss: 18.321444 (18.9217)  Loss_kd:  0.461218 (0.4577)  Loss_ce:  2.398278 (2.3963)  Loss_crd: 19.311720 (19.8988)  Loss_g:  0.012571 (0.1487)  Loss_critic: 23.092334 (16.2532)  Time: 1.173s,  391.18/s  (1.186s,  387.02/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5250/8374 ( 63%)]  Loss: 18.533342 (18.9165)  Loss_kd:  0.412769 (0.4574)  Loss_ce:  2.492437 (2.3966)  Loss_crd: 19.327217 (19.8929)  Loss_g:  0.166363 (0.1482)  Loss_critic: 12.088216 (16.2548)  Time: 1.240s,  370.31/s  (1.186s,  387.02/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5300/8374 ( 63%)]  Loss: 18.301130 (18.9104)  Loss_kd:  0.434984 (0.4574)  Loss_ce:  2.280962 (2.3964)  Loss_crd: 19.105255 (19.8864)  Loss_g:  0.300980 (0.1476)  Loss_critic: 13.011561 (16.2750)  Time: 1.244s,  368.90/s  (1.186s,  387.02/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5350/8374 ( 64%)]  Loss: 18.106153 (18.9054)  Loss_kd:  0.479165 (0.4575)  Loss_ce:  2.269565 (2.3962)  Loss_crd: 19.172886 (19.8802)  Loss_g:  0.019114 (0.1476)  Loss_critic: 28.016164 (16.3217)  Time: 1.176s,  390.14/s  (1.186s,  387.03/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5400/8374 ( 64%)]  Loss: 18.404827 (18.9002)  Loss_kd:  0.449955 (0.4575)  Loss_ce:  2.417589 (2.3963)  Loss_crd: 19.325241 (19.8741)  Loss_g:  0.077091 (0.1472)  Loss_critic: 22.026364 (16.3529)  Time: 1.176s,  390.38/s  (1.186s,  387.06/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5450/8374 ( 65%)]  Loss: 18.737530 (18.8947)  Loss_kd:  0.457662 (0.4573)  Loss_ce:  2.650038 (2.3963)  Loss_crd: 19.329659 (19.8678)  Loss_g:  0.166103 (0.1468)  Loss_critic: 14.278310 (16.3551)  Time: 1.174s,  391.03/s  (1.186s,  387.08/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5500/8374 ( 66%)]  Loss: 18.219183 (18.8893)  Loss_kd:  0.436636 (0.4573)  Loss_ce:  2.266474 (2.3965)  Loss_crd: 19.247673 (19.8618)  Loss_g:  0.117932 (0.1461)  Loss_critic: 15.935310 (16.3906)  Time: 1.193s,  384.85/s  (1.186s,  387.11/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5550/8374 ( 66%)]  Loss: 18.104542 (18.8839)  Loss_kd:  0.441576 (0.4573)  Loss_ce:  2.222726 (2.3960)  Loss_crd: 19.213425 (19.8561)  Loss_g:  0.069500 (0.1457)  Loss_critic: 22.301011 (16.4053)  Time: 1.175s,  390.60/s  (1.186s,  387.12/s)  LR: 0.0100000  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 1 [5600/8374 ( 67%)]  Loss: 18.722103 (18.8784)  Loss_kd:  0.407795 (0.4569)  Loss_ce:  2.716431 (2.3963)  Loss_crd: 19.389179 (19.8500)  Loss_g:  0.086533 (0.1452)  Loss_critic: 14.565136 (16.4144)  Time: 1.174s,  390.83/s  (1.186s,  387.14/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5650/8374 ( 67%)]  Loss: 18.316092 (18.8722)  Loss_kd:  0.438888 (0.4566)  Loss_ce:  2.331856 (2.3955)  Loss_crd: 19.115696 (19.8441)  Loss_g:  0.252791 (0.1448)  Loss_critic: 11.675508 (16.4168)  Time: 1.180s,  388.94/s  (1.186s,  387.15/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5700/8374 ( 68%)]  Loss: 18.163465 (18.8657)  Loss_kd:  0.409553 (0.4566)  Loss_ce:  2.244828 (2.3945)  Loss_crd: 19.228725 (19.8380)  Loss_g:  0.126105 (0.1442)  Loss_critic: 13.989928 (16.4049)  Time: 1.174s,  390.99/s  (1.186s,  387.17/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5750/8374 ( 69%)]  Loss: 18.347242 (18.8623)  Loss_kd:  0.508096 (0.4564)  Loss_ce:  2.547672 (2.3960)  Loss_crd: 19.084450 (19.8320)  Loss_g:  0.023915 (0.1444)  Loss_critic: 25.594024 (16.3995)  Time: 1.178s,  389.54/s  (1.185s,  387.18/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5800/8374 ( 69%)]  Loss: 18.265306 (18.8571)  Loss_kd:  0.505705 (0.4566)  Loss_ce:  2.468123 (2.3953)  Loss_crd: 19.001648 (19.8258)  Loss_g:  0.090158 (0.1446)  Loss_critic: 18.388201 (16.4102)  Time: 1.173s,  391.25/s  (1.186s,  387.16/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5850/8374 ( 70%)]  Loss: 18.576286 (18.8530)  Loss_kd:  0.422675 (0.4565)  Loss_ce:  2.606548 (2.3961)  Loss_crd: 19.173912 (19.8200)  Loss_g:  0.207935 (0.1443)  Loss_critic: 15.579572 (16.4237)  Time: 1.171s,  391.93/s  (1.186s,  387.17/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5900/8374 ( 70%)]  Loss: 17.944498 (18.8482)  Loss_kd:  0.465005 (0.4565)  Loss_ce:  2.229422 (2.3965)  Loss_crd: 19.024080 (19.8142)  Loss_g:  0.030807 (0.1438)  Loss_critic: 59.434221 (16.6001)  Time: 1.171s,  391.87/s  (1.185s,  387.19/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [5950/8374 ( 71%)]  Loss: 17.976122 (18.8413)  Loss_kd:  0.496851 (0.4565)  Loss_ce:  2.138413 (2.3952)  Loss_crd: 19.086824 (19.8083)  Loss_g:  0.071398 (0.1430)  Loss_critic: 29.621169 (16.8675)  Time: 1.175s,  390.72/s  (1.185s,  387.21/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6000/8374 ( 72%)]  Loss: 18.479259 (18.8362)  Loss_kd:  0.423023 (0.4566)  Loss_ce:  2.635613 (2.3951)  Loss_crd: 19.208572 (19.8022)  Loss_g:  0.053763 (0.1427)  Loss_critic: 18.902758 (16.8748)  Time: 1.176s,  390.15/s  (1.185s,  387.23/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'sRGB' 41 1\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'gAMA' 54 4\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'cHRM' 70 32\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 114 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 135 65401\n",
      "INFO:root:Train: 1 [6050/8374 ( 72%)]  Loss: 18.041174 (18.8342)  Loss_kd:  0.518615 (0.4565)  Loss_ce:  2.234690 (2.3979)  Loss_crd: 19.082491 (19.7963)  Loss_g:  0.021877 (0.1427)  Loss_critic: 35.613437 (16.8900)  Time: 1.175s,  390.51/s  (1.185s,  387.26/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6100/8374 ( 73%)]  Loss: 18.240597 (18.8295)  Loss_kd:  0.472999 (0.4570)  Loss_ce:  2.328096 (2.3974)  Loss_crd: 19.110485 (19.7903)  Loss_g:  0.151113 (0.1429)  Loss_critic: 16.887181 (16.9659)  Time: 1.176s,  390.22/s  (1.185s,  387.27/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6150/8374 ( 73%)]  Loss: 18.367071 (18.8253)  Loss_kd:  0.415986 (0.4569)  Loss_ce:  2.551101 (2.3982)  Loss_crd: 19.128839 (19.7845)  Loss_g:  0.096913 (0.1426)  Loss_critic: 15.422150 (16.9852)  Time: 1.178s,  389.62/s  (1.185s,  387.30/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6200/8374 ( 74%)]  Loss: 18.057924 (18.8213)  Loss_kd:  0.498858 (0.4570)  Loss_ce:  2.300982 (2.3989)  Loss_crd: 18.903175 (19.7786)  Loss_g:  0.135544 (0.1426)  Loss_critic: 14.930070 (16.9577)  Time: 1.245s,  368.65/s  (1.185s,  387.32/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6250/8374 ( 75%)]  Loss: 18.543081 (18.8155)  Loss_kd:  0.445767 (0.4571)  Loss_ce:  2.616996 (2.3982)  Loss_crd: 19.155123 (19.7726)  Loss_g:  0.156219 (0.1422)  Loss_critic: 14.250083 (16.9772)  Time: 1.174s,  390.93/s  (1.185s,  387.34/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6300/8374 ( 75%)]  Loss: 18.475824 (18.8112)  Loss_kd:  0.470925 (0.4569)  Loss_ce:  2.539005 (2.3986)  Loss_crd: 19.044561 (19.7667)  Loss_g:  0.230245 (0.1423)  Loss_critic: 14.091410 (16.9637)  Time: 1.174s,  390.93/s  (1.185s,  387.36/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6350/8374 ( 76%)]  Loss: 18.090393 (18.8073)  Loss_kd:  0.494355 (0.4573)  Loss_ce:  2.397395 (2.3988)  Loss_crd: 18.980989 (19.7606)  Loss_g:  0.013851 (0.1428)  Loss_critic: 36.422006 (16.9918)  Time: 1.172s,  391.54/s  (1.185s,  387.39/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6400/8374 ( 76%)]  Loss: 18.223629 (18.8022)  Loss_kd:  0.459183 (0.4572)  Loss_ce:  2.455612 (2.3990)  Loss_crd: 19.088081 (19.7542)  Loss_g:  0.038368 (0.1425)  Loss_critic: 16.859503 (17.0187)  Time: 1.186s,  386.98/s  (1.185s,  387.40/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6450/8374 ( 77%)]  Loss: 18.269501 (18.7965)  Loss_kd:  0.470654 (0.4572)  Loss_ce:  2.469393 (2.3989)  Loss_crd: 18.869846 (19.7479)  Loss_g:  0.233575 (0.1422)  Loss_critic: 13.316630 (17.0088)  Time: 1.175s,  390.59/s  (1.185s,  387.41/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6500/8374 ( 78%)]  Loss: 18.464222 (18.7915)  Loss_kd:  0.421484 (0.4570)  Loss_ce:  2.698153 (2.3990)  Loss_crd: 19.049646 (19.7421)  Loss_g:  0.104868 (0.1418)  Loss_critic: 17.602627 (17.0180)  Time: 1.174s,  391.05/s  (1.185s,  387.42/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6550/8374 ( 78%)]  Loss: 17.937124 (18.7872)  Loss_kd:  0.521291 (0.4570)  Loss_ce:  2.259838 (2.3997)  Loss_crd: 18.755640 (19.7361)  Loss_g:  0.151483 (0.1416)  Loss_critic: 25.529302 (17.0244)  Time: 1.174s,  390.92/s  (1.185s,  387.44/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6600/8374 ( 79%)]  Loss: 18.307051 (18.7823)  Loss_kd:  0.478840 (0.4574)  Loss_ce:  2.414069 (2.3995)  Loss_crd: 19.112183 (19.7300)  Loss_g:  0.124396 (0.1413)  Loss_critic: 18.623915 (17.0505)  Time: 1.182s,  388.16/s  (1.185s,  387.43/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6650/8374 ( 79%)]  Loss: 18.113758 (18.7768)  Loss_kd:  0.459853 (0.4573)  Loss_ce:  2.555615 (2.3993)  Loss_crd: 18.859030 (19.7240)  Loss_g:  0.011066 (0.1410)  Loss_critic: 20.001706 (17.0441)  Time: 1.185s,  387.48/s  (1.185s,  387.44/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6700/8374 ( 80%)]  Loss: 17.725399 (18.7707)  Loss_kd:  0.429350 (0.4572)  Loss_ce:  2.120447 (2.3988)  Loss_crd: 18.842136 (19.7176)  Loss_g:  0.101893 (0.1406)  Loss_critic: 16.458204 (17.0370)  Time: 1.177s,  389.84/s  (1.185s,  387.45/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6750/8374 ( 81%)]  Loss: 18.099358 (18.7648)  Loss_kd:  0.467137 (0.4572)  Loss_ce:  2.466660 (2.3981)  Loss_crd: 18.871803 (19.7116)  Loss_g:  0.068118 (0.1402)  Loss_critic: 15.219210 (17.0433)  Time: 1.174s,  390.90/s  (1.185s,  387.46/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6800/8374 ( 81%)]  Loss: 20.959995 (18.7740)  Loss_kd:  0.704626 (0.4583)  Loss_ce:  5.152321 (2.4111)  Loss_crd: 18.698433 (19.7055)  Loss_g:  0.144303 (0.1401)  Loss_critic: 12.760174 (17.0348)  Time: 1.174s,  391.10/s  (1.185s,  387.47/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6850/8374 ( 82%)]  Loss: 19.540186 (18.7845)  Loss_kd:  0.663174 (0.4601)  Loss_ce:  3.629565 (2.4251)  Loss_crd: 18.847158 (19.6996)  Loss_g:  0.169720 (0.1397)  Loss_critic: 21.196086 (17.0938)  Time: 1.233s,  372.13/s  (1.185s,  387.46/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [6900/8374 ( 82%)]  Loss: 19.132574 (18.7904)  Loss_kd:  0.602046 (0.4613)  Loss_ce:  3.432426 (2.4346)  Loss_crd: 18.838892 (19.6939)  Loss_g:  0.026988 (0.1394)  Loss_critic: 25.630248 (17.1725)  Time: 1.178s,  389.77/s  (1.185s,  387.46/s)  LR: 0.0100000  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 1 [6950/8374 ( 83%)]  Loss: 18.676432 (18.7932)  Loss_kd:  0.625959 (0.4622)  Loss_ce:  3.112163 (2.4417)  Loss_crd: 18.640774 (19.6879)  Loss_g:  0.025690 (0.1390)  Loss_critic: 25.476229 (17.1822)  Time: 1.240s,  370.04/s  (1.185s,  387.47/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7000/8374 ( 84%)]  Loss: 18.400888 (18.7941)  Loss_kd:  0.563430 (0.4631)  Loss_ce:  2.940079 (2.4470)  Loss_crd: 18.603928 (19.6816)  Loss_g:  0.014236 (0.1388)  Loss_critic: 22.891755 (17.1893)  Time: 1.174s,  390.94/s  (1.185s,  387.47/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7050/8374 ( 84%)]  Loss: 18.875568 (18.7943)  Loss_kd:  0.545014 (0.4635)  Loss_ce:  3.171125 (2.4520)  Loss_crd: 18.831345 (19.6755)  Loss_g:  0.094353 (0.1383)  Loss_critic: 15.222957 (17.1950)  Time: 1.173s,  391.20/s  (1.185s,  387.46/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7100/8374 ( 85%)]  Loss: 18.874449 (18.7945)  Loss_kd:  0.592542 (0.4643)  Loss_ce:  3.082528 (2.4565)  Loss_crd: 18.929325 (19.6696)  Loss_g:  0.055918 (0.1380)  Loss_critic: 16.451495 (17.1928)  Time: 1.177s,  390.00/s  (1.185s,  387.47/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7150/8374 ( 85%)]  Loss: 19.397791 (18.7949)  Loss_kd:  0.556006 (0.4650)  Loss_ce:  3.494860 (2.4614)  Loss_crd: 18.762794 (19.6639)  Loss_g:  0.336689 (0.1375)  Loss_critic: 12.459361 (17.2154)  Time: 1.209s,  379.57/s  (1.185s,  387.47/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7200/8374 ( 86%)]  Loss: 18.473309 (18.7958)  Loss_kd:  0.572307 (0.4656)  Loss_ce:  2.909709 (2.4666)  Loss_crd: 18.695108 (19.6581)  Loss_g:  0.035205 (0.1371)  Loss_critic: 26.546651 (17.2507)  Time: 1.182s,  388.41/s  (1.185s,  387.48/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7250/8374 ( 87%)]  Loss: 18.480854 (18.7944)  Loss_kd:  0.499164 (0.4661)  Loss_ce:  2.942568 (2.4697)  Loss_crd: 18.778538 (19.6520)  Loss_g:  0.016292 (0.1370)  Loss_critic: 18.723653 (17.2469)  Time: 1.209s,  379.61/s  (1.185s,  387.49/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7300/8374 ( 87%)]  Loss: 18.173040 (18.7936)  Loss_kd:  0.539591 (0.4663)  Loss_ce:  2.485695 (2.4735)  Loss_crd: 18.857027 (19.6460)  Loss_g:  0.062131 (0.1369)  Loss_critic: 27.732949 (17.2386)  Time: 1.186s,  387.00/s  (1.185s,  387.49/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7350/8374 ( 88%)]  Loss: 18.574829 (18.7905)  Loss_kd:  0.522365 (0.4669)  Loss_ce:  2.904549 (2.4753)  Loss_crd: 18.781790 (19.6402)  Loss_g:  0.122484 (0.1362)  Loss_critic: 18.731488 (17.3870)  Time: 1.268s,  362.00/s  (1.185s,  387.49/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7400/8374 ( 88%)]  Loss: 18.739029 (18.7896)  Loss_kd:  0.497817 (0.4675)  Loss_ce:  2.984980 (2.4787)  Loss_crd: 18.925163 (19.6341)  Loss_g:  0.116100 (0.1362)  Loss_critic: 16.198411 (17.3832)  Time: 1.174s,  390.89/s  (1.185s,  387.50/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7450/8374 ( 89%)]  Loss: 18.307270 (18.7896)  Loss_kd:  0.598483 (0.4678)  Loss_ce:  2.817027 (2.4833)  Loss_crd: 18.537998 (19.6282)  Loss_g:  0.061361 (0.1359)  Loss_critic: 34.159625 (17.4152)  Time: 1.174s,  391.05/s  (1.184s,  387.51/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7500/8374 ( 90%)]  Loss: 20.821575 (18.7881)  Loss_kd:  0.689216 (0.4683)  Loss_ce:  5.208796 (2.4860)  Loss_crd: 18.608913 (19.6221)  Loss_g:  0.036435 (0.1361)  Loss_critic: 23.214245 (17.4284)  Time: 1.197s,  383.52/s  (1.184s,  387.52/s)  LR: 0.0100000  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 1 [7550/8374 ( 90%)]  Loss: 21.034029 (18.8081)  Loss_kd:  0.793772 (0.4701)  Loss_ce:  5.104829 (2.5095)  Loss_crd: 18.669184 (19.6162)  Loss_g:  0.200080 (0.1356)  Loss_critic: 12.250147 (17.4383)  Time: 1.173s,  391.45/s  (1.184s,  387.52/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7600/8374 ( 91%)]  Loss: 19.698092 (18.8169)  Loss_kd:  0.620776 (0.4715)  Loss_ce:  4.056365 (2.5222)  Loss_crd: 18.579323 (19.6101)  Loss_g:  0.157491 (0.1351)  Loss_critic: 13.349101 (17.4275)  Time: 1.173s,  391.43/s  (1.184s,  387.52/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7650/8374 ( 91%)]  Loss: 18.996342 (18.8213)  Loss_kd:  0.618603 (0.4725)  Loss_ce:  3.437883 (2.5307)  Loss_crd: 18.593998 (19.6042)  Loss_g:  0.064658 (0.1348)  Loss_critic: 16.400986 (17.4355)  Time: 1.181s,  388.64/s  (1.184s,  387.53/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7700/8374 ( 92%)]  Loss: 19.241829 (18.8240)  Loss_kd:  0.607145 (0.4734)  Loss_ce:  3.534172 (2.5372)  Loss_crd: 18.855820 (19.5982)  Loss_g:  0.015858 (0.1348)  Loss_critic: 20.542700 (17.4290)  Time: 1.179s,  389.19/s  (1.184s,  387.53/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7750/8374 ( 93%)]  Loss: 18.973831 (18.8257)  Loss_kd:  0.603147 (0.4739)  Loss_ce:  3.361521 (2.5436)  Loss_crd: 18.742180 (19.5920)  Loss_g:  0.015420 (0.1345)  Loss_critic: 24.628375 (17.4275)  Time: 1.175s,  390.61/s  (1.184s,  387.54/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7800/8374 ( 93%)]  Loss: 19.122831 (18.8264)  Loss_kd:  0.643058 (0.4749)  Loss_ce:  3.478700 (2.5484)  Loss_crd: 18.697861 (19.5858)  Loss_g:  0.042783 (0.1345)  Loss_critic: 20.866996 (17.4385)  Time: 1.173s,  391.23/s  (1.184s,  387.55/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7850/8374 ( 94%)]  Loss: 19.072279 (18.8278)  Loss_kd:  0.517789 (0.4755)  Loss_ce:  3.551838 (2.5543)  Loss_crd: 18.512161 (19.5798)  Loss_g:  0.192923 (0.1342)  Loss_critic: 11.972985 (17.4471)  Time: 1.173s,  391.23/s  (1.184s,  387.55/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "/home/cutz/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "INFO:root:Train: 1 [7900/8374 ( 94%)]  Loss: 18.690916 (18.8277)  Loss_kd:  0.636423 (0.4762)  Loss_ce:  3.157903 (2.5586)  Loss_crd: 18.300388 (19.5739)  Loss_g:  0.256277 (0.1338)  Loss_critic: 13.428094 (17.4574)  Time: 1.176s,  390.25/s  (1.184s,  387.55/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [7950/8374 ( 95%)]  Loss: 18.772722 (18.8273)  Loss_kd:  0.551287 (0.4768)  Loss_ce:  3.279057 (2.5626)  Loss_crd: 18.461050 (19.5680)  Loss_g:  0.173538 (0.1335)  Loss_critic: 15.309470 (17.4610)  Time: 1.175s,  390.64/s  (1.184s,  387.56/s)  LR: 0.0100000  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 1 [8000/8374 ( 96%)]  Loss: 18.239697 (18.8274)  Loss_kd:  0.566776 (0.4772)  Loss_ce:  2.977072 (2.5674)  Loss_crd: 18.354671 (19.5619)  Loss_g:  0.012112 (0.1333)  Loss_critic: 37.598610 (17.5226)  Time: 1.172s,  391.75/s  (1.184s,  387.56/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [8050/8374 ( 96%)]  Loss: 18.804270 (18.8270)  Loss_kd:  0.566919 (0.4780)  Loss_ce:  3.339725 (2.5707)  Loss_crd: 18.594639 (19.5562)  Loss_g:  0.021913 (0.1335)  Loss_critic: 24.749068 (17.5424)  Time: 1.175s,  390.60/s  (1.184s,  387.57/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [8100/8374 ( 97%)]  Loss: 18.607517 (18.8275)  Loss_kd:  0.500658 (0.4782)  Loss_ce:  3.222725 (2.5759)  Loss_crd: 18.496912 (19.5503)  Loss_g:  0.086602 (0.1331)  Loss_critic: 14.562160 (17.5457)  Time: 1.174s,  390.89/s  (1.184s,  387.58/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [8150/8374 ( 97%)]  Loss: 19.096592 (18.8265)  Loss_kd:  0.573519 (0.4790)  Loss_ce:  3.600246 (2.5792)  Loss_crd: 18.576561 (19.5442)  Loss_g:  0.061577 (0.1329)  Loss_critic: 20.493927 (17.5452)  Time: 1.174s,  391.09/s  (1.184s,  387.59/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [8200/8374 ( 98%)]  Loss: 18.999964 (18.8283)  Loss_kd:  0.623216 (0.4797)  Loss_ce:  3.440302 (2.5854)  Loss_crd: 18.662079 (19.5383)  Loss_g:  0.006783 (0.1326)  Loss_critic: 36.186517 (17.5569)  Time: 1.172s,  391.67/s  (1.184s,  387.60/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [8250/8374 ( 99%)]  Loss: 18.871483 (18.8294)  Loss_kd:  0.634452 (0.4803)  Loss_ce:  3.417362 (2.5909)  Loss_crd: 18.441444 (19.5324)  Loss_g:  0.066513 (0.1322)  Loss_critic: 16.718346 (17.5560)  Time: 1.177s,  389.87/s  (1.184s,  387.61/s)  LR: 0.0100000  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 1 [8300/8374 ( 99%)]  Loss: 18.529005 (18.8341)  Loss_kd:  0.608237 (0.4814)  Loss_ce:  3.175880 (2.5996)  Loss_crd: 18.377851 (19.5265)  Loss_g:  0.042607 (0.1319)  Loss_critic: 17.075572 (17.5501)  Time: 1.175s,  390.71/s  (1.184s,  387.63/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [8350/8374 (100%)]  Loss: 18.949936 (18.8352)  Loss_kd:  0.596438 (0.4820)  Loss_ce:  3.472723 (2.6051)  Loss_crd: 18.516731 (19.5207)  Loss_g:  0.067390 (0.1316)  Loss_critic: 14.373836 (17.5365)  Time: 1.186s,  387.17/s  (1.184s,  387.63/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 1 [8373/8374 (100%)]  Loss: 18.878410 (18.8352)  Loss_kd:  0.597585 (0.4823)  Loss_ce:  3.363386 (2.6069)  Loss_crd: 18.488459 (19.5180)  Loss_g:  0.126672 (0.1316)  Loss_critic: 13.444145 (17.5244)  Time: 7.205s,   40.81/s  (1.185s,  248.14/s)  LR: 0.0100000  Data: 0.008 (0.014)\n",
      "INFO:root:Test: [   0/327]  Time: 7.410s (7.410s,   20.65/s)  Loss:  1.5958 (1.5958)  Acc@1:  71.242 ( 71.242)  Acc@5:  88.889 ( 88.889)\n",
      "INFO:root:Test: [  10/327]  Time: 0.349s (0.985s,  155.27/s)  Loss:  2.2209 (1.7932)  Acc@1:  52.941 ( 65.597)  Acc@5:  79.085 ( 86.393)\n",
      "INFO:root:Test: [  20/327]  Time: 0.333s (0.676s,  226.35/s)  Loss:  3.0016 (2.0914)  Acc@1:  37.908 ( 58.948)  Acc@5:  67.320 ( 83.287)\n",
      "INFO:root:Test: [  30/327]  Time: 0.335s (0.565s,  270.69/s)  Loss:  1.2833 (2.0462)  Acc@1:  75.163 ( 60.046)  Acc@5:  90.850 ( 83.597)\n",
      "INFO:root:Test: [  40/327]  Time: 0.336s (0.509s,  300.88/s)  Loss:  2.8338 (2.1539)  Acc@1:  42.484 ( 59.509)  Acc@5:  71.242 ( 82.767)\n",
      "INFO:root:Test: [  50/327]  Time: 0.333s (0.474s,  322.80/s)  Loss:  1.7075 (2.0857)  Acc@1:  61.438 ( 60.887)  Acc@5:  89.542 ( 83.442)\n",
      "INFO:root:Test: [  60/327]  Time: 0.333s (0.451s,  339.35/s)  Loss:  2.7078 (2.0920)  Acc@1:  39.869 ( 59.552)  Acc@5:  70.588 ( 83.199)\n",
      "INFO:root:Test: [  70/327]  Time: 0.332s (0.434s,  352.20/s)  Loss:  2.1179 (2.1140)  Acc@1:  56.209 ( 58.262)  Acc@5:  81.046 ( 82.979)\n",
      "INFO:root:Test: [  80/327]  Time: 0.333s (0.422s,  362.65/s)  Loss:  1.7692 (2.1296)  Acc@1:  66.013 ( 58.581)  Acc@5:  86.928 ( 83.297)\n",
      "INFO:root:Test: [  90/327]  Time: 0.332s (0.412s,  371.13/s)  Loss:  1.3354 (2.0984)  Acc@1:  71.242 ( 58.637)  Acc@5:  92.810 ( 83.682)\n",
      "INFO:root:Test: [ 100/327]  Time: 0.332s (0.404s,  378.30/s)  Loss:  1.4217 (2.1068)  Acc@1:  70.588 ( 59.069)  Acc@5:  92.157 ( 84.204)\n",
      "INFO:root:Test: [ 110/327]  Time: 0.337s (0.398s,  384.33/s)  Loss:  2.2642 (2.0902)  Acc@1:  62.092 ( 59.618)  Acc@5:  83.660 ( 84.349)\n",
      "INFO:root:Test: [ 120/327]  Time: 0.331s (0.393s,  389.48/s)  Loss:  2.6464 (2.1129)  Acc@1:  62.745 ( 59.558)  Acc@5:  86.928 ( 84.346)\n",
      "INFO:root:Test: [ 130/327]  Time: 0.337s (0.388s,  393.99/s)  Loss: 12.5280 (2.1852)  Acc@1:  48.366 ( 59.602)  Acc@5:  71.242 ( 84.289)\n",
      "INFO:root:Test: [ 140/327]  Time: 0.333s (0.385s,  397.83/s)  Loss:  2.7596 (2.2704)  Acc@1:  54.902 ( 59.018)  Acc@5:  80.392 ( 83.767)\n",
      "INFO:root:Test: [ 150/327]  Time: 0.333s (0.381s,  401.39/s)  Loss:  3.1388 (2.3354)  Acc@1:  32.026 ( 58.334)  Acc@5:  74.510 ( 83.210)\n",
      "INFO:root:Test: [ 160/327]  Time: 0.333s (0.378s,  404.43/s)  Loss:  2.4864 (2.4444)  Acc@1:  50.980 ( 57.772)  Acc@5:  75.817 ( 82.670)\n",
      "INFO:root:Test: [ 170/327]  Time: 0.332s (0.376s,  407.21/s)  Loss:  2.3783 (2.5113)  Acc@1:  57.516 ( 57.291)  Acc@5:  80.392 ( 82.215)\n",
      "INFO:root:Test: [ 180/327]  Time: 0.336s (0.373s,  409.68/s)  Loss:  3.8607 (2.5396)  Acc@1:  59.477 ( 57.159)  Acc@5:  73.203 ( 81.916)\n",
      "INFO:root:Test: [ 190/327]  Time: 0.334s (0.371s,  411.90/s)  Loss:  8.0984 (2.5662)  Acc@1:  54.902 ( 57.369)  Acc@5:  78.431 ( 81.864)\n",
      "INFO:root:Test: [ 200/327]  Time: 0.333s (0.370s,  414.06/s)  Loss:  2.5074 (2.5959)  Acc@1:  66.667 ( 57.181)  Acc@5:  80.392 ( 81.475)\n",
      "INFO:root:Test: [ 210/327]  Time: 0.334s (0.368s,  415.92/s)  Loss:  6.0215 (2.6493)  Acc@1:  43.137 ( 56.634)  Acc@5:  62.092 ( 81.027)\n",
      "INFO:root:Test: [ 220/327]  Time: 0.333s (0.366s,  417.69/s)  Loss:  2.8757 (2.6619)  Acc@1:  39.869 ( 56.502)  Acc@5:  66.667 ( 80.859)\n",
      "INFO:root:Test: [ 230/327]  Time: 0.333s (0.365s,  419.33/s)  Loss:  2.6495 (2.7027)  Acc@1:  47.059 ( 56.322)  Acc@5:  75.163 ( 80.585)\n",
      "INFO:root:Test: [ 240/327]  Time: 0.333s (0.364s,  420.84/s)  Loss:  2.4125 (2.7117)  Acc@1:  60.784 ( 56.179)  Acc@5:  82.353 ( 80.330)\n",
      "INFO:root:Test: [ 250/327]  Time: 0.332s (0.362s,  422.19/s)  Loss:  3.2363 (2.7673)  Acc@1:  52.288 ( 55.858)  Acc@5:  67.974 ( 79.931)\n",
      "INFO:root:Test: [ 260/327]  Time: 0.333s (0.361s,  423.46/s)  Loss:  2.6460 (2.7840)  Acc@1:  53.595 ( 55.718)  Acc@5:  74.510 ( 79.651)\n",
      "INFO:root:Test: [ 270/327]  Time: 0.333s (0.360s,  424.67/s)  Loss:  3.1790 (2.8169)  Acc@1:  45.098 ( 55.654)  Acc@5:  69.281 ( 79.575)\n",
      "INFO:root:Test: [ 280/327]  Time: 0.332s (0.359s,  425.80/s)  Loss:  2.3202 (2.9388)  Acc@1:  53.595 ( 55.395)  Acc@5:  76.471 ( 79.266)\n",
      "INFO:root:Test: [ 290/327]  Time: 0.332s (0.358s,  426.87/s)  Loss:  4.4195 (2.9523)  Acc@1:  59.477 ( 55.230)  Acc@5:  74.510 ( 79.103)\n",
      "INFO:root:Test: [ 300/327]  Time: 0.332s (0.358s,  427.89/s)  Loss:  2.2985 (2.9906)  Acc@1:  59.477 ( 55.104)  Acc@5:  73.856 ( 78.983)\n",
      "INFO:root:Test: [ 310/327]  Time: 0.332s (0.357s,  428.87/s)  Loss:  2.2088 (2.9659)  Acc@1:  53.595 ( 55.133)  Acc@5:  79.085 ( 79.089)\n",
      "INFO:root:Test: [ 320/327]  Time: 0.331s (0.356s,  429.81/s)  Loss:  2.7874 (2.9656)  Acc@1:  67.320 ( 54.977)  Acc@5:  85.621 ( 78.969)\n",
      "INFO:root: * Acc@1 55.226 (44.774) Acc@5 79.106 (20.894)\n",
      "INFO:root:Current checkpoints:\n",
      " ('./output/train/20200705-164051-tf_efficientnet_b1-240/checkpoint-1.pth.tar', 55.226)\n",
      "\n",
      "INFO:root:Train: 2 [   0/8374 (  0%)]  Loss: 27.505713 (27.5057)  Loss_kd:  0.567664 (0.5677)  Loss_ce:  3.288822 (3.2888)  Loss_crd: 29.362669 (29.3627)  Loss_g:  0.159090 (0.1591)  Loss_critic: 13.639689 (13.6397)  Time: 7.528s,   60.97/s  (7.528s,   60.97/s)  LR: 0.0100000  Data: 4.711 (4.711)\n",
      "INFO:root:Train: 2 [  50/8374 (  1%)]  Loss: 22.103754 (23.6594)  Loss_kd:  0.633696 (0.6266)  Loss_ce:  3.285619 (3.5128)  Loss_crd: 22.662289 (24.2291)  Loss_g:  0.054608 (0.1367)  Loss_critic: 16.035156 (13.9370)  Time: 1.229s,  373.41/s  (1.352s,  339.57/s)  LR: 0.0100000  Data: 0.013 (0.105)\n",
      "INFO:root:Train: 2 [ 100/8374 (  1%)]  Loss: 20.063492 (22.2197)  Loss_kd:  0.592201 (0.6114)  Loss_ce:  3.657324 (3.4342)  Loss_crd: 19.534805 (22.5283)  Loss_g:  0.186123 (0.1514)  Loss_critic: 13.431897 (14.2792)  Time: 1.186s,  386.86/s  (1.276s,  359.80/s)  LR: 0.0100000  Data: 0.013 (0.060)\n",
      "INFO:root:Train: 2 [ 150/8374 (  2%)]  Loss: 19.813339 (21.3591)  Loss_kd:  0.611103 (0.6057)  Loss_ce:  3.408967 (3.3827)  Loss_crd: 19.538528 (21.5248)  Loss_g:  0.162446 (0.1509)  Loss_critic: 19.780333 (15.8365)  Time: 1.187s,  386.72/s  (1.248s,  367.76/s)  LR: 0.0100000  Data: 0.013 (0.044)\n",
      "INFO:root:Train: 2 [ 200/8374 (  2%)]  Loss: 18.991791 (20.8464)  Loss_kd:  0.667275 (0.6051)  Loss_ce:  3.306693 (3.3437)  Loss_crd: 18.664331 (20.9221)  Loss_g:  0.086358 (0.1599)  Loss_critic: 31.783180 (17.3411)  Time: 1.190s,  385.67/s  (1.234s,  371.93/s)  LR: 0.0100000  Data: 0.013 (0.036)\n",
      "INFO:root:Train: 2 [ 250/8374 (  3%)]  Loss: 19.306995 (20.4747)  Loss_kd:  0.565998 (0.6035)  Loss_ce:  3.639563 (3.3028)  Loss_crd: 18.769421 (20.5069)  Loss_g:  0.085897 (0.1629)  Loss_critic: 37.419083 (19.9584)  Time: 1.195s,  384.25/s  (1.226s,  374.42/s)  LR: 0.0100000  Data: 0.013 (0.032)\n",
      "INFO:root:Train: 2 [ 300/8374 (  4%)]  Loss: 18.590408 (20.2610)  Loss_kd:  0.564647 (0.6036)  Loss_ce:  3.386471 (3.3537)  Loss_crd: 18.259350 (20.1837)  Loss_g:  0.031811 (0.1567)  Loss_critic: 36.686510 (23.5840)  Time: 1.187s,  386.66/s  (1.221s,  375.92/s)  LR: 0.0100000  Data: 0.013 (0.029)\n",
      "INFO:root:Train: 2 [ 350/8374 (  4%)]  Loss: 18.718712 (20.0819)  Loss_kd:  0.705202 (0.5996)  Loss_ce:  3.581185 (3.4218)  Loss_crd: 17.882160 (19.8839)  Loss_g:  0.126595 (0.1533)  Loss_critic: 39.250252 (25.3108)  Time: 1.256s,  365.48/s  (1.217s,  377.09/s)  LR: 0.0100000  Data: 0.013 (0.026)\n",
      "INFO:root:Train: 2 [ 400/8374 (  5%)]  Loss: 17.445160 (19.8449)  Loss_kd:  0.647189 (0.6080)  Loss_ce:  3.094512 (3.4145)  Loss_crd: 17.080303 (19.5897)  Loss_g:  0.039218 (0.1506)  Loss_critic: 38.863855 (26.4948)  Time: 1.188s,  386.35/s  (1.214s,  378.15/s)  LR: 0.0100000  Data: 0.013 (0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 2 [ 450/8374 (  5%)]  Loss: 17.286768 (19.5994)  Loss_kd:  0.503846 (0.6045)  Loss_ce:  3.207016 (3.4027)  Loss_crd: 16.875326 (19.3086)  Loss_g:  0.075646 (0.1453)  Loss_critic: 29.743185 (27.0368)  Time: 1.189s,  386.11/s  (1.211s,  378.94/s)  LR: 0.0100000  Data: 0.013 (0.023)\n",
      "INFO:root:Train: 2 [ 500/8374 (  6%)]  Loss: 16.855762 (19.3621)  Loss_kd:  0.555714 (0.6018)  Loss_ce:  2.957006 (3.3751)  Loss_crd: 16.609770 (19.0559)  Loss_g:  0.055228 (0.1404)  Loss_critic: 27.206404 (27.2200)  Time: 1.186s,  386.91/s  (1.209s,  379.51/s)  LR: 0.0100000  Data: 0.013 (0.022)\n",
      "INFO:root:Train: 2 [ 550/8374 (  7%)]  Loss: 16.924164 (19.1353)  Loss_kd:  0.572648 (0.5984)  Loss_ce:  3.142022 (3.3423)  Loss_crd: 16.431767 (18.8242)  Loss_g:  0.064080 (0.1353)  Loss_critic: 22.614855 (27.1155)  Time: 1.200s,  382.53/s  (1.208s,  379.95/s)  LR: 0.0100000  Data: 0.013 (0.022)\n",
      "INFO:root:Train: 2 [ 600/8374 (  7%)]  Loss: 16.991049 (18.9335)  Loss_kd:  0.561135 (0.5959)  Loss_ce:  2.995216 (3.3137)  Loss_crd: 16.611826 (18.6168)  Loss_g:  0.145235 (0.1304)  Loss_critic: 16.453480 (26.6977)  Time: 1.199s,  382.97/s  (1.207s,  380.24/s)  LR: 0.0100000  Data: 0.013 (0.021)\n",
      "INFO:root:Train: 2 [ 650/8374 (  8%)]  Loss: 16.500601 (18.7551)  Loss_kd:  0.548553 (0.5926)  Loss_ce:  2.935145 (3.2892)  Loss_crd: 16.146221 (18.4301)  Loss_g:  0.099925 (0.1292)  Loss_critic: 20.368083 (25.9317)  Time: 1.200s,  382.47/s  (1.207s,  380.42/s)  LR: 0.0100000  Data: 0.013 (0.020)\n",
      "INFO:root:Train: 2 [ 700/8374 (  8%)]  Loss: 16.645994 (18.6033)  Loss_kd:  0.598704 (0.5902)  Loss_ce:  3.013994 (3.2715)  Loss_crd: 15.988945 (18.2612)  Loss_g:  0.242140 (0.1326)  Loss_critic: 17.026800 (25.1540)  Time: 1.191s,  385.40/s  (1.206s,  380.68/s)  LR: 0.0100000  Data: 0.013 (0.020)\n",
      "INFO:root:Train: 2 [ 750/8374 (  9%)]  Loss: 16.968761 (18.4887)  Loss_kd:  0.564581 (0.5905)  Loss_ce:  3.328650 (3.2663)  Loss_crd: 16.106623 (18.1195)  Loss_g:  0.190232 (0.1362)  Loss_critic: 16.495781 (24.5565)  Time: 1.191s,  385.45/s  (1.205s,  380.98/s)  LR: 0.0100000  Data: 0.013 (0.019)\n",
      "INFO:root:Train: 2 [ 800/8374 ( 10%)]  Loss: 17.657316 (18.4821)  Loss_kd:  0.666759 (0.5936)  Loss_ce:  3.637305 (3.2736)  Loss_crd: 16.691565 (18.0301)  Loss_g:  0.000001 (0.1908)  Loss_critic: 98.157971 (26.8721)  Time: 1.192s,  384.96/s  (1.204s,  381.20/s)  LR: 0.0100000  Data: 0.013 (0.019)\n",
      "INFO:root:Train: 2 [ 850/8374 ( 10%)]  Loss: 16.141455 (18.3673)  Loss_kd:  0.552724 (0.5934)  Loss_ce:  3.223012 (3.2682)  Loss_crd: 15.453471 (17.9068)  Loss_g:  0.002941 (0.1802)  Loss_critic: 44.792594 (29.3946)  Time: 1.199s,  382.73/s  (1.204s,  381.37/s)  LR: 0.0100000  Data: 0.013 (0.019)\n",
      "INFO:root:Train: 2 [ 900/8374 ( 11%)]  Loss: 17.062654 (18.2627)  Loss_kd:  0.549082 (0.5903)  Loss_ce:  3.289182 (3.2561)  Loss_crd: 16.513603 (17.7976)  Loss_g:  0.013509 (0.1783)  Loss_critic: 25.261255 (28.9961)  Time: 1.217s,  377.13/s  (1.203s,  381.39/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 2 [ 950/8374 ( 11%)]  Loss: 16.386576 (18.1790)  Loss_kd:  0.580160 (0.5899)  Loss_ce:  2.872458 (3.2546)  Loss_crd: 16.094666 (17.6986)  Loss_g:  0.058225 (0.1756)  Loss_critic: 40.106909 (28.8240)  Time: 1.189s,  386.18/s  (1.203s,  381.51/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 2 [1000/8374 ( 12%)]  Loss: 16.649647 (18.0869)  Loss_kd:  0.560346 (0.5889)  Loss_ce:  3.026104 (3.2422)  Loss_crd: 15.796027 (17.6024)  Loss_g:  0.426375 (0.1739)  Loss_critic: 27.635596 (28.9187)  Time: 1.189s,  386.04/s  (1.203s,  381.70/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "INFO:root:Train: 2 [1050/8374 ( 13%)]  Loss: 16.346863 (18.0007)  Loss_kd:  0.554968 (0.5864)  Loss_ce:  3.206101 (3.2345)  Loss_crd: 15.659498 (17.5114)  Loss_g:  0.058194 (0.1707)  Loss_critic: 29.562889 (28.9388)  Time: 1.189s,  385.94/s  (1.202s,  381.76/s)  LR: 0.0100000  Data: 0.013 (0.018)\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'sRGB' 41 1\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'gAMA' 54 4\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'cHRM' 70 32\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'pHYs' 114 9\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 135 65401\n",
      "INFO:root:Train: 2 [1100/8374 ( 13%)]  Loss: 16.108631 (17.9135)  Loss_kd:  0.508075 (0.5828)  Loss_ce:  3.197077 (3.2254)  Loss_crd: 15.402793 (17.4224)  Loss_g:  0.081243 (0.1674)  Loss_critic: 28.634027 (28.8842)  Time: 1.190s,  385.64/s  (1.202s,  381.85/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 2 [1150/8374 ( 14%)]  Loss: 15.525101 (17.8235)  Loss_kd:  0.548889 (0.5807)  Loss_ce:  2.803769 (3.2114)  Loss_crd: 15.151455 (17.3346)  Loss_g:  0.051279 (0.1637)  Loss_critic: 28.114786 (28.8144)  Time: 1.190s,  385.74/s  (1.202s,  381.88/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 2 [1200/8374 ( 14%)]  Loss: 15.787455 (17.7362)  Loss_kd:  0.521591 (0.5788)  Loss_ce:  2.976403 (3.1962)  Loss_crd: 15.235604 (17.2506)  Loss_g:  0.100978 (0.1607)  Loss_critic: 20.060467 (28.5900)  Time: 1.189s,  386.07/s  (1.202s,  381.98/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 2 [1250/8374 ( 15%)]  Loss: 15.811054 (17.6570)  Loss_kd:  0.521979 (0.5767)  Loss_ce:  2.842720 (3.1847)  Loss_crd: 15.460693 (17.1732)  Loss_g:  0.077801 (0.1569)  Loss_critic: 22.429211 (28.3174)  Time: 1.190s,  385.75/s  (1.201s,  382.05/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 2 [1300/8374 ( 16%)]  Loss: 17.662231 (17.6033)  Loss_kd:  0.680301 (0.5764)  Loss_ce:  3.947665 (3.1862)  Loss_crd: 16.144495 (17.1068)  Loss_g:  0.118669 (0.1552)  Loss_critic: 21.527958 (27.9747)  Time: 1.190s,  385.88/s  (1.201s,  382.05/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 2 [1350/8374 ( 16%)]  Loss: 15.898901 (17.5601)  Loss_kd:  0.550313 (0.5770)  Loss_ce:  2.850840 (3.1908)  Loss_crd: 15.447609 (17.0479)  Loss_g:  0.139660 (0.1539)  Loss_critic: 17.635694 (27.6227)  Time: 1.208s,  380.11/s  (1.201s,  382.13/s)  LR: 0.0100000  Data: 0.013 (0.017)\n",
      "INFO:root:Train: 2 [1400/8374 ( 17%)]  Loss: 16.100475 (17.5219)  Loss_kd:  0.578321 (0.5777)  Loss_ce:  2.917227 (3.1935)  Loss_crd: 15.627695 (16.9962)  Loss_g:  0.102770 (0.1537)  Loss_critic: 18.309072 (27.2124)  Time: 1.187s,  386.75/s  (1.201s,  382.15/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1450/8374 ( 17%)]  Loss: 24.020727 (17.5238)  Loss_kd:  0.820892 (0.5794)  Loss_ce:  7.156880 (3.2124)  Loss_crd: 19.144907 (16.9682)  Loss_g:  0.727031 (0.1575)  Loss_critic: 21.156660 (27.1025)  Time: 1.189s,  386.07/s  (1.201s,  382.22/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1500/8374 ( 18%)]  Loss: 21.525381 (17.7019)  Loss_kd:  0.700093 (0.5845)  Loss_ce:  6.992538 (3.3386)  Loss_crd: 17.205990 (17.0280)  Loss_g:  0.067956 (0.1563)  Loss_critic: 27.648193 (28.1711)  Time: 1.190s,  385.70/s  (1.201s,  382.32/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1550/8374 ( 19%)]  Loss: 21.698053 (17.8490)  Loss_kd:  0.705275 (0.5884)  Loss_ce:  6.998881 (3.4558)  Loss_crd: 17.492357 (17.0628)  Loss_g:  0.000009 (0.1545)  Loss_critic: 110.143126 (28.8310)  Time: 1.188s,  386.30/s  (1.200s,  382.41/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1600/8374 ( 19%)]  Loss: 21.368881 (17.9619)  Loss_kd:  0.643758 (0.5910)  Loss_ce:  6.930378 (3.5649)  Loss_crd: 17.243429 (17.0704)  Loss_g:  0.000000 (0.1498)  Loss_critic: 106.476529 (31.3228)  Time: 1.190s,  385.78/s  (1.200s,  382.47/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1650/8374 ( 20%)]  Loss: 21.341665 (18.0614)  Loss_kd:  0.677887 (0.5926)  Loss_ce:  6.926013 (3.6663)  Loss_crd: 17.172207 (17.0716)  Loss_g:  0.000000 (0.1452)  Loss_critic: 103.278040 (33.5216)  Time: 1.192s,  385.00/s  (1.200s,  382.51/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1700/8374 ( 20%)]  Loss: 21.148294 (18.1544)  Loss_kd:  0.639334 (0.5940)  Loss_ce:  6.917402 (3.7614)  Loss_crd: 16.989384 (17.0725)  Loss_g:  0.000052 (0.1410)  Loss_critic: 99.290430 (35.5063)  Time: 1.191s,  385.36/s  (1.200s,  382.56/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1750/8374 ( 21%)]  Loss: 21.148251 (18.2404)  Loss_kd:  0.681745 (0.5953)  Loss_ce:  6.899733 (3.8507)  Loss_crd: 16.958445 (17.0718)  Loss_g:  0.000017 (0.1369)  Loss_critic: 94.205371 (37.3185)  Time: 1.192s,  385.16/s  (1.200s,  382.54/s)  LR: 0.0100000  Data: 0.013 (0.016)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 2 [1800/8374 ( 21%)]  Loss: 21.259764 (18.3218)  Loss_kd:  0.658104 (0.5968)  Loss_ce:  6.900223 (3.9349)  Loss_crd: 17.126774 (17.0710)  Loss_g:  0.000017 (0.1333)  Loss_critic: 46.472806 (38.0886)  Time: 1.186s,  386.95/s  (1.200s,  382.55/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1850/8374 ( 22%)]  Loss: 21.015694 (18.4001)  Loss_kd:  0.641757 (0.5986)  Loss_ce:  6.890121 (4.0149)  Loss_crd: 16.851742 (17.0702)  Loss_g:  0.002423 (0.1305)  Loss_critic: 28.787366 (37.8708)  Time: 1.194s,  384.36/s  (1.200s,  382.61/s)  LR: 0.0100000  Data: 0.013 (0.016)\n",
      "INFO:root:Train: 2 [1900/8374 ( 23%)]  Loss: 21.726221 (18.4734)  Loss_kd:  0.673366 (0.6000)  Loss_ce:  6.893696 (4.0906)  Loss_crd: 17.586487 (17.0691)  Loss_g:  0.089969 (0.1276)  Loss_critic: 17.927829 (37.5389)  Time: 1.197s,  383.45/s  (1.200s,  382.63/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [1950/8374 ( 23%)]  Loss: 21.089409 (18.5405)  Loss_kd:  0.681916 (0.6014)  Loss_ce:  6.839101 (4.1620)  Loss_crd: 16.960190 (17.0659)  Loss_g:  0.000240 (0.1244)  Loss_critic: 70.646831 (37.4114)  Time: 1.192s,  384.99/s  (1.199s,  382.66/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2000/8374 ( 24%)]  Loss: 20.961296 (18.6044)  Loss_kd:  0.635047 (0.6028)  Loss_ce:  6.846612 (4.2294)  Loss_crd: 16.845398 (17.0633)  Loss_g:  0.003320 (0.1216)  Loss_critic: 29.690859 (38.0205)  Time: 1.189s,  386.10/s  (1.199s,  382.71/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2050/8374 ( 24%)]  Loss: 20.899225 (18.6634)  Loss_kd:  0.641847 (0.6038)  Loss_ce:  6.841881 (4.2930)  Loss_crd: 16.744129 (17.0597)  Loss_g:  0.020194 (0.1189)  Loss_critic: 24.143477 (37.8367)  Time: 1.195s,  384.04/s  (1.199s,  382.74/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2100/8374 ( 25%)]  Loss: 20.965347 (18.7191)  Loss_kd:  0.723696 (0.6052)  Loss_ce:  6.852537 (4.3534)  Loss_crd: 16.735529 (17.0554)  Loss_g:  0.000691 (0.1162)  Loss_critic: 23.303646 (37.5577)  Time: 1.195s,  384.08/s  (1.199s,  382.74/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2150/8374 ( 26%)]  Loss: 20.963026 (18.7712)  Loss_kd:  0.674821 (0.6068)  Loss_ce:  6.820045 (4.4103)  Loss_crd: 16.833996 (17.0506)  Loss_g:  0.000963 (0.1137)  Loss_critic: 43.929873 (37.3375)  Time: 1.199s,  382.75/s  (1.199s,  382.79/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2200/8374 ( 26%)]  Loss: 20.792122 (18.8243)  Loss_kd:  0.657180 (0.6078)  Loss_ce:  6.840232 (4.4654)  Loss_crd: 16.618387 (17.0497)  Loss_g:  0.000000 (0.1113)  Loss_critic: 128.295376 (39.1480)  Time: 1.192s,  385.19/s  (1.199s,  382.83/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2250/8374 ( 27%)]  Loss: 20.575518 (18.8702)  Loss_kd:  0.638679 (0.6087)  Loss_ce:  6.771015 (4.5170)  Loss_crd: 16.457275 (17.0445)  Loss_g:  0.000003 (0.1088)  Loss_critic: 117.355118 (41.0875)  Time: 1.190s,  385.60/s  (1.199s,  382.82/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2300/8374 ( 27%)]  Loss: 20.902626 (18.9123)  Loss_kd:  0.658390 (0.6098)  Loss_ce:  6.801585 (4.5659)  Loss_crd: 16.803312 (17.0376)  Loss_g:  0.000000 (0.1065)  Loss_critic: 121.078320 (42.8685)  Time: 1.202s,  381.96/s  (1.199s,  382.84/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2350/8374 ( 28%)]  Loss: 20.626146 (18.9516)  Loss_kd:  0.628670 (0.6108)  Loss_ce:  6.728014 (4.6123)  Loss_crd: 16.586815 (17.0304)  Loss_g:  0.000009 (0.1042)  Loss_critic: 111.964945 (44.4647)  Time: 1.186s,  386.87/s  (1.199s,  382.83/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2400/8374 ( 29%)]  Loss: 20.734198 (18.9877)  Loss_kd:  0.633275 (0.6118)  Loss_ce:  6.714352 (4.6562)  Loss_crd: 16.733213 (17.0221)  Loss_g:  0.000000 (0.1020)  Loss_critic: 112.108417 (45.8903)  Time: 1.189s,  386.10/s  (1.199s,  382.87/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2450/8374 ( 29%)]  Loss: 20.929310 (19.0224)  Loss_kd:  0.674588 (0.6128)  Loss_ce:  6.754310 (4.6983)  Loss_crd: 16.875509 (17.0141)  Loss_g:  0.000004 (0.1000)  Loss_critic: 101.916010 (47.1432)  Time: 1.196s,  383.71/s  (1.199s,  382.86/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2500/8374 ( 30%)]  Loss: 20.572460 (19.0557)  Loss_kd:  0.659876 (0.6137)  Loss_ce:  6.705298 (4.7384)  Loss_crd: 16.509096 (17.0071)  Loss_g:  0.000007 (0.0980)  Loss_critic: 95.078971 (48.1739)  Time: 1.189s,  385.98/s  (1.199s,  382.90/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2550/8374 ( 30%)]  Loss: 20.657614 (19.0859)  Loss_kd:  0.672781 (0.6146)  Loss_ce:  6.665878 (4.7764)  Loss_crd: 16.648582 (16.9984)  Loss_g:  0.000089 (0.0961)  Loss_critic: 62.736563 (48.8239)  Time: 1.188s,  386.30/s  (1.199s,  382.92/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2600/8374 ( 31%)]  Loss: 20.746840 (19.1145)  Loss_kd:  0.715881 (0.6155)  Loss_ce:  6.659852 (4.8128)  Loss_crd: 16.711174 (16.9898)  Loss_g:  0.002169 (0.0943)  Loss_critic:  9.894772 (48.1637)  Time: 1.187s,  386.64/s  (1.199s,  382.90/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2650/8374 ( 32%)]  Loss: 20.627811 (19.1430)  Loss_kd:  0.697304 (0.6169)  Loss_ce:  6.647736 (4.8477)  Loss_crd: 16.587482 (16.9822)  Loss_g:  0.012787 (0.0927)  Loss_critic: 10.795001 (47.4498)  Time: 1.193s,  384.73/s  (1.199s,  382.89/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2700/8374 ( 32%)]  Loss: 20.533659 (19.1704)  Loss_kd:  0.682244 (0.6180)  Loss_ce:  6.662906 (4.8812)  Loss_crd: 16.480696 (16.9751)  Loss_g:  0.003952 (0.0911)  Loss_critic: 16.593650 (46.8381)  Time: 1.193s,  384.84/s  (1.199s,  382.87/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2750/8374 ( 33%)]  Loss: 20.350515 (19.1946)  Loss_kd:  0.654635 (0.6188)  Loss_ce:  6.658419 (4.9134)  Loss_crd: 16.288805 (16.9660)  Loss_g:  0.006416 (0.0896)  Loss_critic: 10.340521 (46.2211)  Time: 1.201s,  382.10/s  (1.199s,  382.89/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2800/8374 ( 33%)]  Loss: 20.587807 (19.2187)  Loss_kd:  0.691233 (0.6200)  Loss_ce:  6.652009 (4.9438)  Loss_crd: 16.547354 (16.9584)  Loss_g:  0.006680 (0.0882)  Loss_critic: 12.690873 (45.5886)  Time: 1.194s,  384.51/s  (1.199s,  382.92/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2850/8374 ( 34%)]  Loss: 20.464954 (19.2413)  Loss_kd:  0.693754 (0.6210)  Loss_ce:  6.600911 (4.9735)  Loss_crd: 16.437593 (16.9500)  Loss_g:  0.020215 (0.0868)  Loss_critic:  9.774439 (45.0047)  Time: 1.191s,  385.55/s  (1.199s,  382.96/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2900/8374 ( 35%)]  Loss: 20.386787 (19.2629)  Loss_kd:  0.708798 (0.6222)  Loss_ce:  6.623791 (5.0016)  Loss_crd: 16.277243 (16.9419)  Loss_g:  0.032403 (0.0856)  Loss_critic: 10.175182 (44.3947)  Time: 1.186s,  386.95/s  (1.199s,  382.97/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [2950/8374 ( 35%)]  Loss: 20.324987 (19.2842)  Loss_kd:  0.645575 (0.6231)  Loss_ce:  6.630423 (5.0290)  Loss_crd: 16.277390 (16.9344)  Loss_g:  0.027078 (0.0845)  Loss_critic: 12.441392 (43.8827)  Time: 1.243s,  369.40/s  (1.199s,  382.96/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [3000/8374 ( 36%)]  Loss: 20.677692 (19.3041)  Loss_kd:  0.709761 (0.6245)  Loss_ce:  6.560279 (5.0551)  Loss_crd: 16.625879 (16.9263)  Loss_g:  0.106949 (0.0835)  Loss_critic: 10.437664 (43.3317)  Time: 1.189s,  386.18/s  (1.199s,  382.96/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [3050/8374 ( 36%)]  Loss: 20.309080 (19.3308)  Loss_kd:  0.660406 (0.6256)  Loss_ce:  6.643907 (5.0816)  Loss_crd: 16.255959 (16.9226)  Loss_g:  0.000000 (0.0855)  Loss_critic: 109.524846 (43.8432)  Time: 1.206s,  380.45/s  (1.199s,  382.95/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [3100/8374 ( 37%)]  Loss: 20.333494 (19.3482)  Loss_kd:  0.725326 (0.6265)  Loss_ce:  6.643539 (5.1062)  Loss_crd: 16.205786 (16.9143)  Loss_g:  0.000000 (0.0841)  Loss_critic: 104.673724 (44.8547)  Time: 1.209s,  379.64/s  (1.199s,  382.93/s)  LR: 0.0100000  Data: 0.013 (0.015)\n",
      "INFO:root:Train: 2 [3150/8374 ( 38%)]  Loss: 20.132156 (19.3640)  Loss_kd:  0.675415 (0.6274)  Loss_ce:  6.555582 (5.1295)  Loss_crd: 16.126450 (16.9055)  Loss_g:  0.000000 (0.0828)  Loss_critic: 100.809234 (45.7617)  Time: 1.209s,  379.57/s  (1.199s,  382.95/s)  LR: 0.0100000  Data: 0.013 (0.015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 2 [3200/8374 ( 38%)]  Loss: 20.366098 (19.3787)  Loss_kd:  0.693317 (0.6284)  Loss_ce:  6.559687 (5.1513)  Loss_crd: 16.391369 (16.8969)  Loss_g:  0.000000 (0.0815)  Loss_critic: 100.397798 (46.6214)  Time: 1.199s,  382.67/s  (1.199s,  382.97/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3250/8374 ( 39%)]  Loss: 20.286169 (19.3919)  Loss_kd:  0.663503 (0.6292)  Loss_ce:  6.527094 (5.1722)  Loss_crd: 16.369465 (16.8878)  Loss_g:  0.000000 (0.0802)  Loss_critic: 100.429291 (47.4491)  Time: 1.192s,  384.91/s  (1.199s,  382.98/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3300/8374 ( 39%)]  Loss: 20.144520 (19.4043)  Loss_kd:  0.679138 (0.6300)  Loss_ce:  6.485237 (5.1924)  Loss_crd: 16.225180 (16.8785)  Loss_g:  0.000000 (0.0790)  Loss_critic: 100.287060 (48.2506)  Time: 1.190s,  385.84/s  (1.198s,  382.98/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3350/8374 ( 40%)]  Loss: 19.998583 (19.4157)  Loss_kd:  0.663861 (0.6309)  Loss_ce:  6.416669 (5.2118)  Loss_crd: 16.147566 (16.8690)  Loss_g:  0.000000 (0.0778)  Loss_critic: 100.145003 (49.0265)  Time: 1.191s,  385.34/s  (1.199s,  382.98/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3400/8374 ( 41%)]  Loss: 20.164236 (19.4263)  Loss_kd:  0.684442 (0.6317)  Loss_ce:  6.480444 (5.2302)  Loss_crd: 16.249187 (16.8597)  Loss_g:  0.000000 (0.0767)  Loss_critic: 100.292571 (49.7794)  Time: 1.189s,  386.10/s  (1.199s,  382.96/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3450/8374 ( 41%)]  Loss: 19.847288 (19.4364)  Loss_kd:  0.659207 (0.6325)  Loss_ce:  6.394683 (5.2481)  Loss_crd: 15.991748 (16.8503)  Loss_g:  0.000000 (0.0756)  Loss_critic: 100.069443 (50.5099)  Time: 1.188s,  386.34/s  (1.199s,  382.96/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3500/8374 ( 42%)]  Loss: 19.859921 (19.4454)  Loss_kd:  0.683927 (0.6334)  Loss_ce:  6.461046 (5.2648)  Loss_crd: 15.893682 (16.8410)  Loss_g:  0.000000 (0.0745)  Loss_critic: 100.106310 (51.2187)  Time: 1.193s,  384.65/s  (1.198s,  382.98/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3550/8374 ( 42%)]  Loss: 19.928703 (19.4539)  Loss_kd:  0.704670 (0.6342)  Loss_ce:  6.358388 (5.2813)  Loss_crd: 16.082056 (16.8313)  Loss_g:  0.000000 (0.0734)  Loss_critic: 100.152002 (51.9076)  Time: 1.193s,  384.64/s  (1.198s,  382.99/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3600/8374 ( 43%)]  Loss: 20.221989 (19.4617)  Loss_kd:  0.683135 (0.6349)  Loss_ce:  6.305391 (5.2970)  Loss_crd: 16.541828 (16.8217)  Loss_g:  0.000000 (0.0724)  Loss_critic: 99.589233 (52.5769)  Time: 1.187s,  386.77/s  (1.198s,  383.01/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3650/8374 ( 44%)]  Loss: 20.100782 (19.4690)  Loss_kd:  0.695591 (0.6357)  Loss_ce:  6.334281 (5.3118)  Loss_crd: 16.338636 (16.8126)  Loss_g:  0.000000 (0.0714)  Loss_critic: 100.065319 (53.2275)  Time: 1.200s,  382.66/s  (1.198s,  383.04/s)  LR: 0.0100000  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 2 [3700/8374 ( 44%)]  Loss: 20.034424 (19.4758)  Loss_kd:  0.707452 (0.6366)  Loss_ce:  6.374981 (5.3263)  Loss_crd: 16.189989 (16.8030)  Loss_g:  0.000000 (0.0705)  Loss_critic: 100.089191 (53.8597)  Time: 1.189s,  386.03/s  (1.198s,  383.06/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3750/8374 ( 45%)]  Loss: 19.872856 (19.4812)  Loss_kd:  0.708845 (0.6373)  Loss_ce:  6.275959 (5.3402)  Loss_crd: 16.110065 (16.7927)  Loss_g:  0.000000 (0.0695)  Loss_critic: 100.223140 (54.4734)  Time: 1.193s,  384.70/s  (1.198s,  383.08/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3800/8374 ( 45%)]  Loss: 20.048517 (19.4872)  Loss_kd:  0.719908 (0.6381)  Loss_ce:  6.445891 (5.3537)  Loss_crd: 16.103397 (16.7834)  Loss_g:  0.000000 (0.0686)  Loss_critic: 99.509129 (55.0650)  Time: 1.192s,  384.99/s  (1.198s,  383.04/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3850/8374 ( 46%)]  Loss: 19.933117 (19.4921)  Loss_kd:  0.667844 (0.6389)  Loss_ce:  6.290794 (5.3664)  Loss_crd: 16.218098 (16.7740)  Loss_g:  0.000000 (0.0677)  Loss_critic: 100.061004 (55.6415)  Time: 1.189s,  385.98/s  (1.198s,  383.01/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3900/8374 ( 47%)]  Loss: 19.778791 (19.4968)  Loss_kd:  0.713856 (0.6396)  Loss_ce:  6.274108 (5.3787)  Loss_crd: 15.988535 (16.7646)  Loss_g:  0.000000 (0.0669)  Loss_critic: 99.514065 (56.2021)  Time: 1.196s,  383.94/s  (1.198s,  383.01/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [3950/8374 ( 47%)]  Loss: 19.922592 (19.5004)  Loss_kd:  0.682620 (0.6404)  Loss_ce:  6.371979 (5.3904)  Loss_crd: 16.084991 (16.7545)  Loss_g:  0.000000 (0.0660)  Loss_critic: 98.384713 (56.7426)  Time: 1.191s,  385.52/s  (1.198s,  383.03/s)  LR: 0.0100000  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 2 [4000/8374 ( 48%)]  Loss: 19.657978 (19.5035)  Loss_kd:  0.729117 (0.6411)  Loss_ce:  6.292856 (5.4015)  Loss_crd: 15.795006 (16.7446)  Loss_g:  0.000000 (0.0652)  Loss_critic: 94.443574 (57.2569)  Time: 1.190s,  385.63/s  (1.198s,  383.02/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4050/8374 ( 48%)]  Loss: 19.957640 (19.5062)  Loss_kd:  0.715456 (0.6419)  Loss_ce:  6.385084 (5.4122)  Loss_crd: 16.053715 (16.7347)  Loss_g:  0.014127 (0.0644)  Loss_critic: 93.175141 (57.7331)  Time: 1.189s,  386.00/s  (1.198s,  383.04/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4100/8374 ( 49%)]  Loss: 19.499069 (19.5083)  Loss_kd:  0.713018 (0.6426)  Loss_ce:  6.273821 (5.4225)  Loss_crd: 15.640118 (16.7246)  Loss_g:  0.000135 (0.0636)  Loss_critic: 86.342289 (58.1469)  Time: 1.200s,  382.61/s  (1.198s,  383.03/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4150/8374 ( 50%)]  Loss: 19.628069 (19.5107)  Loss_kd:  0.717359 (0.6434)  Loss_ce:  6.214330 (5.4326)  Loss_crd: 15.864788 (16.7149)  Loss_g:  0.004549 (0.0629)  Loss_critic: 65.447341 (58.3988)  Time: 1.228s,  373.76/s  (1.198s,  383.03/s)  LR: 0.0100000  Data: 0.014 (0.014)\n",
      "INFO:root:Train: 2 [4200/8374 ( 50%)]  Loss: 19.832560 (19.5126)  Loss_kd:  0.709401 (0.6441)  Loss_ce:  6.238983 (5.4422)  Loss_crd: 16.084721 (16.7051)  Loss_g:  0.016398 (0.0622)  Loss_critic:  7.755531 (58.1065)  Time: 1.189s,  386.04/s  (1.198s,  383.05/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4250/8374 ( 51%)]  Loss: 19.984703 (19.5158)  Loss_kd:  0.727297 (0.6450)  Loss_ce:  6.316017 (5.4520)  Loss_crd: 16.139385 (16.6961)  Loss_g:  0.029881 (0.0619)  Loss_critic: 13.231849 (57.5427)  Time: 1.191s,  385.47/s  (1.198s,  383.05/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4300/8374 ( 51%)]  Loss: 19.864426 (19.5195)  Loss_kd:  0.702108 (0.6459)  Loss_ce:  6.298433 (5.4620)  Loss_crd: 16.056656 (16.6875)  Loss_g:  0.018561 (0.0616)  Loss_critic: 13.400483 (57.0323)  Time: 1.189s,  386.19/s  (1.198s,  383.07/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4350/8374 ( 52%)]  Loss: 19.780708 (19.5232)  Loss_kd:  0.689464 (0.6467)  Loss_ce:  6.325797 (5.4716)  Loss_crd: 15.932806 (16.6795)  Loss_g:  0.019201 (0.0613)  Loss_critic: 20.553006 (56.5803)  Time: 1.242s,  369.54/s  (1.198s,  383.08/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4400/8374 ( 53%)]  Loss: 19.853430 (19.5269)  Loss_kd:  0.684979 (0.6470)  Loss_ce:  6.418357 (5.4819)  Loss_crd: 15.916997 (16.6711)  Loss_g:  0.016497 (0.0612)  Loss_critic: 18.177392 (56.1522)  Time: 1.189s,  386.10/s  (1.198s,  383.11/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4450/8374 ( 53%)]  Loss: 19.507614 (19.5286)  Loss_kd:  0.722924 (0.6478)  Loss_ce:  6.237358 (5.4906)  Loss_crd: 15.630114 (16.6616)  Loss_g:  0.043241 (0.0608)  Loss_critic: 15.306855 (55.6934)  Time: 1.189s,  385.95/s  (1.198s,  383.12/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4500/8374 ( 54%)]  Loss: 19.710953 (19.5301)  Loss_kd:  0.669811 (0.6484)  Loss_ce:  6.452135 (5.4994)  Loss_crd: 15.675445 (16.6521)  Loss_g:  0.048651 (0.0605)  Loss_critic: 24.358734 (55.3052)  Time: 1.195s,  384.25/s  (1.198s,  383.14/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4550/8374 ( 54%)]  Loss: 19.800537 (19.5320)  Loss_kd:  0.702047 (0.6486)  Loss_ce:  6.352657 (5.5086)  Loss_crd: 15.900860 (16.6432)  Loss_g:  0.025144 (0.0602)  Loss_critic: 18.918592 (54.9431)  Time: 1.192s,  384.99/s  (1.198s,  383.14/s)  LR: 0.0100000  Data: 0.013 (0.014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train: 2 [4600/8374 ( 55%)]  Loss: 19.905861 (19.5321)  Loss_kd:  0.775665 (0.6495)  Loss_ce:  6.330792 (5.5161)  Loss_crd: 15.942432 (16.6333)  Loss_g:  0.045457 (0.0598)  Loss_critic: 16.565355 (54.5311)  Time: 1.188s,  386.25/s  (1.198s,  383.16/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4650/8374 ( 56%)]  Loss: 19.874672 (19.5335)  Loss_kd:  0.687891 (0.6504)  Loss_ce:  6.417292 (5.5241)  Loss_crd: 15.923868 (16.6242)  Loss_g:  0.030395 (0.0596)  Loss_critic: 24.177089 (54.1769)  Time: 1.190s,  385.87/s  (1.198s,  383.17/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "/home/cutz/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "INFO:root:Train: 2 [4700/8374 ( 56%)]  Loss: 19.566723 (19.5353)  Loss_kd:  0.699164 (0.6505)  Loss_ce:  6.380479 (5.5332)  Loss_crd: 15.566801 (16.6151)  Loss_g:  0.033640 (0.0594)  Loss_critic: 17.957432 (53.8285)  Time: 1.193s,  384.59/s  (1.198s,  383.17/s)  LR: 0.0100000  Data: 0.016 (0.014)\n",
      "INFO:root:Train: 2 [4750/8374 ( 57%)]  Loss: 19.530300 (19.5370)  Loss_kd:  0.695660 (0.6513)  Loss_ce:  6.291886 (5.5414)  Loss_crd: 15.629954 (16.6064)  Loss_g:  0.038789 (0.0592)  Loss_critic: 22.773217 (53.4540)  Time: 1.190s,  385.72/s  (1.198s,  383.18/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4800/8374 ( 57%)]  Loss: 19.431936 (19.5384)  Loss_kd:  0.648694 (0.6515)  Loss_ce:  6.274277 (5.5501)  Loss_crd: 15.588396 (16.5974)  Loss_g:  0.038248 (0.0589)  Loss_critic: 21.175227 (53.1531)  Time: 1.190s,  385.56/s  (1.198s,  383.18/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4850/8374 ( 58%)]  Loss: 19.645126 (19.5382)  Loss_kd:  0.770842 (0.6522)  Loss_ce:  6.261770 (5.5569)  Loss_crd: 15.733601 (16.5881)  Loss_g:  0.025635 (0.0587)  Loss_critic: 16.469965 (52.7836)  Time: 1.192s,  384.91/s  (1.198s,  383.19/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4900/8374 ( 59%)]  Loss: 19.701691 (19.5385)  Loss_kd:  0.650383 (0.6529)  Loss_ce:  6.343470 (5.5638)  Loss_crd: 15.831189 (16.5789)  Loss_g:  0.042884 (0.0586)  Loss_critic: 25.116737 (52.4587)  Time: 1.190s,  385.79/s  (1.198s,  383.20/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [4950/8374 ( 59%)]  Loss: 19.834822 (19.5413)  Loss_kd:  0.767432 (0.6530)  Loss_ce:  6.260978 (5.5728)  Loss_crd: 15.977610 (16.5708)  Loss_g:  0.024325 (0.0590)  Loss_critic: 18.362818 (52.1423)  Time: 1.194s,  384.55/s  (1.198s,  383.21/s)  LR: 0.0100000  Data: 0.013 (0.014)\n",
      "INFO:root:Train: 2 [5000/8374 ( 60%)]  Loss: 20.910515 (19.5526)  Loss_kd:  0.666999 (0.6537)  Loss_ce:  6.734408 (5.5816)  Loss_crd: 16.884230 (16.5702)  Loss_g:  0.001725 (0.0611)  Loss_critic: 39.596437 (52.1010)  Time: 1.190s,  385.71/s  (1.198s,  383.23/s)  LR: 0.0100000  Data: 0.013 (0.014)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    if args.distributed:\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "    train_metrics = train_epoch(\n",
    "        epoch, model_g, model_raw, model_ns, model_critic, train_loader, optimizer, optimizer_critic, \n",
    "        train_loss_fn, crd_loss_fn, critic_loss_fn, args,\n",
    "        lr_scheduler=lr_scheduler, lr_scheduler_critic=lr_scheduler_critic, \n",
    "        saver=saver, output_dir=output_dir,\n",
    "        use_amp=use_amp, model_ema=model_ema)\n",
    "\n",
    "    eval_metrics = val_epoch(model_raw, model_g, val_loader, validate_loss_fn, args)\n",
    "\n",
    "    if model_ema is not None and not args.model_ema_force_cpu:\n",
    "        if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "            distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        # step LR for next epoch\n",
    "        lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "        \n",
    "    update_summary( \n",
    "        epoch, train_metrics, eval_metrics, os.path.join(output_dir, 'summary.csv'),\n",
    "        write_header=best_metric is None)\n",
    "\n",
    "    if saver is not None:\n",
    "    # save proper checkpoint with eval metric\n",
    "        save_metric = eval_metrics[eval_metric]\n",
    "        best_metric, best_epoch = saver.save_checkpoint(\n",
    "            model_g, optimizer, args,\n",
    "            epoch=epoch, model_ema=model_ema, metric=save_metric, use_amp=use_amp)\n",
    "        torch.save(model_raw, 'raw')\n",
    "        torch.save(model_critic, 'critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
